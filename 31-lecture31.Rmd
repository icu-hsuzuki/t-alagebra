# The "Inverse" of $R$ {#lec31}
**Wednesday, April 14, 1993** 

Let $\Gamma = (X, E)$ be any graph of diameter $D\geq 2$. Fix a vertex $x\in X$. Let $E^*_i\equiv E^*_i(x)$, and $T \equiv T(x)$.

Recall adjacency matrix
\begin{align}
A & = R + L + F\\
R & = \sum_{i=0}^D E^*_{i+1}AE^*_i,\\
L & = \sum_{i=0}^D E^*_{i-1}AE^*_i,\\
F & = \sum_{i=0}^D E^*_{i}AE^*_i.
\end{align}

Observe $R$ is not invertible (indeed $RE%*_D = O$.)
So, $R^{-1}$ does not exist.

Below we find a matrix "$R^{-1}$"$\in T(x)$ such that $R^{-1}Rv = v$ for "almost all" $v\in V$.

:::{.lemma #inverse-of-r}
Let $\Gamma = (X, E)$ denote any graph, and the standard module $V$ over $\mathbb{C}$.

Fix a vertex $x\in X$, write
$$R\equiv R(x), \quad L \equiv L(x), \quad E^*_i\equiv E^*_i(x) \quad \text{for all }i.$$
Then,

| $(i)$ There exists unique "$R^{_1}$"$\in \mathrm{Mat}_X(\mathbb{C})$ such that;

| $\quad (ia)$ $R^{-1}v = 0$ if $Lv = 0$ for $v\in V$.

| $\quad (ib)$ $R^{-1}RLv = Lv$ for all $v\in V$.

| $(ii)$ $R^{-1}(E^*_iV)\subseteq E^*_{i-1}V$ $(0\leq i\leq D)$, $E^*_{-1}V = 0$.

| $(iii)$ $R^{-1}\in \mathrm{Mat}_X(\mathbb{Q})$.

| $(iv)$ $R^{-1}\in T(x)$.
:::

:::{.proof}
| $(i)$ Consider the orthogonal direct sum.
$$V = (\mathrm{Ker}L) + (\mathrm{Ker}L)^\bot.$$

Claim 1. $RL(\mathrm{Ker}L)^\bot \subseteq (\mathrm{Ker}L)^\bot$.

_Proof of Claim 1._ 
Pick $v\in (\mathrm{Ker}L)^\bot$, and $w\in \mathrm{Ker}L$. Show
$$\langle RLv, w\rangle = 0.$$
But 
$$\bar{R}^\top = R^\top = \left(\sum_{i=0}^D E^*_{i+1}AE^*_i\right)^\top = \sum_{i=0}^D E^*_iAE^*_{i+1} = L.$$
So,
$$\langle RLv, w\rangle = \langle Lv, \bar{R}^\top w\rangle = \langle Lv, Lw\rangle = 0.$$

|
| Claim 2. $RL: (\mathrm{Ker}L)^\bot \to (\mathrm{Ker}L)^\bot$ is an isomorphism of vector spaces.

_Proof of Claim 2._
It suffices to show above map is one-to-one.

Suppose there is  a vector $v\in (\mathrm{Ker}L)^\bot$ such that $RLv = 0$.

Then,
$$0 = \langle RLv , v\rangle = \langle Lv, \bar{R}^\top v\rangle = \|Lv\|^2.$$
So $Lv = 0$.

Hence $v\in \mathrm{Ker}L \cap (\mathrm{Ker}L)^\bot = 0$.

This proves Claim 2.

Now "$R^{-1}$ denote the unique matrix in $\mathrm{Mat}(\mathbb{C})$ such that
\begin{equation}
R^{-1}v = \begin{cases}
0 & \text{if } v\in \mathrm{Ker}L \\
L(RL)^{-1}v & \text{if } v\in (\mathrm{Ker}L)^\bot.
\end{cases}(\#eq:rinversev)
\end{equation}
Observe that $(RL)^{-1}:(\mathrm{Ker}L)^\bot \to (\mathrm{Ker}L)^\bot$ exists by Claim 2.

Observe $R^{-1}$ satisfies $(ia)$ by \@ref(eq:rinversev).

Claim 3. $R^{-1}$ satisfies $(ib)$.

_Proof of Claim 3._
It suffices to check 
$$R^{-1}RLv = Lv$$
for $v\in \mathrm{Ker}L$ and $v\in (\mathrm{Ker}L)^\bot$.

The case $v\in \mathrm{Ker}L$ is clear. So assume $v\in (\mathrm{Ker}L)^\bot$ by Claim 1.
So,
$$R^{-1}(RLv) = L(RL)^{-1}RLv = Lv$$
as desired.

Uniqueness: 
Suppose a matrix $\hat{R}^{-1}\in \mathrm{Mat}_X(\mathbb{C})$ satisfies $(ia), (ib)$. Then, $\hat{R}^{-1}$ satisfies \@ref(eq:rinversev) atove.

(_Pf._ The first part is clear. Let $v\in (\mathrm{Ker}L)^\bot$. By Claim 2, there exists $w\in (\mathrm{Ker}L)^\bot$ such that $v\in RLw$. So $\hat{R}^{-1}v = \hat{R}^{-1}RLw = Lw = L(RL)^{-1}v$.)

Therefore, $\hat{R}^{-1}$ agrees with $R^{-1}$ on a basis for $V$, and $\hat{R}^{-1} = R^{-1}$.


| $(ii)$ Pick $v\in E^*_iV$. Show $R^{-1}v\in E^*_{i-1}V$.
Without loss of generality we may assume that $v\in \mathrm{Ker}L$ or $v\in (\mathrm{Ker}L)^\bot$.

If $v\in \mathrm{Ker}L$, then $R^{-1}v = 0\in E^*_{i-1}V$.

If $v\in (\mathrm{Ker}L)^\bot$, then
$$R^{-1}v = L(RL)^{-1}v \in LE^*_iV \subseteq E^*_{i-1}V.$$

| $(iii)$ Observe $R, L\in \mathrm(Mat)_X(\mathbb{Q})$. 

So $V$, $\mathrm{Ker}L$, each has basis consisting of vectors in $\mathbb{Q}^{|X|}$.

Replacing the construction of $R^{-1}$ with the base field replaced by $\mathbb{Q}$, we find a matrix $\tilde{R}^{-1}\in \mathrm{Mat}_X(\mathbb{Q})$ satisfying $(ia)$, $(ib)$.

Now $R^{-1}$ and $\tilde{R}^{-1}$ agree on a basis, and hence $R^{-1} = \tilde{R}^{-1}$.

| $(iv)$ $RL = \bar{L}^\top L$ is a real symmetric matrix. So it is diagonalizable.

Let $\theta$ be any eigenvalue of $RL$. Let $V_\theta$ denote the corresponding maximal eigenspace in $V$.
Then
$$V = \sum_{\theta:\text{eigenvalue for }RL}V_\theta \quad (\text{orthogonal direct sum}).$$
Let $E_\theta: V\to V_\theta$ denote the orthogonal projection. Then $E_\theta$ is a complex polynomial in $RL$.

Thus $E_\theta\in T(x)$.

:::{.hs}
**HS MEMO**

$E_\theta$ is real. Since $RL$ is an integral matrix, every eigenvalue of $RL$ is an algebraic integer.
:::

Claim 4. We have
\begin{equation}
R^{-1} = \sum_{\theta: \text{eigenvalue of }RL}\theta^{-1}LE_\theta. (\#eq:rinverse)
\end{equation}
In particular, $R^{-1}\in T(x)$.

_Proof of Claim 4._
Show two sides of \@ref(eq:rinverse) agree, when applied to arbitrary $v\in V$.

Without loss of generality, we may assume that $v\in V_\theta$ for some eigenvalue $\theta$ of $RL$.

Let $\theta'$ denote any eigenvalue of $RL$.
$$E_{\theta'}v = \begin{cases} 0 & \text{if } \theta'\neq \theta,\\
v & \text{if } \theta' = \theta.
\end{cases}$$
$\mathrm{RHS}$ of  \@ref(eq:rinverse) applied to $v$ equals
$$\begin{cases} 0 & \text{if } \theta = 0,\\
\theta^{-1}Lv & \text{if } \theta \neq 0.
\end{cases}$$

Show this equals $R^{-1}v$.

Case $\theta = 0$:
Since $RLv = 0$, 
$$0 = \langle v, RLv\rangle = \|Lv\|^2.$$

Hence $Lv = 0$, or $v\in \mathrm{Ker}L$.
By $(ia)$, $R^{-1}v = 0$.

Case $\theta \neq 0$:
Since $RLv = \theta v$, $v = \theta^{-1}RLv$. Hence, 
$$R^{-1}v = \theta^{-1}R^{-1}RLv = \theta^{-1}Lv$$
by $(ib)$.
:::
