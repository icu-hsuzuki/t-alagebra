[["index.html", "Lecture Note on Terwilliger Algebra About this lecturenote Setting Another Host", " Lecture Note on Terwilliger Algebra P. Terwilliger, edited by H. Suzuki 2022-11-17 About this lecturenote Setting This note is created by bookdown package on RStudio. For bookdown See (Xie 2015), (Xie 2017), (Yihui Xie 2018). Log-in to my GitHub Account Go to RStudio/bookdown-demo repository: https://github.com/rstudio/bookdown-demo Use This Template Input Repository Name Select Public - default Create repository from template From Code download ZIP Move the extracted folder into a favorite directory Open RStudio Project in the folder Use Terminal in the buttom left pane confirm that the current directory is the home directry of the project by pwd (failed to proceed by ssh) Use Console library(usethis) use_git() use_github() — Error gh_token_help() create_github_token(): create a token in the github page. Copy the token gitcreds::gitcreds_set(): paste the token, the token is to be expired in 30 days Use Terminal git remote add origin https://github.com/icu-hsuzuki/t-alagebra.git git push -u origin main type in the password of the computer Use GIT in R Studio Another Host library(usethis) use_git() create_github_token() gitcreds::gitcreds_set(): Replace these credentials References "],["lec1.html", "Chapter 1 Subconstituent Algebra of a Graph", " Chapter 1 Subconstituent Algebra of a Graph Wednesday, January 20, 1993 A graph (undirected, without loops or multiple edges) is a pair \\(\\Gamma = (X, E)\\), where \\[\\begin{align} X &amp;= \\textrm{finite set (of vertices)}\\\\ E &amp; = \\textrm{set of (distinct) 2-element subsets of }X \\textrm{ (= edges of ) }\\Gamma. \\end{align}\\] vertices \\(x\\) and \\(y\\in X\\) are adjacent if and only if \\(xy\\in E\\). Example 1.1 Let \\(\\Gamma\\) be a graph. \\(X = \\{a, b, c, d\\}\\), \\(E = \\{ab, ac, bc, bd\\}\\). Set \\(n = |X|\\), the order of \\(\\Gamma\\). Pick a field \\(K\\) (\\(=\\mathbb{R}\\) or \\(\\mathbb{C}\\)). Then \\(\\mathrm{Mat}_X(K)\\) denotes the \\(K\\) algebra of all \\(n\\times n\\) matrices with entries in \\(K\\). (rows and columns are indexed by \\(X\\)) Adjacency matrix \\(A\\in \\mathrm{Mat}_X(K)\\) is defined by \\[\\begin{align} A_{xy} &amp; = \\left\\{\\begin{array}{cl} 1 &amp; \\textrm{ if } \\; xy\\in E\\\\ 0 &amp; \\textrm{ else } . \\end{array}\\right. \\end{align}\\] Example 1.2 Let \\(a, b, c, d\\) be labels of rows and columns. Then \\[A = \\begin{matrix} \\\\ a\\\\ b\\\\c\\\\d\\end{matrix}\\begin{matrix}\\begin{matrix} a &amp; b &amp; c &amp; d \\end{matrix}\\\\\\begin{pmatrix} 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\end{pmatrix}\\end{matrix}\\] The subalgebra \\(M\\) of \\(\\mathrm{Mat}_X(K)\\) generated by \\(A\\) is called the Bose-Mesner algebra of \\(\\Gamma\\). Set \\(V = K^n\\), the set of \\(n\\)-dimensional column vectors, the coorinates are indexed by \\(X\\). Let \\(\\langle\\; , \\;\\rangle\\) denote the Hermitean inner product: \\[\\langle u, v\\rangle = u^\\top\\cdot v \\quad (u, v\\in V)\\] \\(V\\) with \\(\\langle\\; , \\;\\rangle\\) is the standard module of \\(\\Gamma\\). \\(M\\) acts on \\(V\\): For every \\(x\\in X\\), write \\[\\hat{x} = \\begin{pmatrix} 0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}\\] where \\(1\\) is at the \\(x\\) position. Then \\[A\\hat{x} = \\sum_{y\\in X, xy\\in E}\\hat{y}.\\] Since \\(A\\) is a real symmetrix matrix, \\[V = V_0 + V_1 + \\cdots + V_r \\quad \\textrm{ some } r\\in \\mathbb{Z}^{\\geq0},\\] the orthogonal direct sum of maximal \\(A\\)-eigenspaces. Let \\(E_i\\in\\mathrm{Mat}_X(K)\\) denote the orthogonal projection, \\[E_i: V \\longrightarrow V_i.\\] Then \\(E_0, \\ldots, E_r\\) are the primitive idempotents of \\(M\\). \\[M = \\mathrm{Span}_K(E_0, \\ldots, E_r),\\] \\[E_iE_j = \\delta_{ij}E_i \\quad \\textrm{for all }\\; i, j, \\quad E_0 + \\cdots + E_r = I.\\] Let \\(\\theta_i\\) denote the eigenvalue of \\(A\\) for \\(V_i\\) in \\(\\mathbb{R}\\). Without loss of generality we may assume that \\[\\theta_0 &gt; \\theta_1 &gt; \\cdots &gt; \\theta_r.\\] Let \\[m_i = \\textrm{the multiplicity of }\\: \\theta_i = \\mathrm{dim} V_i = \\mathrm{rank} E_i.\\] Set \\[\\mathrm{Spec}(\\Gamma) = \\begin{pmatrix} \\theta_0, &amp; \\theta_1, &amp; \\cdots, &amp; \\theta_r\\\\m_0, &amp; m_1, &amp; \\cdots, &amp; m_r\\end{pmatrix}.\\] Problem. What can we say about \\(\\Gamma\\) when \\(\\mathrm{Spec}(\\Gamma)\\) is given? The following Lemma1.1, is an example of Problem. For every \\(x\\in X\\), \\[k(x) \\equiv \\textrm{ valency of }x \\equiv \\textrm{ degree of }x \\equiv |\\{y\\mid y\\in X, \\: xy\\in E\\}|.\\] Definition 1.1 The graph \\(\\Gamma\\) is regular of valency \\(k\\) if \\(k = k(x)\\) for every \\(x\\in X\\). Lemma 1.1 With the above notation, \\((i)\\) \\(\\theta_0\\leq \\max\\{k(x) \\mid x\\in X\\} = k^{\\max}\\). \\((ii)\\) If \\(\\Gamma\\) is regular of valency \\(k\\), then \\(\\theta_0 = k\\). Proof. \\((i)\\) Without loss of generality we may assume that \\(\\theta_0&gt;0\\), else done. Let \\(v:=\\sum_{x\\in X}\\alpha_x\\hat{x}\\) denote the eivenvector for \\(\\theta_0\\). Pick \\(x\\in X\\) with \\(|\\alpha_x|\\) maximal. Then \\(|\\alpha_x|\\neq 0\\). Since \\(Av = \\theta_0v\\), \\[\\theta_0\\alpha_x = \\sum_{y\\in X, xy\\in E}\\alpha_y.\\] So, \\[\\theta_0 |\\alpha_x| = |\\theta_0\\alpha_x| \\leq \\sum_{y\\in X, xy\\in E}|\\alpha_y| \\leq k(x)|\\alpha_x| \\leq k^{\\max}|\\alpha_x|.\\] \\((ii)\\) All 1’s vector \\(v = \\sum_{x\\in X}\\hat{x}\\) satisfies \\(Av = kv\\). Subconstituent Algebra Let \\(x, y\\in X\\) and \\(\\ell \\in \\mathbb{Z}^{\\geq 0}\\). Definition 1.2 A path of length \\(\\ell\\) connecting \\(x, y\\) is a sequence \\[x = x_0, x_1, \\ldots, x_{\\ell} = y, \\quad x_i\\in X, \\; 0\\leq i\\leq \\ell\\] such that \\(x_ix_{i+1}\\in E\\) for \\(0\\leq i \\leq \\ell-1\\). Definition 1.3 The distance \\(\\partial(x,y)\\) is the length of a shortest path connecting \\(x\\) and \\(y\\). \\[\\partial(x,y) \\in \\mathbb{Z}^{\\geq 0} \\cup \\{\\infty\\}.\\] Definition 1.4 The graph \\(\\Gamma\\) is connected if and only if \\(\\partial(x,y) &lt; \\infty\\) for all \\(x, y\\in X\\). From now on, assume that \\(\\Gamma\\) is connected with \\(|X|\\geq 2\\). Set \\[d_\\Gamma = d = \\max\\{\\partial(x,y)\\mid x, y\\in X\\} \\equiv \\textrm{the diameter of }\\Gamma.\\] Fix a ‘base’ vertex \\(x\\in X\\). Definition 1.5 \\[d(x) = \\textrm{the diameter with respect to }x = \\max\\{\\partial(x,y)\\mid y\\in X\\} \\leq d.\\] Observe that \\[V = V_0^* + V_1^* + \\cdots + V_{d(x)}^* \\quad \\textrm{(orthogonal direct sum)},\\] where \\[V_i^* = \\mathrm{Span}_K(\\hat{y}\\mid \\partial(x,y) = i) \\equiv V_i*(x)\\] and \\(V_i^* = V_i^*(x)\\) is called the \\(i\\)-the subconstituent with respect to \\(x\\). Let \\(E_i^* = E_i^*(x)\\) denote the orthogonal projection \\[E_i^*: V \\longrightarrow V_i^*(x).\\] View \\(E_i^*(x) \\in \\mathrm{Mat}_X(K)\\). So, \\(E_i^*(x)\\) is diagonal with \\(yy\\) entry \\[(E_i^*(x))_{yy} = \\begin{cases} 1 &amp; \\textrm{if } \\: \\partial(x,y) = i\\\\ 0 &amp; \\textrm{else,}\\end{cases} \\quad \\textrm{ for } y\\in X.\\] Set \\[M^* = M^*(x) \\equiv \\textrm{Span}_K(E_0^*(x), \\ldots, E_{d(x)}^*(x)).\\] Then \\(M^*(x)\\) is a commutative subalgebra of \\(\\mathrm{Mat}_X(K)\\) and is calle the dual Bose-Mesner algbara with respect to \\(x\\). Definition 1.6 (Subconstituent Algebra) Let \\(\\Gamma = (X, E)\\), \\(x\\), \\(M\\), \\(M^*(x)\\) be as above. Let \\(T = T(x)\\) denote the subalgebra of \\(\\mathrm{Mat}_X(K)\\) generated by \\(M\\) and \\(M^*(x)\\). \\(T\\) is the subconstituent algebra of \\(\\Gamma\\) with respect to \\(x\\). Definition 1.7 A \\(T\\)-module is any subspace \\(W\\subset V\\) such that \\(aw\\in W\\) for all \\(a\\in T\\) and \\(w\\in W\\). \\(T\\)-module \\(W\\) is irreducible if and only if \\(W\\neq 0\\) and \\(W\\) does not properly contain a nonzero \\(T\\)-module. For any \\(a\\in \\mathrm{Mat}_X(K)\\), let \\(a^*\\) denbote the conjugate transpose of \\(a\\). Observe that \\[\\langle au, v\\rangle = \\langle u, a^*v\\rangle \\quad \\textrm{for all }\\; a\\in \\mathrm{Mat}_X(K), \\textrm{ and for all } \\; u,v\\in V.\\] Lemma 1.2 Let \\(\\Gamma = (X,E)\\), \\(x\\in X\\) and \\(T \\equiv T(x)\\) be as above. \\((i)\\) If \\(a\\in T\\), then \\(a^*\\in T\\). \\((ii)\\) For any \\(T\\)-module \\(W\\subset V\\), \\[W^\\bot := \\{v\\in V\\mid \\langle w, v\\rangle = 0, \\textrm{ for all }w\\in W\\}\\] is a \\(T\\)-module. \\((iii)\\) \\(V\\) decomposes as an orthogonal direct sum of irreducible \\(T\\)-modules. Proof. \\((i)\\) It is becase \\(T\\) is generated by symmetric real matrices \\[A, E^*_0(x), E^*_1(x), \\ldots, E^*_{d(x)(x)}.\\] \\((ii)\\) Pick \\(v\\in W^\\bot\\) and \\(a\\in T\\), it suffices to show that \\(av\\in W^\\bot\\). For all \\(w\\in W\\), \\[\\langle w, av\\rangle = \\langle a^*w, v\\rangle = 0\\] as \\(a^*\\in T\\). \\((iii)\\) This is proved by the induction on the dimension of \\(T\\)-modules. If \\(W\\) is an irreducible \\(T\\)-module of \\(V\\), then \\[V = W + W^\\bot \\quad \\textrm{(orthogonal direct sum)}.\\] Problem. What does the structure of the \\(T(x)\\)-module tell us about \\(\\Gamma\\)? Study those \\(\\Gamma\\) whose modules take ‘simple’ form. The \\(\\Gamma\\)’s involved are highly regular. Remark. The subconstituent algebra \\(T\\) is semisimple as the left regular representation of \\(T\\) is completely reducible. See Curtis-Reiner 25.2 (Charles W. Curtis 2006). The inner product \\(\\langle a, b\\rangle_T = \\mathrm{tr}(a^\\top\\bar{b})\\) is nondegenerate on \\(T\\). In general, \\[\\begin{align*} T\\textrm{: Semisimple and Artinian} &amp; \\Leftrightarrow T\\textrm{: Artinian with } J(T) = 0 \\\\ &amp; \\Leftarrow T\\textrm{: Artinian with nonzero nilpotent element} \\\\ &amp; \\Leftarrow T \\subset \\mathrm{Mat}_X(K) \\textrm{ such that for all } a\\in T \\textrm{ is normal.} \\end{align*}\\] References "],["lec2.html", "Chapter 2 Perron-Frobenius Theorem", " Chapter 2 Perron-Frobenius Theorem Friday, January 22, 1993 In this lecture we use the Perron Frobenius theory of nonnegative matrices to obtain informaiton on eigenvalues of a graph. Let \\(K = \\mathbb{R}\\). For \\(n\\in \\mathbb{Z}^{&gt; 0}\\), pick a symmetrix matrix \\(C\\in \\mathrm{Mat}_n(\\mathbb{R})\\). Definition 2.1 The matrix \\(C\\) is reducible if and only if there is a bipartition \\(\\{1, 2, \\ldots, n\\} = X^+ \\cup X^-\\) (disjoint union of nonempty sets) such that \\(C_{ij} = 0\\) for all \\(i\\in X^+\\), and for all \\(j\\in X^-\\), and for all \\(i\\in X^-\\), and for all \\(j\\in X^+\\),i.e., \\[ C \\sim \\begin{pmatrix} \\ast &amp; O \\\\ O &amp; \\ast \\end{pmatrix}.\\] Definition 2.2 The matrix \\(C\\) is bipartite if and only if there is a bipartition \\(\\{1, 2, \\ldots, n\\} = X^+ \\cup X^-\\) (disjoint union of nonempty sets) such that \\(C_{ij} = 0\\) for all \\(i,j\\in X^+\\), and for all \\(i,j\\in X^-\\), i.e., \\[ C \\sim \\begin{pmatrix} O &amp; \\ast \\\\ \\ast &amp; O \\end{pmatrix}.\\] Note. If \\(C\\) is bipatite, for every eigenvalue \\(\\theta\\) of \\(C\\), \\(-\\theta\\) is an eigenvalue of \\(C\\) such that \\(\\mathrm{mult}(\\theta) = \\mathrm{mult}(-\\theta)\\). Indeed, let \\(C = \\begin{pmatrix} O &amp; A \\\\ B &amp; O \\end{pmatrix}\\), \\[\\begin{pmatrix} O &amp; A \\\\ B &amp; O \\end{pmatrix} \\begin{pmatrix}x\\\\y\\end{pmatrix} = \\theta \\begin{pmatrix}x\\\\y\\end{pmatrix}\\Leftrightarrow \\begin{pmatrix} O &amp; A \\\\ B &amp; O \\end{pmatrix} \\begin{pmatrix}x\\\\-y\\end{pmatrix} = -\\theta \\begin{pmatrix}x\\\\-y\\end{pmatrix}, \\] where \\(Ay = \\theta x\\) and \\(Bx = \\theta y\\). If \\(C\\) is bipartite, \\(C^2\\) is reducible. The matrix \\(C\\) is irreducible and \\(C^2\\) is reducible, if \\(C_{ij} \\geq 0\\) for all \\(i,j\\) and \\(C\\) is reducible. (Exercise) Remark. Note 1. Even if \\(C\\) is not symmetric \\[\\begin{pmatrix} O &amp; A \\\\ B &amp; O \\end{pmatrix} \\begin{pmatrix}x\\\\y\\end{pmatrix} = \\theta \\begin{pmatrix}x\\\\y\\end{pmatrix}\\Leftrightarrow \\begin{pmatrix} O &amp; A \\\\ B &amp; O \\end{pmatrix} \\begin{pmatrix}x\\\\-y\\end{pmatrix} = -\\theta \\begin{pmatrix}x\\\\-y\\end{pmatrix}\\] holds. So the geometrix multiplicities coincide. How about the algebraic multiplicities? Note 3. Set \\(x \\sim y\\) if and only if \\(C_{xy}&gt;0\\). So the graph may have loops. Then \\[(C^2)_{xy} &gt; 0 \\Leftrightarrow \\textrm{ if there exists } z\\in X \\textrm{ such that } x\\sim z \\sim y.\\] Note that \\(C\\) is irreducible if and only if \\(\\Gamma(C)\\) is connected. Let \\[\\begin{align} X^+ &amp; = \\{y\\mid \\textrm{there is a path of even length from }x \\textrm{ to }y\\}\\\\ X^- &amp; = \\{y\\mid \\textrm{there is no path of even length from }x \\textrm{ to }y\\} \\neq \\emptyset. \\end{align}\\] If there is an edge \\(y\\sim z\\) in \\(X^+\\) and \\(w\\in X^-\\). Then there would be a path from \\(x\\) to \\(y\\) of even length. So \\(\\mathrm{e}(X^+, X^+) = \\mathrm{e}(X^-, X^-) = 0.\\). Theorem 2.1 (Perron-Frobenius) Given a matrix \\(C\\) in \\(\\mathrm{Mat}_n(\\mathbb{R})\\) such that \\((a)\\) \\(C\\) is symmetric. \\((b)\\) \\(C\\) is irreducible. \\((c)\\) \\(C_{ij} \\geq 0\\) for all \\(i,j\\). Let \\(\\theta_0\\) be the maximal eigenvalue of \\(C\\) with eigenspace \\(V_0 \\subseteq \\mathbb{R}^n\\), and let \\(\\theta_r\\) be the maximal eigenvalue of \\(C\\) with eigenspace \\(V_r \\subseteq \\mathbb{R}^n\\). Then the following hold. \\((i)\\) Suppose \\(0\\neq v = \\begin{pmatrix}\\alpha_1\\\\\\alpha_2\\\\\\vdots\\\\\\alpha_n\\end{pmatrix} \\in V_0\\). Then \\(\\alpha_0 &gt; 0\\) for all \\(i\\), or \\(\\alpha_i &lt; 0\\) for all \\(i\\). \\((ii)\\) \\(\\mathrm{dim}V_0 = 1\\). \\((iii)\\) \\(\\theta_r \\geq -\\theta_0\\). \\((iv)\\) \\(\\theta_r = \\theta_0\\) if and only if \\(C\\) is bipartite. First, we prove the following lemma. Lemma 2.1 Let \\(\\langle \\;,\\; \\rangle\\) be the dot product in \\(V = \\mathbb{R}^n\\). Pick a symmetric matrix \\(B \\in \\mathrm{Mat}_n(\\mathbb{R})\\). Suppose all eigenvalues of \\(B\\) are nonnegative. (i.e., \\(B\\) is positive semidefinite.) Then there exist vectors \\(v_1, v_2, \\ldots, v_n\\in V\\) such that \\(B_{ij} = \\langle v_i, v_j\\rangle\\) for \\((1\\leq i, j \\leq n)\\). Proof. By elementary linear algebra, there exists an orthonormal basis \\(w_1, w_2, \\ldots, w_n\\) of \\(V\\) consisting of eigenvectors of \\(B\\). Set the \\(i\\)-th column of \\(P\\) is \\(w_i\\) and \\(D = \\mathrm{diag}(\\theta_1,\\ldots, \\theta_n)\\). Then \\(P^\\top P = I\\) and \\(BP = PD\\). Hence, \\[B = PDP^{-1} = PDP^\\top = QQ^\\top,\\] where \\[Q = P\\cdot \\mathrm{diag}(\\sqrt{\\theta_1}, \\sqrt{\\theta_2}, \\ldots, \\sqrt{\\theta_n}) \\in \\mathrm{Mat}_n(\\mathbb{R}).\\] Now, let \\(v_i\\) be the \\(i\\)-th column of \\(Q^\\top\\). Then \\[B_{ij} = v_i^\\top\\cdot v_j^- = \\langle v_i, v_j\\rangle.\\] Now we start the proof of Theorem 2.1. Proof of Theorem 2.1\\((i)\\) Let \\(\\langle , \\rangle\\) denote the dot product on \\(V = \\mathbb{R}^n\\). Set \\[\\begin{align} B &amp; = \\theta I - C\\\\ &amp; = \\textrm{symmetric matrix with eigenvalues }\\theta_0 - \\theta_i \\geq 0\\\\ &amp; = (\\langle v_i, v_j\\rangle)_{1\\leq i,j\\leq n} \\end{align}\\] with the same \\(v_1, \\ldots, v_n \\in V\\) by Lemma 2.1. Observe: \\(\\sum_{i = 1}^n \\alpha_iv_i = 0\\). Pf. \\[\\begin{align} \\|\\sum_{i = 1}^n \\alpha_iv_i\\|^2 &amp; = \\langle \\sum_{i=1}^n\\alpha_iv_i, \\sum_{i=1}^n\\alpha_iv_i\\rangle\\\\ &amp; = \\begin{pmatrix} \\alpha_1 &amp;\\ldots &amp;\\alpha_n\\end{pmatrix}B\\begin{pmatrix} \\alpha_1\\\\\\vdots\\\\\\alpha_n\\end{pmatrix}\\\\ &amp; = v^\\top Bv\\\\ &amp; = 0, \\end{align}\\] since \\(Bv = (\\theta_0 I - C)v = 0\\). Now set \\[s = \\textrm{the number of indices} i, \\textrm{ where } \\alpha_i &gt;0.\\] Replacing \\(v\\) by \\(-v\\) if necessary, without loss of generality we may assume that \\(s\\geq 1\\). We want to show \\(s = n\\). Assume \\(s &lt; n\\). Without loss of generality, we may assume that \\(\\alpha_i &gt;0\\) for \\(1\\leq s\\leq s\\) and \\(\\alpha_i = 0\\) for \\(s+1 \\leq i \\leq n\\). Set \\[ \\rho = \\alpha_1v_1 + \\cdots + \\alpha_sv_s = -\\alpha_{s+1}v_{s+1} - \\cdots - \\alpha_nv_n.\\] Then, for \\(i = 1,\\ldots, s\\), \\[\\begin{align} \\langle v_i, \\rho \\rangle &amp; = \\sum_{j = s+1}^n -\\alpha_j\\langle v_i, v_j\\rangle \\quad (\\langle v_i, v_j\\rangle = B_{ij}, B = \\theta_0I - C)\\\\ &amp; = \\sum_{j = s+1}^n (-\\alpha_{ij})(-C_{ij})\\\\ &amp; \\leq 0. \\end{align}\\] Hence \\[0\\leq \\langle \\rho, \\rho\\rangle = \\sum_{i=1}^s \\alpha_i \\langle v_i, \\rho\\rangle \\leq 0,\\] as \\(\\alpha &gt; 0\\) and \\(\\langle v_i, \\rho\\rangle \\leq 0\\). Thus, we have \\(\\langle, \\rho, \\rho \\rangle = 0\\) and \\(\\rho = 0\\). For \\(j = s+1, \\ldots, n\\), \\[0 = \\langle \\rho, v_j\\rangle = \\sum_{i=1}^s \\alpha_i\\langle v_i, v_j\\rangle \\leq 0,\\] as \\(\\langle v_i, v_j\\rangle = -C_{ij}\\). Therefore, \\[0 = \\langle v_i, v_j \\rangle = -C_{ij} \\textrm{ for } 1\\leq i, \\leq s, \\: s+1 \\leq j \\leq n.\\] Since \\(C\\) is symmetric, \\[C = \\begin{pmatrix} \\ast &amp; O \\\\ O &amp; \\ast\\end{pmatrix}\\] Thus \\(C\\) is reducible, which is not the case. Hence \\(s = n\\). Proof of Theorem 2.1 \\((ii)\\). Suppose \\(\\dim V_0 \\geq 2\\). Then, \\[\\dim\\left(V_0 \\cap \\begin{pmatrix}1\\\\0\\\\\\vdots\\\\0\\end{pmatrix}^\\bot\\right) \\geq 1.\\] So, there is a vector \\[0\\neq v = \\begin{pmatrix}\\alpha_1\\\\\\vdots\\\\\\alpha_n\\end{pmatrix} \\in V_0\\] with \\(\\alpha_1 = 0\\). This contradicts 1. Now pick \\[0\\neq w = \\begin{pmatrix}\\beta_1\\\\\\vdots\\\\\\beta_n\\end{pmatrix} \\in V_r.\\] Proof of Theorem 2.1 \\((iii)\\). Suppose \\(\\theta_r &lt; -\\theta_0\\). Since the eigenvalues of \\(C^2\\) are the squares of those of \\(C\\), \\(\\theta_r^2\\) is the maximal eigenvalue of \\(C^2\\). Also we have \\(C^2w = \\theta_r^2w\\). Observe that \\(C^2\\) is irreducible. (As otherwise, \\(C\\) is bipartite by Note 3, and we must have \\(\\theta_r = -\\theta_0\\).) Therefore, \\(\\beta_i &gt; 0\\) for all \\(i\\) or \\(\\beta_i &lt; 0\\) for all \\(i\\). We have \\[\\langle v, w\\rangle = \\sum_{i=1}^n\\alpha_i\\beta_j \\neq 0.\\] This is a contradiction, as \\(V_0 \\bot V_r\\). Proof of Theorem 2.1 \\((iv)\\) \\(\\Rightarrow\\): Let \\(\\theta_r = -\\theta_0\\). Then \\(\\theta = \\theta_1^2 = \\theta_0^2\\) is the maximal eigenvalue of \\(C^2\\), and \\(v\\) and \\(w\\) are linearly independent eigenvalues for \\(\\theta\\) for \\(C^2\\). Hence, for \\(C^2\\), \\(\\mathrm{mult}(\\theta) \\geq 2\\). Thus by 2, \\(C^2\\) must be reducible. Therefore, \\(C\\) is bipartite by Note 3. \\(\\Leftarrow\\): This is Note 1. \\(\\Box\\) Let \\(\\Gamma = (X, E)\\) be any graph. Definition 2.3 \\(\\Gamma\\) is said to be bipartite if the adjacency matrix \\(A\\) is bipartite. That is, \\(X\\) can be written as a disjoint union of \\(X^+\\) and \\(X^-\\) such that \\(X^+, X^-\\) contain no edges of \\(\\Gamma\\). Corollary 2.1 For any (connected) graph \\(\\Gamma\\) with \\[\\mathrm{Spec}(\\Gamma) = \\begin{pmatrix}\\theta_0 &amp; \\theta_1 &amp;\\cdots &amp; \\theta_r\\\\m_1 &amp; m_1 &amp; \\cdots &amp; m_r\\end{pmatrix} \\:\\textrm{ with }\\: \\theta_0 &gt; \\theta_1 &gt; \\cdots &gt; \\theta_r.\\] Let \\(V_i\\) be the eigenspace of \\(\\theta_i\\). Then the following holds. Supppose \\(0\\neq v = \\begin{pmatrix} \\alpha_1\\\\\\vdots \\\\\\alpha_n \\end{pmatrix} \\in V_0\\in \\mathbb{R}^n\\). Then \\(\\alpha_i &gt; 0\\) for all \\(i\\) or \\(\\alpha_i &lt; 0\\) for all \\(i\\). \\(m_0 = 1\\). \\(\\theta_r \\geq -\\theta_0\\) if and only if \\(\\Gamma\\) is bipartite. In this case, \\[-\\theta_i = \\theta_{r-i} \\; \\textrm{and} \\; m_i = m_{r-i} \\quad (0\\leq i\\leq r)\\] Proof. This is a direct consequences of Theorem 2.1 and Note 3. "],["lec3.html", "Chapter 3 Cayley Graphs", " Chapter 3 Cayley Graphs Monday, January 25, 1993 Given graphs \\(\\Gamma = (X, E)\\) and \\(\\Gamma&#39; = (X&#39;, E&#39;)\\). Definition 3.1 A map \\(\\sigma: X \\to X&#39;\\) is an isomorphism\\index{isomorophism of graphs whenever; \\(\\sigma\\) is one-to-one and onto, \\(xy\\in E\\) if and only if \\(\\sigma x \\sigma y \\in E&#39;\\) for all \\(x, y\\in X\\). We do not distinguish between isomorphic graphs. Definition 3.2 Suppose \\(\\Gamma = \\Gamma&#39;\\). Above isomorphism \\(\\sigma\\) is called an automorphism of \\(\\Gamma\\). Then set \\(\\mathrm{Aut}(\\Gamma)\\) of all automorphisms of \\(\\Gamma\\) becomes a finite group under composition. Definition 3.3 If \\(\\mathrm{Aut}(\\Gamma)\\) acts transitive on \\(X\\), \\(\\Gamma\\) is called vertex transitive. Example 3.1 A Cayley graphs: Definition 3.4 (Cayley Graphs) Let \\(G\\) be any finite group, and \\(\\Delta\\) any generating set for \\(G\\) such that \\(1_G \\not\\in \\Delta\\) and \\(g\\in \\Delta \\to g^{-1}\\in \\Delta\\). Then Cayley graph \\(\\Gamma = \\Gamma(G, \\Delta)\\) is defined on the vetex set \\(X = G\\) with the edge set \\(E\\) define by the following. \\[E = \\{(h_1,h_2)\\mid h_1, h_2\\in G, h_1^{-1}h_2\\in \\Delta\\} = \\{(h, hg) \\mid h\\in G, g\\in \\Delta\\}\\] Example 3.2 \\(G = \\langle a \\mid a^6 = 1\\rangle\\), \\(\\Delta = \\{a, a^{-1}\\}\\). Example 3.3 \\(G = \\langle a \\mid a^6 = 1\\rangle\\), \\(\\Delta = \\{a, a^{-1}, a^2, a^{-2}\\}\\). Example 3.4 \\(G = \\langle a, b \\mid a^6 = 1, b^2, ab = ba\\rangle\\), \\(\\Delta = \\{a, a^{-1}, b\\}\\). Remark. \\(\\mathrm{Aut}(\\Gamma) \\simeq D_6\\times \\mathbb{Z}_2\\) contains two regular subgroups isomorphic to \\(D_6\\) and \\(\\mathbb{Z}_5 \\times \\mathbb{Z}_2\\) and \\(\\Gamma\\) is obtained as Cayley graphs in two ways. Cayley graphs are vertex transitive, indeed. Theorem 3.1 The following hold. \\((i)\\) For any Cayley graph \\(\\Gamma = \\Gamma(G, \\Delta)\\), the map \\[G \\to \\mathrm{Aut}(\\Gamma) \\; (g\\mapsto \\hat{g})\\] is an injective homomorphism of groups, where \\[\\hat{g}(x) = gx \\quad \\textrm{for all }\\: g\\in G \\textrm{ and for all } x\\in X (= G).\\] Also, the image \\(\\hat{G}\\) is regular on \\(X\\). i.e., the image \\(\\hat{G}\\) acts transitively on \\(X\\) with trivial vertex stabilizers. \\((ii)\\) For any graph \\(\\Gamma = (X, E)\\), suppose there exists a subgroup \\(G \\subseteq \\mathrm{Aut}(\\Gamma)\\) that is regular on \\(X\\). Pick \\(x\\in X\\), and let \\[\\Delta = \\{g\\in G\\mid \\langle x, g(x)\\in E\\}.\\] Then \\(1\\not\\in \\Delta\\), \\(g\\in \\Delta \\to g^{-1}\\in \\Delta\\), and \\(\\Delta\\) generates \\(G\\). Moreover, \\(\\Gamma \\simeq \\Gamma(G, \\Delta)\\). Proof. \\((i)\\) Let \\(g\\in G\\). We want to show that \\(\\hat{g}\\in \\mathrm{Aut}(\\Gamma)\\). Let \\(h_1, h_2\\in X = G\\). Then, \\[\\begin{align} (h_1, h_2)\\in E &amp; \\to h_1^{-1}h_2\\in \\Delta\\\\ &amp; \\to (gh_1)^{-1}(gh_2)\\in \\Delta \\\\ &amp; \\to (gh_1, gh_2)\\in E\\\\ &amp; \\to (\\hat{g}(h_1), \\hat{g}(h_2)) \\in E. \\end{align}\\] Hence, \\(\\hat{g}\\in \\mathrm{Aut}(\\Gamma)\\). Observe: \\(g \\mapsto \\hat{g}\\) is a homomorphism of groups: \\[\\hat{1}_G = 1, \\; \\widehat{g_1g_2} = \\widehat{g_1}\\widehat{g_2}.\\] Observe: \\(g \\mapsto \\hat{g}\\) is one-to-one: \\[\\widehat{g_1} = \\widehat{g_2} \\to g_1 = \\widehat{g_1}(1_G) = \\widehat{g_2}(1_G) = g_2.\\] Observe: \\(\\hat{G}\\) is regular on \\(X\\): Clear by construction. \\((ii)\\) \\(1_G\\not\\in \\Delta\\): Since \\(\\Gamma\\) has not loops, \\((x, 1_Gx) \\not\\in E\\). \\(g\\in \\Delta \\to g^{-1}\\in \\Delta\\): \\[g\\in \\Delta \\to (x, g(x))\\in E \\to E \\ni (g^{-1}(x), g^{-1}(g(x))) = (g^{-1}(x), x).\\] \\(\\Delta\\) generates \\(G\\): Suppose \\(\\langle \\Delta \\subsetneq G\\). Let \\(\\hat{X} = \\{g(x)\\mid g\\in \\langle \\Delta\\rangle\\} \\subsetneq X\\). (\\(\\hat{X} \\subsetneq X\\) as \\(G\\) acts regularly on \\(X\\).) Since \\(\\Gamma\\) is connected, there exists \\(y\\in \\hat{X}\\) and \\(z\\in X\\setminus \\hat{X}\\) with \\(yz\\in E\\). Let \\(y = g(x)\\), \\(g\\in \\langle \\Delta\\rangle\\), \\(z\\in h(x)\\), \\(h\\in G\\setminus \\langle \\Delta\\rangle\\). Then \\[(y,z)=(g(x),h(x))\\in E \\to (x,g^{-1}h(x))\\in E \\to g^{-1}h\\in \\langle \\Delta \\rangle \\to h\\in \\langle \\Delta \\rangle. \\] This is a contradition. Therefore, \\(\\Delta\\) generates \\(G\\). Let \\(\\Gamma&#39; = (X&#39;, E&#39;)\\) denote \\(\\Gamma(G, \\Delta)\\). We shall show that \\[\\theta: X&#39; \\to X \\; (g\\mapsto g(x))\\] is an isomorphism of graphs. \\(\\theta\\) is one-to-one: For \\(h_1, h_2\\in X&#39; = G\\), \\[\\theta(h_1)=\\theta(h_2) \\to h_1(x) = h_2(x) \\to h_2^{-1}h_1(x)=x \\to h_2^{-1}h_1\\in \\mathrm{Stab}_G(x) = \\{1_G\\} \\to h_1 = h_2.\\] (\\(\\mathrm{Stab}_G = \\{g\\in G\\mid g(x) = x\\}\\).) \\(\\theta\\) is onto: Since \\(G\\) is transitive, \\[X = \\{g(x)\\mid g\\in G\\} = \\theta(X&#39;) = \\theta(G).\\] \\(\\theta\\) respects adjacency: For \\(h_1, h_2\\in X&#39; = G\\), \\[(h_1,h_2)\\in E&#39; \\leftrightarrow h_1^{-1}h_2\\in \\Delta \\leftrightarrow (x, h_1^{-1}h_2(x))\\in E \\leftrightarrow (h_1(x),h_2(x))\\in E \\leftrightarrow (\\theta(h_1), \\theta(h_2))\\in E.\\] Therefore \\(\\theta\\) is an isomorphism between graphs \\(\\Gamma(G, \\Delta)\\) and \\(\\Gamma(X, E)\\). How to compute the eigenvalues of the Cayley graph of and abelian group. Let \\(G\\) be any finite abelian group. Let \\(\\mathbb{C}^*\\) be the multiplicative group on \\(\\mathbb{C}\\setminus \\{0\\}\\). Definition 3.5 A (linear) \\(G\\)-character is any group homomorphism \\(\\theta: G \\to \\mathbb{C}^*\\). Example 3.5 \\(G = \\langle a\\mid a^3 =1\\rangle\\) has three characters, \\(\\theta_0, \\theta_1, \\theta_2\\). \\[ \\begin{array}{c|ccc} \\theta_i(a^j) &amp; 1 &amp; a &amp; a^2 \\\\ \\hline \\theta_0 &amp; 1 &amp; 1 &amp; 1\\\\ \\theta_1 &amp; 1 &amp; \\omega &amp; \\omega^2\\\\ \\theta_2 &amp; 1 &amp; \\omega^2 &amp; \\omega \\end{array}, \\quad \\textrm{with }\\; \\omega = \\frac{-1+\\sqrt{-3}}{2}. \\] Here \\(\\omega\\) is a primitive cube root pf \\(q\\) in \\(\\mathbb{C}^*\\), i.e., \\(1+\\omega + \\omega^2 = 0\\). For arbitraty group \\(G\\), let \\(X(G)\\) be the set of all characters of \\(G\\). Observe: For \\(\\theta_1, \\theta_2\\in X(G)\\), one can define product $_1_2: \\[\\theta_1\\theta_2(g) = \\theta_1(g)\\theta_2(g) \\quad \\textrm{for all }\\; g\\in G.\\] Then \\(\\theta_1\\theta_2\\in X(G)\\). Observe: \\(X(G)\\) with this product is an (abelian) group. Lemma 3.1 The groups \\(G\\) and \\(X(G)\\) are isomorphic for all finite abelian groups \\(G\\). Proof. \\(G\\) is a direct sum of cyclic groups; \\[G = G_1\\oplus G_2 \\oplus \\cdots \\oplus G_m, \\quad \\textrm{where } \\; G_i = \\langle a_i\\mid a_i^{d_i} = 1\\rangle \\quad (1\\leq i\\leq m).\\] Pick any alement \\(\\omega_i\\) of order \\(d_i\\) in \\(\\mathbb{C}^*\\), i.e., a primitive \\(d_i\\)-the root of \\(1\\). Define \\[\\theta_i: G \\to \\mathbb{C}^* \\quad (a_1^{\\varepsilon_1}\\cdots a_m^{\\varepsilon_m} \\mapsto \\omega_i^{\\varepsilon_i} \\quad \\textrm{where }\\; 0\\leq \\varepsilon_i &lt; d_i, 1\\leq i\\leq m).\\] Then \\(\\theta_i\\in X(G)\\). (Exercise) Claim: There exists an isomorphism of groups \\(G \\to X(G)\\) that sends \\(a_i\\) to \\(\\theta_i\\). Observe: \\(\\theta_i^{d_i} = 1\\). For every \\(g = a_1^{\\varepsilon_1}\\cdots a_m^{\\varepsilon_m} \\in G\\), \\[\\theta_i^{d_i}(g) = (\\theta_i(g))^{d_i} = (\\omega_i^{\\varepsilon_i})^{d_i} = (\\omega_i^{d_i})^{\\varepsilon_i} = 1.\\] Observe: If \\(\\theta_1^{\\varepsilon_1}\\theta_2^{\\varepsilon_2}\\cdots \\theta_m^{\\varepsilon_m} = 1\\) for some \\(0\\leq \\varepsilon_i &lt; d_i, 1\\leq i\\leq m\\). Then \\(\\varepsilon_1 = \\varepsilon_2 = \\cdots = \\varepsilon_m = 0\\). Pf. \\(1 = \\theta_1^{\\varepsilon_1}\\theta_2^{\\varepsilon_2}\\cdots \\theta_m^{\\varepsilon_m}(a_i) = \\omega_i^{\\varepsilon_i}\\), Since \\(\\omega_i\\) is a primitive \\(d_i\\)-th root of \\(1\\), \\(\\varepsilon_i = 0\\) for \\(1\\leq i\\leq m\\). Observe: \\(\\theta_1, \\ldots, \\theta_m\\) generate \\(X(G)\\). Pick \\(\\theta\\in X(G)\\). Since \\(a_i^{d_i} = 1\\), \\(1 = \\theta(a_i^{d_i}) = \\theta(a_i)^{d_i}\\). Hence \\(\\theta(a_i) = \\omega^{\\varepsilon_i}\\) for some \\(\\varepsilon_i\\) with \\(0\\leq \\varepsilon_i &lt; d_i\\). Now \\(\\theta = \\theta_1^{\\varepsilon_1}\\cdots \\theta_m^{\\varepsilon_m}\\), since these are both equal to \\(\\omega_i^{\\varepsilon_i}\\) at \\(a_i\\) for \\(1\\leq i \\leq m\\). Therefore, \\[G \\to X(G) \\quad (a_i \\mapsto \\theta_i)\\] is an isomorphism of groups. Note. The correspondence above is clearly a group homomorphism. "],["lec4.html", "Chapter 4 Examples", " Chapter 4 Examples Wednesday, January 27, 1993 Theorem 4.1 Given a Cayley graph \\(\\Gamma = \\Gamma(G, \\Delta)\\). View the standard module \\(V \\equiv \\mathbb{C}G\\) (the group algebra), so \\[\\left\\langle \\sum_{g\\in G}\\alpha_g g, \\;\\sum_{g\\in G}\\beta_g g\\right\\rangle = \\sum_{g\\in G}\\alpha_g\\overline{\\beta_g}, \\quad \\textrm{with}\\; \\alpha_g, \\beta_g\\in \\mathbb{C}.\\] For any \\(\\theta\\in X(G)\\), write \\[\\hat{\\theta} = \\sum_{g\\in G}\\theta(g^{-1})g.\\] Then the following hold.   \\((i)\\) \\(\\langle \\hat{\\theta_1}, \\hat{\\theta_2}\\rangle = |G|\\) if \\(\\theta_1 = \\theta_2\\) and \\(0\\) othewise for \\(\\theta_1, \\theta_2\\in X(G)\\). In particular, \\(\\{\\hat{\\theta}\\mid \\theta\\in X(G)\\}\\) forms a basis for \\(V\\).   \\((ii)\\) \\(A\\hat{\\theta} = \\Delta_\\theta \\hat{\\theta}\\) for \\(\\theta \\in X(G)\\), where \\(A\\) is the adjacency matrix and \\[\\Delta_\\theta = \\sum_{g\\in \\Delta}\\theta(g).\\] In particular, the eigenvalues of \\(\\Gamma\\) are precisely \\[\\Delta_\\theta \\mid \\theta\\in X(G)\\}.\\] Proof. \\((i)\\) Claim: For every \\(\\theta \\in X(G)\\), let \\[s:= \\sum_{g\\in G}\\theta(g^{-1}) = \\begin{cases} |G| &amp; \\text{if }\\;\\theta = 1\\\\ 0 &amp; \\text{if } \\;\\theta \\neq 1. \\end{cases}\\] Pf. Clear if \\(\\theta =1\\). Let \\(\\theta \\neq 1\\). Then \\(\\theta(h)\\neq 1\\) for some \\(h\\in G\\). \\[s\\cdot \\theta(h) = \\left(\\sum_{g\\in G}\\theta(g^{-1})\\right)\\theta(h) = \\sum_{g\\in G}\\theta(g^{-1}h) = \\sum_{g&#39;\\in G}\\theta(g&#39;^{-1}) = s.\\] Since \\(\\theta(h)\\neq 1\\), \\(s = 0\\). Claim. \\(\\theta(g^{-1}) = \\overline{\\theta(g)}\\) for every \\(\\theta\\in X(G)\\) and every \\(g\\in G\\). Since \\(\\theta(g)\\in \\mathbb{C}\\) is a root of \\(1\\), \\[|\\theta(g)|^2 = \\theta(g)\\overline{\\theta(g)} = 1.\\] On the other hand, since \\(\\theta\\) is a homomorphism, \\[\\theta(g)\\theta(g^{-1}) = \\theta(1) = 1.\\] Hence \\(\\theta(g^{1}) = \\overline{\\theta(g)}\\). Now \\[\\begin{align} \\langle \\widehat{\\theta_1}, \\widehat{\\theta_2}\\rangle &amp; = \\sum_{g\\in G}\\theta_1(g^{-1})\\overline{\\theta_2(g^{-1})}\\\\ &amp; = \\sum_{g\\in G}\\theta_1(g^{-1})\\theta_2(g)\\\\ &amp; = \\sum_{g\\in G}\\theta_1\\theta_2^{-1}(g^{-1})\\\\ &amp; = \\begin{cases} |G| &amp; \\text{if}\\quad \\theta_1\\theta_2^{-1} = 1\\\\ 0 &amp; \\text{if} \\quad \\theta_1\\theta_2^{-1}\\neq 1. \\end{cases} \\end{align}\\] Since \\(|G| = |X(G)|\\) by Lemma 3.1, and \\(\\widehat{\\theta_i}\\)’s are orthogonal nonzero elements in \\(V\\), thet form a basis of \\(V\\).  \\((ii)\\) Let \\(\\Delta = \\{g_1, \\ldots, g_r\\}\\). Then \\[\\begin{align} A\\hat{\\theta} &amp; = A\\left(\\sum_{g\\in G}\\theta(g^{-1}g)\\right)\\\\ &amp; = \\sum_{g\\in G}\\theta(g^{-1})(gg_1 + \\cdots + gg_r) \\quad (\\Gamma(g) = \\{gg_1, \\ldots, gg_r\\})\\\\ &amp; = \\sum_{i = 1}^r \\left(\\sum_{g\\in G}\\theta(g^{-1})(gg_i)\\right)\\\\ &amp; = \\sum_{i=1}^r\\left(\\sum_{g\\in G}\\theta(g_ig_i^{-1}g^{-1})(gg_i)\\right)\\\\ &amp; = \\sum_{i = 1}^r\\left(\\sum_{g\\in G}\\theta(g_i)\\theta((gg_i)^{-1})gg_i\\right)\\\\ &amp; = \\sum_{i = 1}^r\\theta(g_i)\\sum_{h\\in G}\\theta(h^{-1})h \\\\ &amp; = \\Delta_\\theta\\cdot \\hat{\\theta}. \\end{align}\\] Since \\(\\{\\hat{\\theta}\\mid \\theta\\in X(G)\\}\\) forms a basis, the eigenvalues of \\(\\Gamma\\) are precisely, \\[\\{\\Delta_\\theta\\mid \\theta\\in X(G)\\}.\\] This completes the proof. Example 4.1 Let \\(G = \\langle a\\mid a^6 = 1\\rangle\\), and \\(\\Delta = \\{a, a^{-1}\\}\\). Pick a primitive 6-th root of 1, \\(\\omega\\). Then \\[X(G) = \\{\\theta^i\\mid 0\\leq i\\leq 5\\} \\quad \\text{such that }\\quad \\theta(a) = \\omega, \\; \\omega + \\omega^{-1} = 1.\\] \\[\\begin{array}{c | c | c} \\varphi\\in X(G) &amp; \\varphi(a) &amp; \\Delta_\\varphi = \\theta(a) + \\theta(a)^{-1}\\\\ \\hline 1 &amp; 1 &amp; 2\\\\ \\theta &amp; \\omega &amp; \\omega+\\omega^{-1} = 1\\\\ \\theta^2 &amp; \\omega^2 &amp; -1\\\\ \\theta^3 &amp; \\omega^3 = -1 &amp; -2\\\\ \\theta^4 &amp; \\omega^4 &amp; -1\\\\ \\theta^5 &amp; \\omega^5 &amp; 1 \\end{array}\\] \\[\\text{Spec}(\\Gamma) = \\begin{pmatrix} 2 &amp; 1 &amp; -1 &amp; -2\\\\ 1 &amp; 2 &amp; 2 &amp; 1\\end{pmatrix}.\\] Example 4.2 \\(D\\)-cube, \\(H(D,2)\\). Let \\[X = \\{(a_1, \\ldots, a_D)\\mid a_i\\in \\{1,-1\\}, \\; 1\\leq i\\leq D\\},\\] \\[E = \\{xy\\mid x, y\\in X, \\; x, y \\text{: different in exactly one coordinate}\\}.\\] Also \\(H(D,2)\\) is a Cayley graph \\(\\Gamma(G, \\Delta)\\), where \\[G = G_1\\oplus G_2 \\oplus \\cdots \\oplus G_D, \\] \\[G_i = \\langle a_i\\mid a_i^2 = 1\\rangle,\\quad \\Delta = \\{a_1, \\ldots, a_D\\}.\\] Homework: The spectrum of \\(H(D,2)\\) is \\[\\begin{pmatrix} \\theta_0 &amp; \\theta_1 &amp; \\cdots &amp; \\theta_D\\\\ m_0 &amp; m_1 &amp; \\cdots &amp; m_D\\end{pmatrix},\\] where \\[\\theta_i = D-2i \\quad (0\\leq i\\leq D), \\quad m_i = \\binom{D}{i}.\\] Remark. Let \\(\\theta \\in X(G)\\). Then \\(\\theta: X \\to \\{\\pm 1\\}\\). If \\[\\nu(\\theta) = |\\{i\\mid \\theta(a_i) = -1\\}|, \\] then \\(\\Delta_\\theta = D-2i\\). Since there are \\(\\binom{D}{i}\\) such \\(\\theta\\), we have te assertion. We want to compute the subconstituent algebra for \\(H(D,2)\\). First, we make a few observations about arbitrary graphd. Let \\(\\Gamma = (X,E)\\) be any graph, \\(A\\), the adjacemcy matrix of \\(\\Gamma\\), and \\(V\\), the standard module over \\(K = \\mathbb{C}\\). Fix a base \\(x\\in X\\). Write \\(E_i^* = E_i^*(x)\\), and \\[T \\equiv T(x) = \\text{the algebra generated by}\\; A, E_0^*, E_1^*, \\ldots .\\] Definition 4.1 Let \\(W\\) be any orreducible \\(T\\)-module (\\(\\subseteq V\\)). Then the endpoint \\(r \\equiv r(W)\\) satisfied \\[r = \\min\\{i\\mid E_i^*W \\neq 0\\}.\\] The diameter \\(d = d(W)\\) satisfied \\[d = |\\{i\\mid E_i^*W \\neq 0\\}| - 1.\\] Lemma 4.1 With the above notation, let \\(W\\) be an irreducible \\(T\\)-module. Then \\((i)\\) \\(E_i^*AE_j^* = 0 \\; \\text{ if }\\; |i-j|=1, \\quad \\neq 0 \\; \\text{ if }\\; |i-j| = 1, \\quad 0\\leq i,j\\leq d(x)\\). \\((ii)\\) \\(AE_j^*W \\subseteq E_{j-1}^*W + E_j^*W + E^*_{j+1}W\\), \\(0\\leq j \\leq d(x)\\). \\((E_i^*W = 0 \\; \\text{ if } i&lt;j\\) or \\(i &gt; d(x)\\).) \\((iii)\\) \\(E^*_jW \\neq 0\\) if \\(r\\leq j \\leq r+d\\), \\(=0\\) if \\(0\\leq j\\leq r\\) or \\(r+d &lt; j \\leq d(x)\\). \\((iv)\\) \\(E_i^*AE^*_jW \\neq 0\\), if \\(|i-j| = 1\\) \\((r \\leq i,j \\leq r+d)\\). Proof. \\((i)\\) Pick \\(y\\in X\\) with \\(\\partial(x,y) = j\\). We want to find \\(E_i^*AE^*_j \\hat{y}\\). Note, \\[E_j^*\\hat{y} = \\begin{cases} 0 &amp; \\text{if }\\; \\partial(x.y)\\neq j\\\\ \\hat{y} &amp; \\text{if }\\; \\partial(x,y) = j.\\end{cases}.\\] \\[\\begin{align} E_i^*AE_j^*\\hat{y} &amp;= E_i^*A\\hat{y} \\\\ &amp; = E_i^*\\sum_{z\\in X, yz\\in E}\\hat{z}\\\\ &amp; = \\sum_{z\\in X, yz\\in E, \\partial(x, z) = i}\\hat{z} \\qquad (*)\\\\ &amp; = 0 \\; \\text{ if }\\; |i-j|&gt;1 \\; \\text{by triangle inequality.} \\end{align}\\] If \\(|i-j| = 1\\), there exist \\(y, y&#39;\\in X\\) such that \\(\\partial(x,y) = j\\), \\(\\partial(x,y&#39;) = i\\), \\(yy&#39;\\in E\\) by connectivity of \\(\\Gamma\\). Hence (*) contains \\(\\widehat{y&#39;}\\) and \\(* \\neq 0\\). \\((ii)\\) We have \\[\\begin{align} AE_j^*W &amp; = \\left(\\sum_{i=0}^{d(x)}E_i^*\\right)AE_j^*W\\\\ &amp; = E_{j-1}^*AE^*_jW + E^*_jAE_j^*W + E^*_{j+1}AE_j^*W\\\\ &amp; \\subseteq E^*_{j-1}W + E^*_jW + E^*_{j+1}W. \\end{align}\\] \\((iii)\\) Suppose \\(E_j^*W = 0\\) for some \\(j\\) \\((r\\leq j \\leq r+d)\\). Then \\(r &lt; j\\) by the definition of \\(r\\). Set \\[\\tilde{W} = E^*_iW + E^*_{r+1}W + \\cdots + E^*_{j-1}W.\\] Observe \\(0\\subsetneq \\tilde{W} \\subsetneq W\\). Also \\(A\\tilde{W} \\subseteq \\tilde{W}\\) by \\((ii)\\) and \\(E_i^*\\tilde{W} \\subseteq \\tilde{W}\\) for every \\(i\\) by construction. Thus \\(T\\tilde{W} \\subseteq \\tilde{W}\\), contradicting \\(W\\) beging irreducible. "],["lec5.html", "Chapter 5 \\(T\\)-Modules of \\(H(D,2)\\), I", " Chapter 5 \\(T\\)-Modules of \\(H(D,2)\\), I Friday, January 29, 1993 Let \\(\\Gamma = (X, E)\\) be a graph, \\(A\\) the adjacency matrix, and \\(V\\) the standard module over \\(K = \\mathbb{C}\\). Fix a base \\(x\\in X\\) and write \\(E_I^* \\equiv E_i^*(x)\\), and \\(T \\equiv T(x)\\). Let \\(W\\) be an irreducible \\(T\\)-module with endpoint \\(r:= \\min\\{i\\mid E_i^*W \\neq 0\\}\\) and diameter \\(d:=|\\{i\\mid E_i^*W\\neq 0\\}|-1\\). We have \\[\\begin{align} E_i^*W &amp; \\neq 0 &amp; r\\leq i \\leq r+d\\\\ &amp; = 0 &amp; 0 \\leq i &lt; r \\;\\text{ or }\\; r+d &lt; i \\leq d(x). \\end{align}\\] Claim: \\(E_i^*AE_j^*W \\neq 0\\) if $-j| = 1 for \\(r\\leq i,j\\leq r+d\\). (See Lemma 4.1.) Suppose \\(E_{j+1}^*AE_j^*W = 0\\) for some \\(j\\) with \\(r \\leq j &lt; r+d\\). Observe that \\[\\tilde{W} = E^*_rW + \\cdot E^*_jW\\] is \\(T\\)-invariant with \\[0 \\subsetneq \\tilde{W} \\subsetneq W.\\] Becase \\(A\\tilde{W} \\subseteq \\tilde{W}\\) since \\(AE_j^*W \\subseteq E^*_{j-1}W + E^*_jW\\), \\[E_k^*\\tilde{W} \\subseteq \\tilde{W} \\quad\\text{for all }\\; k,\\] we have \\(T\\tilde{W} \\subseteq{W}\\). Suppose \\(E_{i-1}^*AE_i^*W = 0\\) for some \\(i\\) with \\(r \\leq i &lt; r+d\\). Similarly, \\[\\tilde{W} = E^*_iW + \\cdot E^*_{r+d}.W\\] is a \\(T\\)-module with \\(0\\subsetneq \\tilde{W} \\subsetneq W\\). Definition 5.1 Let \\(\\Gamma\\), \\(E^*_i\\), and \\(T\\) be as above. Irreducible \\(T\\)-modules \\(W\\) and \\(W&#39;\\) are isomorphic whenever there is an isomorphism \\(\\sigma: W \\to W&#39;\\) of vector spaces such that \\(a\\sigma = \\sigma a\\) for all \\(a\\in T\\). Recall that the standard module \\(V\\) is an orthogonal direct sum of irreducible \\(T\\)-modules \\(W_1 \\oplus W_2 \\oplus \\cdots\\). Given \\(W\\) in this list, the multiplicity of \\(W\\) in \\(V\\) is \\[|\\{j \\mid W_j \\simeq W\\}|.\\] Remark. It is known that the multiplicity does not depend on the decomposition. Now assume that \\(\\Gamma\\) is the \\(D\\)-cube, \\(H(D,2)\\) with \\(D\\geq 1\\). Vew \\[\\begin{align} X &amp; = \\{a_1\\cdots a_D\\mid a_i\\in \\{1, -1\\}, 1\\leq i\\leq D\\},\\\\ E &amp; = \\{xy\\mid x, y\\in X, \\; x, y \\;\\text{ differ in exactly 1 coordinate.}\\}. \\end{align}\\] Find \\(T\\)-modules. Claim: \\(H(D,2)\\) is bipartite with a partition \\(X = X^+ \\cup X^-\\), where \\[\\begin{align} X^+ &amp; = \\{a_1\\cdots a_D\\in X\\mid \\prod a_i &gt; 0\\}\\\\ X^- &amp; = \\{a_1\\cdots a_D \\in X \\mid \\prod a_i &lt; 0\\} \\end{align}\\] Observe: for all \\(y, z\\in X\\), \\[\\partial(y,z) = i \\Leftrightarrow y, z \\; \\text{ differ in exactly in }\\; i\\; \\text{ coorinates with }\\; 0\\leq i\\leq D.\\] Here, the diameter of \\(H(D, 2) = D = d\\) for all \\(x\\in X\\). Theorem 5.1 Let \\(\\Gamma = H(D,2)\\) be as above. Fix \\(x\\in X\\), and write \\(E_i^* = E^*_i(x)\\), and \\(T = T(x)\\). Let \\(W\\) be an irreducible \\(T\\)-module with endpoint \\(r\\), and diameter \\(d\\) with \\(0\\leq r \\leq r+d\\leq D\\). \\((i)\\) \\(W\\) has a basis \\(w_0, w_1, \\ldots, w_d\\) with \\(w_i\\in E^*_{i+r}W\\) for \\(0\\leq i\\leq d\\). With respect to which the matrix representing \\(A\\) is \\[ \\begin{pmatrix} 0 &amp; d &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; 0\\\\ 1 &amp; 0 &amp; d-1 &amp; \\cdots &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 2 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 3 &amp; \\cdots &amp; 0 &amp; 0 &amp; 0\\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 2 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; d-1 &amp; 0 &amp; 1\\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; d &amp; 0 \\end{pmatrix} \\] \\((ii)\\) $ d= D - 2r$. In particular, \\(0\\leq r\\leq D/2\\). \\((iii)\\) Let \\(W&#39;\\) denote an irreducible \\(T\\)-module with endpoint \\(r&#39;\\). Then \\(W\\) and \\(W&#39;\\) are isormorphic as \\(T\\)-modules if and only if \\(r = r&#39;\\). \\((iv)\\) The multiplicity of the irreducible \\(T\\)-module with endpoint \\(r\\) is \\[\\binom{D}{r} - \\binom{D}{r-1} \\quad \\text{if } 1\\leq r \\leq R/2,\\] and \\(1\\) if \\(r = 0\\). Proof. Recall that \\(\\Gamma\\) is vertex transitive. It is a Cayley graph. Hence without loss of generality, we may assume that \\(x = \\overbrace{11\\cdots 1}^{D}\\). Notation: Set \\(\\Omega = \\{1, 2, \\ldots, D\\}\\). For every subset \\(S \\subseteq \\Omega\\), let \\[\\hat{S} = a_1\\cdot a_d \\in X \\quad a_i = \\begin{cases} -1 &amp; \\text{if }\\; i\\in S\\\\ 1 &amp; \\text{if } i\\not\\in S.\\end{cases}\\] In particular, \\(\\hat{emptyset} = x\\) and \\[|S| = i \\Leftrightarrow \\partial(x, \\hat{S}) = i \\Leftrightarrow \\hat{S}\\in E^*_iV.\\] For all \\(S, T\\subseteq \\Omega\\), we say \\(S\\) covers T$ if and only if \\(S\\supseteq T\\) and \\(|S| = |T| +1\\). Observe that \\(\\hat{S}, \\hat{T}\\) are adjacent in \\(\\Gamma\\) if and only if either \\(T\\) coverse \\(S\\) or \\(S\\) coverr \\(T\\). Define the ‘raising matrix’ \\[R = \\sum_{i=0}^D E^*_{i+1}AE^*_i.\\] Observe that \\[RE_i^*V \\subseteq E^*_{i+1}V \\; \\text{ for }\\; 0\\leq i \\leq D, \\; \\text{ and }E^*_{D+1}V = 0.\\] Indeed for any \\(S\\subseteq \\Omega\\) with \\(|S| = i\\), \\[\\begin{align} R\\hat{S} &amp; = RE^*_i\\hat{S} \\\\ &amp; = E^*_{i+1}A\\hat{S} \\\\ &amp; = \\sum_{T_1 \\subseteq \\Omega, S \\text{ covers }T_1} E^*_{i+1}\\widehat{T_1} + \\sum_{T \\subseteq \\Omega, T \\text{ covers }S} E^*_{i+1}\\hat{T}\\\\ &amp; = \\sum_{T \\subseteq \\Omega, T \\text{ covers }S} E^*_{i+1}\\hat{T}. \\end{align}\\] Define the ‘lowering matrix’ \\[L = \\sum_{i=0}^D E^*_{i-1}AE^*_i.\\] Observe that \\[LE_i^*V \\subseteq E^*_{i-1V} \\; \\text{ for }\\; 0\\leq i \\leq D, \\; \\text{ and }E^*_{-1}V = 0.\\] Indeed for any \\(S\\subseteq \\Omega\\), \\[L\\hat{S} = \\sum_{T\\subseteq \\Omega, S \\text{ covers }T} \\hat{T}.\\] Observe that \\(A = L + R\\). For convenience, set \\[A^* = \\sum_{i=0}^D (D-2i)E_i^*.\\] Claim: The following hold. \\((a)\\) \\(LR - RL = A^*\\). \\((b)\\) \\(A*L - LA^* = 2L\\). \\((c)\\) \\(A^*R - RA^* = -2R\\). In particular \\(\\mathrm{Span}(R,L, A^*)\\) is a ’representation of Lie algebra \\(\\mathrm{sl}_2(\\mathbb{C})\\). Remark (Lie Algebra sl2(C)). \\[\\mathrm{sl}_2(\\mathbb{C}) = \\{X\\mid \\mathrm{Mat}(\\mathbb{C} \\mid \\mathrm{tr}(X) = 0\\}.\\] For \\(X, Y\\in \\mathrm{sl}_2(\\mathbb{C})\\), define a binary operation \\([X, Y] = XY - YX\\). \\[A^*\\sim \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; -1\\end{pmatrix}, \\quad L\\sim \\begin{pmatrix} 0 &amp; 1 \\\\ 0 &amp; 0\\end{pmatrix}, \\quad R\\sim \\begin{pmatrix} 0 &amp; 0 \\\\ 1 &amp; 0\\end{pmatrix}.\\] Then these satisfy the relations \\((a)\\) - \\((c)\\) above. Proof of Claim. Apply both sides to \\(\\hat{S}\\) \\((S\\subseteq \\Omega)\\). Say \\(|S| = i\\). Proof of \\((a)\\): \\[\\begin{align} (LR - RL)\\hat{S} &amp; = L\\left(\\sum_{\\substack{T \\subseteq \\Omega, T \\text{ covers }S\\\\(D-i \\text{ of them})}}\\hat{T}\\right) - R \\left(\\sum_{\\substack{U \\subseteq \\Omega, S \\text{ covers }U\\\\(i \\text{ of them})}}\\hat{T}\\right)\\\\ &amp; = (D-i)\\hat{S} + \\sum_{V \\subseteq \\Omega, |V| = i, |S\\cap V| = i-1}\\hat{V} - \\left(i\\hat{S} + \\sum_{V \\subseteq \\Omega, |V| = i, |S\\cap V| = i-1}\\hat{V}\\right)\\\\ &amp; = (D-2i)\\hat{S}\\\\ &amp; = A^*\\hat{S}. \\end{align}\\] Proof of \\((b)\\): \\[\\begin{align} (A^*L - LA^*)\\hat{S} &amp; = (D-2(i-1))L\\hat{S} - (D-2i)L\\hat{S} \\quad (\\text{since} \\; L\\hat{S}\\in E^*_{i-1}V)\\\\ &amp; = 2L\\hat{S}. \\end{align}\\] Proof of \\((c)\\): \\[\\begin{align} (A^*R - RA^*)\\hat{S} &amp; = (D-2(i+1))R\\hat{S} - (D-2i)R\\hat{S} \\quad (\\text{since} \\; R\\hat{S}\\in E^*_{i+1}V)\\\\ &amp; = 2R\\hat{S}. \\end{align}\\] Let \\(W\\) be an irreducible \\(T\\)-module with endpoint \\(r\\) and diameter \\(d\\) \\((0\\leq r \\leq r+d \\leq D)\\). Proof of \\((i)\\) and \\((ii)\\): Pick \\(0\\neq w \\in E^*_rW\\). Claim: \\(LRw = (D-2r)w\\). Pf. \\[\\begin{align} LRw &amp; = (A^*+RL)w \\quad (\\text{by Claim }(a))\\\\ &amp; = A^*w \\quad (Lw \\in E^*_{r-1}W = 0)\\\\ (D-2r)w. \\end{align}\\] Define \\[w_i = \\frac{1}{i!}R^iw \\in E^*_{r+i}W \\quad (0\\leq i \\leq d).\\] Then, \\[\\begin{align} Rw_i &amp; = (i+1)w_{i+1}\\quad (0\\leq i \\leq d)\\\\ Rw_d &amp; = 0 \\quad (\\text{by definition of }d) \\end{align}\\] Claim: \\(Lw_0 = 0\\) and \\[Lw_i = (D-2r-i+1)w_{i-1} \\quad (1\\leq i\\leq d).\\] Pf. We prove by induction on \\(i\\). The case \\(i=0\\) is trivial, and the case \\(i=1\\) follows from above claim. Let \\(i\\geq 2\\), \\[\\begin{align} Lw_i &amp; = \\frac{1}{i}LRw_{i-1} = \\frac{1}{i}(A^*+RL)w_{i-1} \\quad (\\text{by Claim (a)})\\\\ &amp; \\quad \\text{(by induction hypothesis)}\\\\ &amp; = \\frac{1}{i}((D-2(r+i-1))w_{i-1} + (D-2r-(i-1)+1)Rw_{i-2} \\quad (Rw_{i-2} = (i-1)w_{i-1})\\\\ &amp; = \\frac{1}{i}i(D-2r-i+1)w_{i-1}\\\\ &amp; = (D-2r-i+1)w_{i-1}. \\end{align}\\] Claim: \\(w_0, \\ldots, w_d\\) is a basis for \\(W\\). Pf. Let \\(W&#39; = \\mathrm{Span}\\{w_0, \\ldots, w_d\\}\\). Then \\(W&#39;\\) is \\(R\\) and \\(L\\) invariant. So it is \\(A = R+L\\) invariant. Also it is \\(E^*_i\\)-invariant for every \\(i\\). Hence \\(W&#39;\\) is a \\(T\\)-module. Since \\(W\\) is irreducible, \\(W&#39; = W\\). As \\(w_i\\)’s are orthogonal, theyy are linearly independent. Note that \\(w_i\\neq 0\\) by the definition of \\(d\\) and Lemma 4.1 \\((iv)\\). Claim: \\(d = D-2r\\). Pf. By \\((a)\\), \\[\\begin{align} 0 &amp; = (LR - RL - A^*)w_d \\\\ &amp; = 0 - (D-2r-d+1)Rw_{d-1} - (D-2(r+d))w_d\\\\ &amp; = -d(D-2r-d+1)w_d - (D-2(r+d))w_d\\\\ &amp; = (-dD + 2rd + d^2 - d - D + 2r + 2d)w_d\\\\ &amp; = (d^2 + (2r-D+1)d + 2r - D)w_d\\\\ &amp; = (d+2r-D)(d+1)w_d. \\end{align}\\] Hence \\(d = D-2r\\). Therefore, with respect to a bais \\(w_0, w_1, \\ldots, w_d\\), \\(A = L+R\\), \\(w_{-1} = w_{d+1} = 0\\), \\[Lw_i = (d-i+1)w_{i-1}, \\quad Rw_i = (i+1)w_{i+1}.\\] \\[L = \\begin{pmatrix} 0 &amp; d &amp; 0 &amp; \\cdots &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; d-1 &amp; \\cdots &amp; 0 &amp; 0\\\\ &amp; &amp; \\cdots &amp; \\cdots &amp; &amp; \\\\ &amp; &amp; &amp; &amp; 0 &amp; 1\\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\end{pmatrix}, \\qquad R = \\begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0\\\\ 1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0\\\\ 0 &amp; 2 &amp; 0 &amp; \\cdots &amp; &amp; \\\\ &amp; &amp; &amp; &amp; 0 &amp; 1\\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; d &amp; 0 \\end{pmatrix}.\\] This completes the proof of \\((i)\\) and \\((ii)\\). "],["lec6.html", "Chapter 6 \\(T\\)-Modules of \\(H(D,2)\\), II", " Chapter 6 \\(T\\)-Modules of \\(H(D,2)\\), II Monday, February 1, 1993 Proof of Theorem ?? Continued \\((iii)\\) Let \\(r = r&#39;\\) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
