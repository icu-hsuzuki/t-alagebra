[["index.html", "Lecture Note on Terwilliger Algebra About this lecturenote Setting Another Host", " Lecture Note on Terwilliger Algebra P. Terwilliger, edited by H. Suzuki 2022-11-24 About this lecturenote Setting sudo This note is created by bookdown package on RStudio. For bookdown See (Xie 2015), (Xie 2017), (Yihui Xie 2018). Log-in to my GitHub Account Go to RStudio/bookdown-demo repository: https://github.com/rstudio/bookdown-demo Use This Template Input Repository Name Select Public - default Create repository from template From Code download ZIP Move the extracted folder into a favorite directory Open RStudio Project in the folder Use Terminal in the buttom left pane confirm that the current directory is the home directry of the project by pwd (failed to proceed by ssh) Use Console library(usethis) use_git() use_github() — Error gh_token_help() create_github_token(): create a token in the github page. Copy the token gitcreds::gitcreds_set(): paste the token, the token is to be expired in 30 days Use Terminal git remote add origin https://github.com/icu-hsuzuki/t-alagebra.git git push -u origin main type in the password of the computer Use GIT in R Studio Another Host create a project by version control git git init git remote add origin git@github.com:/.git git branch -r git fetch git pull origin main References "],["lec1.html", "Chapter 1 Subconstituent Algebra of a Graph", " Chapter 1 Subconstituent Algebra of a Graph Wednesday, January 20, 1993 A graph (undirected, without loops or multiple edges) is a pair \\(\\Gamma = (X, E)\\), where \\[\\begin{align} X &amp;= \\textrm{finite set (of vertices)}\\\\ E &amp; = \\textrm{set of (distinct) 2-element subsets of }X \\textrm{ (= edges of ) }\\Gamma. \\end{align}\\] vertices \\(x\\) and \\(y\\in X\\) are adjacent if and only if \\(xy\\in E\\). Example 1.1 Let \\(\\Gamma\\) be a graph. \\(X = \\{a, b, c, d\\}\\), \\(E = \\{ab, ac, bc, bd\\}\\). Set \\(n = |X|\\), the order of \\(\\Gamma\\). Pick a field \\(K\\) (\\(=\\mathbb{R}\\) or \\(\\mathbb{C}\\)). Then \\(\\mathrm{Mat}_X(K)\\) denotes the \\(K\\) algebra of all \\(n\\times n\\) matrices with entries in \\(K\\). (rows and columns are indexed by \\(X\\)) Adjacency matrix \\(A\\in \\mathrm{Mat}_X(K)\\) is defined by \\[\\begin{align} A_{xy} &amp; = \\left\\{\\begin{array}{cl} 1 &amp; \\textrm{ if } \\; xy\\in E\\\\ 0 &amp; \\textrm{ else } . \\end{array}\\right. \\end{align}\\] Example 1.2 Let \\(a, b, c, d\\) be labels of rows and columns. Then \\[A = \\begin{matrix} \\\\ a\\\\ b\\\\c\\\\d\\end{matrix}\\begin{matrix}\\begin{matrix} a &amp; b &amp; c &amp; d \\end{matrix}\\\\\\begin{pmatrix} 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\end{pmatrix}\\end{matrix}\\] The subalgebra \\(M\\) of \\(\\mathrm{Mat}_X(K)\\) generated by \\(A\\) is called the Bose-Mesner algebra of \\(\\Gamma\\). Set \\(V = K^n\\), the set of \\(n\\)-dimensional column vectors, the coorinates are indexed by \\(X\\). Let \\(\\langle\\; , \\;\\rangle\\) denote the Hermitean inner product: \\[\\langle u, v\\rangle = u^\\top\\cdot v \\quad (u, v\\in V)\\] \\(V\\) with \\(\\langle\\; , \\;\\rangle\\) is the standard module of \\(\\Gamma\\). \\(M\\) acts on \\(V\\): For every \\(x\\in X\\), write \\[\\hat{x} = \\begin{pmatrix} 0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}\\] where \\(1\\) is at the \\(x\\) position. Then \\[A\\hat{x} = \\sum_{y\\in X, xy\\in E}\\hat{y}.\\] Since \\(A\\) is a real symmetrix matrix, \\[V = V_0 + V_1 + \\cdots + V_r \\quad \\textrm{ some } r\\in \\mathbb{Z}^{\\geq0},\\] the orthogonal direct sum of maximal \\(A\\)-eigenspaces. Let \\(E_i\\in\\mathrm{Mat}_X(K)\\) denote the orthogonal projection, \\[E_i: V \\longrightarrow V_i.\\] Then \\(E_0, \\ldots, E_r\\) are the primitive idempotents of \\(M\\). \\[M = \\mathrm{Span}_K(E_0, \\ldots, E_r),\\] \\[E_iE_j = \\delta_{ij}E_i \\quad \\textrm{for all }\\; i, j, \\quad E_0 + \\cdots + E_r = I.\\] Let \\(\\theta_i\\) denote the eigenvalue of \\(A\\) for \\(V_i\\) in \\(\\mathbb{R}\\). Without loss of generality we may assume that \\[\\theta_0 &gt; \\theta_1 &gt; \\cdots &gt; \\theta_r.\\] Let \\[m_i = \\textrm{the multiplicity of }\\: \\theta_i = \\mathrm{dim} V_i = \\mathrm{rank} E_i.\\] Set \\[\\mathrm{Spec}(\\Gamma) = \\begin{pmatrix} \\theta_0, &amp; \\theta_1, &amp; \\cdots, &amp; \\theta_r\\\\m_0, &amp; m_1, &amp; \\cdots, &amp; m_r\\end{pmatrix}.\\] Problem. What can we say about \\(\\Gamma\\) when \\(\\mathrm{Spec}(\\Gamma)\\) is given? The following Lemma1.1, is an example of Problem. For every \\(x\\in X\\), \\[k(x) \\equiv \\textrm{ valency of }x \\equiv \\textrm{ degree of }x \\equiv |\\{y\\mid y\\in X, \\: xy\\in E\\}|.\\] Definition 1.1 The graph \\(\\Gamma\\) is regular of valency \\(k\\) if \\(k = k(x)\\) for every \\(x\\in X\\). Lemma 1.1 With the above notation, \\((i)\\) \\(\\theta_0\\leq \\max\\{k(x) \\mid x\\in X\\} = k^{\\max}\\). \\((ii)\\) If \\(\\Gamma\\) is regular of valency \\(k\\), then \\(\\theta_0 = k\\). Proof. \\((i)\\) Without loss of generality we may assume that \\(\\theta_0&gt;0\\), else done. Let \\(v:=\\sum_{x\\in X}\\alpha_x\\hat{x}\\) denote the eivenvector for \\(\\theta_0\\). Pick \\(x\\in X\\) with \\(|\\alpha_x|\\) maximal. Then \\(|\\alpha_x|\\neq 0\\). Since \\(Av = \\theta_0v\\), \\[\\theta_0\\alpha_x = \\sum_{y\\in X, xy\\in E}\\alpha_y.\\] So, \\[\\theta_0 |\\alpha_x| = |\\theta_0\\alpha_x| \\leq \\sum_{y\\in X, xy\\in E}|\\alpha_y| \\leq k(x)|\\alpha_x| \\leq k^{\\max}|\\alpha_x|.\\] \\((ii)\\) All 1’s vector \\(v = \\sum_{x\\in X}\\hat{x}\\) satisfies \\(Av = kv\\). Subconstituent Algebra Let \\(x, y\\in X\\) and \\(\\ell \\in \\mathbb{Z}^{\\geq 0}\\). Definition 1.2 A path of length \\(\\ell\\) connecting \\(x, y\\) is a sequence \\[x = x_0, x_1, \\ldots, x_{\\ell} = y, \\quad x_i\\in X, \\; 0\\leq i\\leq \\ell\\] such that \\(x_ix_{i+1}\\in E\\) for \\(0\\leq i \\leq \\ell-1\\). Definition 1.3 The distance \\(\\partial(x,y)\\) is the length of a shortest path connecting \\(x\\) and \\(y\\). \\[\\partial(x,y) \\in \\mathbb{Z}^{\\geq 0} \\cup \\{\\infty\\}.\\] Definition 1.4 The graph \\(\\Gamma\\) is connected if and only if \\(\\partial(x,y) &lt; \\infty\\) for all \\(x, y\\in X\\). From now on, assume that \\(\\Gamma\\) is connected with \\(|X|\\geq 2\\). Set \\[d_\\Gamma = d = \\max\\{\\partial(x,y)\\mid x, y\\in X\\} \\equiv \\textrm{the diameter of }\\Gamma.\\] Fix a ‘base’ vertex \\(x\\in X\\). Definition 1.5 \\[d(x) = \\textrm{the diameter with respect to }x = \\max\\{\\partial(x,y)\\mid y\\in X\\} \\leq d.\\] Observe that \\[V = V_0^* + V_1^* + \\cdots + V_{d(x)}^* \\quad \\textrm{(orthogonal direct sum)},\\] where \\[V_i^* = \\mathrm{Span}_K(\\hat{y}\\mid \\partial(x,y) = i) \\equiv V_i*(x)\\] and \\(V_i^* = V_i^*(x)\\) is called the \\(i\\)-the subconstituent with respect to \\(x\\). Let \\(E_i^* = E_i^*(x)\\) denote the orthogonal projection \\[E_i^*: V \\longrightarrow V_i^*(x).\\] View \\(E_i^*(x) \\in \\mathrm{Mat}_X(K)\\). So, \\(E_i^*(x)\\) is diagonal with \\(yy\\) entry \\[(E_i^*(x))_{yy} = \\begin{cases} 1 &amp; \\textrm{if } \\: \\partial(x,y) = i\\\\ 0 &amp; \\textrm{else,}\\end{cases} \\quad \\textrm{ for } y\\in X.\\] Set \\[M^* = M^*(x) \\equiv \\textrm{Span}_K(E_0^*(x), \\ldots, E_{d(x)}^*(x)).\\] Then \\(M^*(x)\\) is a commutative subalgebra of \\(\\mathrm{Mat}_X(K)\\) and is calle the dual Bose-Mesner algbara with respect to \\(x\\). Definition 1.6 (Subconstituent Algebra) Let \\(\\Gamma = (X, E)\\), \\(x\\), \\(M\\), \\(M^*(x)\\) be as above. Let \\(T = T(x)\\) denote the subalgebra of \\(\\mathrm{Mat}_X(K)\\) generated by \\(M\\) and \\(M^*(x)\\). \\(T\\) is the subconstituent algebra of \\(\\Gamma\\) with respect to \\(x\\). Definition 1.7 A \\(T\\)-module is any subspace \\(W\\subset V\\) such that \\(aw\\in W\\) for all \\(a\\in T\\) and \\(w\\in W\\). \\(T\\)-module \\(W\\) is irreducible if and only if \\(W\\neq 0\\) and \\(W\\) does not properly contain a nonzero \\(T\\)-module. For any \\(a\\in \\mathrm{Mat}_X(K)\\), let \\(a^*\\) denbote the conjugate transpose of \\(a\\). Observe that \\[\\langle au, v\\rangle = \\langle u, a^*v\\rangle \\quad \\textrm{for all }\\; a\\in \\mathrm{Mat}_X(K), \\textrm{ and for all } \\; u,v\\in V.\\] Lemma 1.2 Let \\(\\Gamma = (X,E)\\), \\(x\\in X\\) and \\(T \\equiv T(x)\\) be as above. \\((i)\\) If \\(a\\in T\\), then \\(a^*\\in T\\). \\((ii)\\) For any \\(T\\)-module \\(W\\subset V\\), \\[W^\\bot := \\{v\\in V\\mid \\langle w, v\\rangle = 0, \\textrm{ for all }w\\in W\\}\\] is a \\(T\\)-module. \\((iii)\\) \\(V\\) decomposes as an orthogonal direct sum of irreducible \\(T\\)-modules. Proof. \\((i)\\) It is becase \\(T\\) is generated by symmetric real matrices \\[A, E^*_0(x), E^*_1(x), \\ldots, E^*_{d(x)(x)}.\\] \\((ii)\\) Pick \\(v\\in W^\\bot\\) and \\(a\\in T\\), it suffices to show that \\(av\\in W^\\bot\\). For all \\(w\\in W\\), \\[\\langle w, av\\rangle = \\langle a^*w, v\\rangle = 0\\] as \\(a^*\\in T\\). \\((iii)\\) This is proved by the induction on the dimension of \\(T\\)-modules. If \\(W\\) is an irreducible \\(T\\)-module of \\(V\\), then \\[V = W + W^\\bot \\quad \\textrm{(orthogonal direct sum)}.\\] Problem. What does the structure of the \\(T(x)\\)-module tell us about \\(\\Gamma\\)? Study those \\(\\Gamma\\) whose modules take ‘simple’ form. The \\(\\Gamma\\)’s involved are highly regular. Remark. The subconstituent algebra \\(T\\) is semisimple as the left regular representation of \\(T\\) is completely reducible. See Curtis-Reiner 25.2 (Charles W. Curtis 2006). The inner product \\(\\langle a, b\\rangle_T = \\mathrm{tr}(a^\\top\\bar{b})\\) is nondegenerate on \\(T\\). In general, \\[\\begin{align*} T\\textrm{: Semisimple and Artinian} &amp; \\Leftrightarrow T\\textrm{: Artinian with } J(T) = 0 \\\\ &amp; \\Leftarrow T\\textrm{: Artinian with nonzero nilpotent element} \\\\ &amp; \\Leftarrow T \\subset \\mathrm{Mat}_X(K) \\textrm{ such that for all } a\\in T \\textrm{ is normal.} \\end{align*}\\] References "],["lec2.html", "Chapter 2 Perron-Frobenius Theorem", " Chapter 2 Perron-Frobenius Theorem Friday, January 22, 1993 In this lecture we use the Perron Frobenius theory of nonnegative matrices to obtain informaiton on eigenvalues of a graph. Let \\(K = \\mathbb{R}\\). For \\(n\\in \\mathbb{Z}^{&gt; 0}\\), pick a symmetrix matrix \\(C\\in \\mathrm{Mat}_n(\\mathbb{R})\\). Definition 2.1 The matrix \\(C\\) is reducible if and only if there is a bipartition \\(\\{1, 2, \\ldots, n\\} = X^+ \\cup X^-\\) (disjoint union of nonempty sets) such that \\(C_{ij} = 0\\) for all \\(i\\in X^+\\), and for all \\(j\\in X^-\\), and for all \\(i\\in X^-\\), and for all \\(j\\in X^+\\),i.e., \\[ C \\sim \\begin{pmatrix} \\ast &amp; O \\\\ O &amp; \\ast \\end{pmatrix}.\\] Definition 2.2 The matrix \\(C\\) is bipartite if and only if there is a bipartition \\(\\{1, 2, \\ldots, n\\} = X^+ \\cup X^-\\) (disjoint union of nonempty sets) such that \\(C_{ij} = 0\\) for all \\(i,j\\in X^+\\), and for all \\(i,j\\in X^-\\), i.e., \\[ C \\sim \\begin{pmatrix} O &amp; \\ast \\\\ \\ast &amp; O \\end{pmatrix}.\\] Note. If \\(C\\) is bipatite, for every eigenvalue \\(\\theta\\) of \\(C\\), \\(-\\theta\\) is an eigenvalue of \\(C\\) such that \\(\\mathrm{mult}(\\theta) = \\mathrm{mult}(-\\theta)\\). Indeed, let \\(C = \\begin{pmatrix} O &amp; A \\\\ B &amp; O \\end{pmatrix}\\), \\[\\begin{pmatrix} O &amp; A \\\\ B &amp; O \\end{pmatrix} \\begin{pmatrix}x\\\\y\\end{pmatrix} = \\theta \\begin{pmatrix}x\\\\y\\end{pmatrix}\\Leftrightarrow \\begin{pmatrix} O &amp; A \\\\ B &amp; O \\end{pmatrix} \\begin{pmatrix}x\\\\-y\\end{pmatrix} = -\\theta \\begin{pmatrix}x\\\\-y\\end{pmatrix}, \\] where \\(Ay = \\theta x\\) and \\(Bx = \\theta y\\). If \\(C\\) is bipartite, \\(C^2\\) is reducible. The matrix \\(C\\) is irreducible and \\(C^2\\) is reducible, if \\(C_{ij} \\geq 0\\) for all \\(i,j\\) and \\(C\\) is reducible. (Exercise) Remark. Note 1. Even if \\(C\\) is not symmetric \\[\\begin{pmatrix} O &amp; A \\\\ B &amp; O \\end{pmatrix} \\begin{pmatrix}x\\\\y\\end{pmatrix} = \\theta \\begin{pmatrix}x\\\\y\\end{pmatrix}\\Leftrightarrow \\begin{pmatrix} O &amp; A \\\\ B &amp; O \\end{pmatrix} \\begin{pmatrix}x\\\\-y\\end{pmatrix} = -\\theta \\begin{pmatrix}x\\\\-y\\end{pmatrix}\\] holds. So the geometrix multiplicities coincide. How about the algebraic multiplicities? Note 3. Set \\(x \\sim y\\) if and only if \\(C_{xy}&gt;0\\). So the graph may have loops. Then \\[(C^2)_{xy} &gt; 0 \\Leftrightarrow \\textrm{ if there exists } z\\in X \\textrm{ such that } x\\sim z \\sim y.\\] Note that \\(C\\) is irreducible if and only if \\(\\Gamma(C)\\) is connected. Let \\[\\begin{align} X^+ &amp; = \\{y\\mid \\textrm{there is a path of even length from }x \\textrm{ to }y\\}\\\\ X^- &amp; = \\{y\\mid \\textrm{there is no path of even length from }x \\textrm{ to }y\\} \\neq \\emptyset. \\end{align}\\] If there is an edge \\(y\\sim z\\) in \\(X^+\\) and \\(w\\in X^-\\). Then there would be a path from \\(x\\) to \\(y\\) of even length. So \\(\\mathrm{e}(X^+, X^+) = \\mathrm{e}(X^-, X^-) = 0.\\). Theorem 2.1 (Perron-Frobenius) Given a matrix \\(C\\) in \\(\\mathrm{Mat}_n(\\mathbb{R})\\) such that \\((a)\\) \\(C\\) is symmetric. \\((b)\\) \\(C\\) is irreducible. \\((c)\\) \\(C_{ij} \\geq 0\\) for all \\(i,j\\). Let \\(\\theta_0\\) be the maximal eigenvalue of \\(C\\) with eigenspace \\(V_0 \\subseteq \\mathbb{R}^n\\), and let \\(\\theta_r\\) be the maximal eigenvalue of \\(C\\) with eigenspace \\(V_r \\subseteq \\mathbb{R}^n\\). Then the following hold. \\((i)\\) Suppose \\(0\\neq v = \\begin{pmatrix}\\alpha_1\\\\\\alpha_2\\\\\\vdots\\\\\\alpha_n\\end{pmatrix} \\in V_0\\). Then \\(\\alpha_0 &gt; 0\\) for all \\(i\\), or \\(\\alpha_i &lt; 0\\) for all \\(i\\). \\((ii)\\) \\(\\mathrm{dim}V_0 = 1\\). \\((iii)\\) \\(\\theta_r \\geq -\\theta_0\\). \\((iv)\\) \\(\\theta_r = \\theta_0\\) if and only if \\(C\\) is bipartite. First, we prove the following lemma. Lemma 2.1 Let \\(\\langle \\;,\\; \\rangle\\) be the dot product in \\(V = \\mathbb{R}^n\\). Pick a symmetric matrix \\(B \\in \\mathrm{Mat}_n(\\mathbb{R})\\). Suppose all eigenvalues of \\(B\\) are nonnegative. (i.e., \\(B\\) is positive semidefinite.) Then there exist vectors \\(v_1, v_2, \\ldots, v_n\\in V\\) such that \\(B_{ij} = \\langle v_i, v_j\\rangle\\) for \\((1\\leq i, j \\leq n)\\). Proof. By elementary linear algebra, there exists an orthonormal basis \\(w_1, w_2, \\ldots, w_n\\) of \\(V\\) consisting of eigenvectors of \\(B\\). Set the \\(i\\)-th column of \\(P\\) is \\(w_i\\) and \\(D = \\mathrm{diag}(\\theta_1,\\ldots, \\theta_n)\\). Then \\(P^\\top P = I\\) and \\(BP = PD\\). Hence, \\[B = PDP^{-1} = PDP^\\top = QQ^\\top,\\] where \\[Q = P\\cdot \\mathrm{diag}(\\sqrt{\\theta_1}, \\sqrt{\\theta_2}, \\ldots, \\sqrt{\\theta_n}) \\in \\mathrm{Mat}_n(\\mathbb{R}).\\] Now, let \\(v_i\\) be the \\(i\\)-th column of \\(Q^\\top\\). Then \\[B_{ij} = v_i^\\top\\cdot v_j^- = \\langle v_i, v_j\\rangle.\\] Now we start the proof of Theorem 2.1. Proof of Theorem 2.1\\((i)\\) Let \\(\\langle , \\rangle\\) denote the dot product on \\(V = \\mathbb{R}^n\\). Set \\[\\begin{align} B &amp; = \\theta I - C\\\\ &amp; = \\textrm{symmetric matrix with eigenvalues }\\theta_0 - \\theta_i \\geq 0\\\\ &amp; = (\\langle v_i, v_j\\rangle)_{1\\leq i,j\\leq n} \\end{align}\\] with the same \\(v_1, \\ldots, v_n \\in V\\) by Lemma 2.1. Observe: \\(\\sum_{i = 1}^n \\alpha_iv_i = 0\\). Pf. \\[\\begin{align} \\|\\sum_{i = 1}^n \\alpha_iv_i\\|^2 &amp; = \\langle \\sum_{i=1}^n\\alpha_iv_i, \\sum_{i=1}^n\\alpha_iv_i\\rangle\\\\ &amp; = \\begin{pmatrix} \\alpha_1 &amp;\\ldots &amp;\\alpha_n\\end{pmatrix}B\\begin{pmatrix} \\alpha_1\\\\\\vdots\\\\\\alpha_n\\end{pmatrix}\\\\ &amp; = v^\\top Bv\\\\ &amp; = 0, \\end{align}\\] since \\(Bv = (\\theta_0 I - C)v = 0\\). Now set \\[s = \\textrm{the number of indices} i, \\textrm{ where } \\alpha_i &gt;0.\\] Replacing \\(v\\) by \\(-v\\) if necessary, without loss of generality we may assume that \\(s\\geq 1\\). We want to show \\(s = n\\). Assume \\(s &lt; n\\). Without loss of generality, we may assume that \\(\\alpha_i &gt;0\\) for \\(1\\leq s\\leq s\\) and \\(\\alpha_i = 0\\) for \\(s+1 \\leq i \\leq n\\). Set \\[ \\rho = \\alpha_1v_1 + \\cdots + \\alpha_sv_s = -\\alpha_{s+1}v_{s+1} - \\cdots - \\alpha_nv_n.\\] Then, for \\(i = 1,\\ldots, s\\), \\[\\begin{align} \\langle v_i, \\rho \\rangle &amp; = \\sum_{j = s+1}^n -\\alpha_j\\langle v_i, v_j\\rangle \\quad (\\langle v_i, v_j\\rangle = B_{ij}, B = \\theta_0I - C)\\\\ &amp; = \\sum_{j = s+1}^n (-\\alpha_{ij})(-C_{ij})\\\\ &amp; \\leq 0. \\end{align}\\] Hence \\[0\\leq \\langle \\rho, \\rho\\rangle = \\sum_{i=1}^s \\alpha_i \\langle v_i, \\rho\\rangle \\leq 0,\\] as \\(\\alpha &gt; 0\\) and \\(\\langle v_i, \\rho\\rangle \\leq 0\\). Thus, we have \\(\\langle, \\rho, \\rho \\rangle = 0\\) and \\(\\rho = 0\\). For \\(j = s+1, \\ldots, n\\), \\[0 = \\langle \\rho, v_j\\rangle = \\sum_{i=1}^s \\alpha_i\\langle v_i, v_j\\rangle \\leq 0,\\] as \\(\\langle v_i, v_j\\rangle = -C_{ij}\\). Therefore, \\[0 = \\langle v_i, v_j \\rangle = -C_{ij} \\textrm{ for } 1\\leq i, \\leq s, \\: s+1 \\leq j \\leq n.\\] Since \\(C\\) is symmetric, \\[C = \\begin{pmatrix} \\ast &amp; O \\\\ O &amp; \\ast\\end{pmatrix}\\] Thus \\(C\\) is reducible, which is not the case. Hence \\(s = n\\). Proof of Theorem 2.1 \\((ii)\\). Suppose \\(\\dim V_0 \\geq 2\\). Then, \\[\\dim\\left(V_0 \\cap \\begin{pmatrix}1\\\\0\\\\\\vdots\\\\0\\end{pmatrix}^\\bot\\right) \\geq 1.\\] So, there is a vector \\[0\\neq v = \\begin{pmatrix}\\alpha_1\\\\\\vdots\\\\\\alpha_n\\end{pmatrix} \\in V_0\\] with \\(\\alpha_1 = 0\\). This contradicts 1. Now pick \\[0\\neq w = \\begin{pmatrix}\\beta_1\\\\\\vdots\\\\\\beta_n\\end{pmatrix} \\in V_r.\\] Proof of Theorem 2.1 \\((iii)\\). Suppose \\(\\theta_r &lt; -\\theta_0\\). Since the eigenvalues of \\(C^2\\) are the squares of those of \\(C\\), \\(\\theta_r^2\\) is the maximal eigenvalue of \\(C^2\\). Also we have \\(C^2w = \\theta_r^2w\\). Observe that \\(C^2\\) is irreducible. (As otherwise, \\(C\\) is bipartite by Note 3, and we must have \\(\\theta_r = -\\theta_0\\).) Therefore, \\(\\beta_i &gt; 0\\) for all \\(i\\) or \\(\\beta_i &lt; 0\\) for all \\(i\\). We have \\[\\langle v, w\\rangle = \\sum_{i=1}^n\\alpha_i\\beta_j \\neq 0.\\] This is a contradiction, as \\(V_0 \\bot V_r\\). Proof of Theorem 2.1 \\((iv)\\) \\(\\Rightarrow\\): Let \\(\\theta_r = -\\theta_0\\). Then \\(\\theta = \\theta_1^2 = \\theta_0^2\\) is the maximal eigenvalue of \\(C^2\\), and \\(v\\) and \\(w\\) are linearly independent eigenvalues for \\(\\theta\\) for \\(C^2\\). Hence, for \\(C^2\\), \\(\\mathrm{mult}(\\theta) \\geq 2\\). Thus by 2, \\(C^2\\) must be reducible. Therefore, \\(C\\) is bipartite by Note 3. \\(\\Leftarrow\\): This is Note 1. \\(\\Box\\) Let \\(\\Gamma = (X, E)\\) be any graph. Definition 2.3 \\(\\Gamma\\) is said to be bipartite if the adjacency matrix \\(A\\) is bipartite. That is, \\(X\\) can be written as a disjoint union of \\(X^+\\) and \\(X^-\\) such that \\(X^+, X^-\\) contain no edges of \\(\\Gamma\\). Corollary 2.1 For any (connected) graph \\(\\Gamma\\) with \\[\\mathrm{Spec}(\\Gamma) = \\begin{pmatrix}\\theta_0 &amp; \\theta_1 &amp;\\cdots &amp; \\theta_r\\\\m_1 &amp; m_1 &amp; \\cdots &amp; m_r\\end{pmatrix} \\:\\textrm{ with }\\: \\theta_0 &gt; \\theta_1 &gt; \\cdots &gt; \\theta_r.\\] Let \\(V_i\\) be the eigenspace of \\(\\theta_i\\). Then the following holds. Supppose \\(0\\neq v = \\begin{pmatrix} \\alpha_1\\\\\\vdots \\\\\\alpha_n \\end{pmatrix} \\in V_0\\in \\mathbb{R}^n\\). Then \\(\\alpha_i &gt; 0\\) for all \\(i\\) or \\(\\alpha_i &lt; 0\\) for all \\(i\\). \\(m_0 = 1\\). \\(\\theta_r \\geq -\\theta_0\\) if and only if \\(\\Gamma\\) is bipartite. In this case, \\[-\\theta_i = \\theta_{r-i} \\; \\textrm{and} \\; m_i = m_{r-i} \\quad (0\\leq i\\leq r)\\] Proof. This is a direct consequences of Theorem 2.1 and Note 3. "],["lec3.html", "Chapter 3 Cayley Graphs", " Chapter 3 Cayley Graphs Monday, January 25, 1993 Given graphs \\(\\Gamma = (X, E)\\) and \\(\\Gamma&#39; = (X&#39;, E&#39;)\\). Definition 3.1 A map \\(\\sigma: X \\to X&#39;\\) is an isomorphism\\index{isomorophism of graphs whenever; \\(\\sigma\\) is one-to-one and onto, \\(xy\\in E\\) if and only if \\(\\sigma x \\sigma y \\in E&#39;\\) for all \\(x, y\\in X\\). We do not distinguish between isomorphic graphs. Definition 3.2 Suppose \\(\\Gamma = \\Gamma&#39;\\). Above isomorphism \\(\\sigma\\) is called an automorphism of \\(\\Gamma\\). Then set \\(\\mathrm{Aut}(\\Gamma)\\) of all automorphisms of \\(\\Gamma\\) becomes a finite group under composition. Definition 3.3 If \\(\\mathrm{Aut}(\\Gamma)\\) acts transitive on \\(X\\), \\(\\Gamma\\) is called vertex transitive. Example 3.1 A Cayley graphs: Definition 3.4 (Cayley Graphs) Let \\(G\\) be any finite group, and \\(\\Delta\\) any generating set for \\(G\\) such that \\(1_G \\not\\in \\Delta\\) and \\(g\\in \\Delta \\to g^{-1}\\in \\Delta\\). Then Cayley graph \\(\\Gamma = \\Gamma(G, \\Delta)\\) is defined on the vetex set \\(X = G\\) with the edge set \\(E\\) define by the following. \\[E = \\{(h_1,h_2)\\mid h_1, h_2\\in G, h_1^{-1}h_2\\in \\Delta\\} = \\{(h, hg) \\mid h\\in G, g\\in \\Delta\\}\\] Example 3.2 \\(G = \\langle a \\mid a^6 = 1\\rangle\\), \\(\\Delta = \\{a, a^{-1}\\}\\). Example 3.3 \\(G = \\langle a \\mid a^6 = 1\\rangle\\), \\(\\Delta = \\{a, a^{-1}, a^2, a^{-2}\\}\\). Example 3.4 \\(G = \\langle a, b \\mid a^6 = 1, b^2, ab = ba\\rangle\\), \\(\\Delta = \\{a, a^{-1}, b\\}\\). Remark. \\(\\mathrm{Aut}(\\Gamma) \\simeq D_6\\times \\mathbb{Z}_2\\) contains two regular subgroups isomorphic to \\(D_6\\) and \\(\\mathbb{Z}_5 \\times \\mathbb{Z}_2\\) and \\(\\Gamma\\) is obtained as Cayley graphs in two ways. Cayley graphs are vertex transitive, indeed. Theorem 3.1 The following hold. \\((i)\\) For any Cayley graph \\(\\Gamma = \\Gamma(G, \\Delta)\\), the map \\[G \\to \\mathrm{Aut}(\\Gamma) \\; (g\\mapsto \\hat{g})\\] is an injective homomorphism of groups, where \\[\\hat{g}(x) = gx \\quad \\textrm{for all }\\: g\\in G \\textrm{ and for all } x\\in X (= G).\\] Also, the image \\(\\hat{G}\\) is regular on \\(X\\). i.e., the image \\(\\hat{G}\\) acts transitively on \\(X\\) with trivial vertex stabilizers. \\((ii)\\) For any graph \\(\\Gamma = (X, E)\\), suppose there exists a subgroup \\(G \\subseteq \\mathrm{Aut}(\\Gamma)\\) that is regular on \\(X\\). Pick \\(x\\in X\\), and let \\[\\Delta = \\{g\\in G\\mid \\langle x, g(x)\\in E\\}.\\] Then \\(1\\not\\in \\Delta\\), \\(g\\in \\Delta \\to g^{-1}\\in \\Delta\\), and \\(\\Delta\\) generates \\(G\\). Moreover, \\(\\Gamma \\simeq \\Gamma(G, \\Delta)\\). Proof. \\((i)\\) Let \\(g\\in G\\). We want to show that \\(\\hat{g}\\in \\mathrm{Aut}(\\Gamma)\\). Let \\(h_1, h_2\\in X = G\\). Then, \\[\\begin{align} (h_1, h_2)\\in E &amp; \\to h_1^{-1}h_2\\in \\Delta\\\\ &amp; \\to (gh_1)^{-1}(gh_2)\\in \\Delta \\\\ &amp; \\to (gh_1, gh_2)\\in E\\\\ &amp; \\to (\\hat{g}(h_1), \\hat{g}(h_2)) \\in E. \\end{align}\\] Hence, \\(\\hat{g}\\in \\mathrm{Aut}(\\Gamma)\\). Observe: \\(g \\mapsto \\hat{g}\\) is a homomorphism of groups: \\[\\hat{1}_G = 1, \\; \\widehat{g_1g_2} = \\widehat{g_1}\\widehat{g_2}.\\] Observe: \\(g \\mapsto \\hat{g}\\) is one-to-one: \\[\\widehat{g_1} = \\widehat{g_2} \\to g_1 = \\widehat{g_1}(1_G) = \\widehat{g_2}(1_G) = g_2.\\] Observe: \\(\\hat{G}\\) is regular on \\(X\\): Clear by construction. \\((ii)\\) \\(1_G\\not\\in \\Delta\\): Since \\(\\Gamma\\) has not loops, \\((x, 1_Gx) \\not\\in E\\). \\(g\\in \\Delta \\to g^{-1}\\in \\Delta\\): \\[g\\in \\Delta \\to (x, g(x))\\in E \\to E \\ni (g^{-1}(x), g^{-1}(g(x))) = (g^{-1}(x), x).\\] \\(\\Delta\\) generates \\(G\\): Suppose \\(\\langle \\Delta \\subsetneq G\\). Let \\(\\hat{X} = \\{g(x)\\mid g\\in \\langle \\Delta\\rangle\\} \\subsetneq X\\). (\\(\\hat{X} \\subsetneq X\\) as \\(G\\) acts regularly on \\(X\\).) Since \\(\\Gamma\\) is connected, there exists \\(y\\in \\hat{X}\\) and \\(z\\in X\\setminus \\hat{X}\\) with \\(yz\\in E\\). Let \\(y = g(x)\\), \\(g\\in \\langle \\Delta\\rangle\\), \\(z\\in h(x)\\), \\(h\\in G\\setminus \\langle \\Delta\\rangle\\). Then \\[(y,z)=(g(x),h(x))\\in E \\to (x,g^{-1}h(x))\\in E \\to g^{-1}h\\in \\langle \\Delta \\rangle \\to h\\in \\langle \\Delta \\rangle. \\] This is a contradition. Therefore, \\(\\Delta\\) generates \\(G\\). Let \\(\\Gamma&#39; = (X&#39;, E&#39;)\\) denote \\(\\Gamma(G, \\Delta)\\). We shall show that \\[\\theta: X&#39; \\to X \\; (g\\mapsto g(x))\\] is an isomorphism of graphs. \\(\\theta\\) is one-to-one: For \\(h_1, h_2\\in X&#39; = G\\), \\[\\theta(h_1)=\\theta(h_2) \\to h_1(x) = h_2(x) \\to h_2^{-1}h_1(x)=x \\to h_2^{-1}h_1\\in \\mathrm{Stab}_G(x) = \\{1_G\\} \\to h_1 = h_2.\\] (\\(\\mathrm{Stab}_G = \\{g\\in G\\mid g(x) = x\\}\\).) \\(\\theta\\) is onto: Since \\(G\\) is transitive, \\[X = \\{g(x)\\mid g\\in G\\} = \\theta(X&#39;) = \\theta(G).\\] \\(\\theta\\) respects adjacency: For \\(h_1, h_2\\in X&#39; = G\\), \\[(h_1,h_2)\\in E&#39; \\leftrightarrow h_1^{-1}h_2\\in \\Delta \\leftrightarrow (x, h_1^{-1}h_2(x))\\in E \\leftrightarrow (h_1(x),h_2(x))\\in E \\leftrightarrow (\\theta(h_1), \\theta(h_2))\\in E.\\] Therefore \\(\\theta\\) is an isomorphism between graphs \\(\\Gamma(G, \\Delta)\\) and \\(\\Gamma(X, E)\\). How to compute the eigenvalues of the Cayley graph of and abelian group. Let \\(G\\) be any finite abelian group. Let \\(\\mathbb{C}^*\\) be the multiplicative group on \\(\\mathbb{C}\\setminus \\{0\\}\\). Definition 3.5 A (linear) \\(G\\)-character is any group homomorphism \\(\\theta: G \\to \\mathbb{C}^*\\). Example 3.5 \\(G = \\langle a\\mid a^3 =1\\rangle\\) has three characters, \\(\\theta_0, \\theta_1, \\theta_2\\). \\[ \\begin{array}{c|ccc} \\theta_i(a^j) &amp; 1 &amp; a &amp; a^2 \\\\ \\hline \\theta_0 &amp; 1 &amp; 1 &amp; 1\\\\ \\theta_1 &amp; 1 &amp; \\omega &amp; \\omega^2\\\\ \\theta_2 &amp; 1 &amp; \\omega^2 &amp; \\omega \\end{array}, \\quad \\textrm{with }\\; \\omega = \\frac{-1+\\sqrt{-3}}{2}. \\] Here \\(\\omega\\) is a primitive cube root pf \\(q\\) in \\(\\mathbb{C}^*\\), i.e., \\(1+\\omega + \\omega^2 = 0\\). For arbitraty group \\(G\\), let \\(X(G)\\) be the set of all characters of \\(G\\). Observe: For \\(\\theta_1, \\theta_2\\in X(G)\\), one can define product $_1_2: \\[\\theta_1\\theta_2(g) = \\theta_1(g)\\theta_2(g) \\quad \\textrm{for all }\\; g\\in G.\\] Then \\(\\theta_1\\theta_2\\in X(G)\\). Observe: \\(X(G)\\) with this product is an (abelian) group. Lemma 3.1 The groups \\(G\\) and \\(X(G)\\) are isomorphic for all finite abelian groups \\(G\\). Proof. \\(G\\) is a direct sum of cyclic groups; \\[G = G_1\\oplus G_2 \\oplus \\cdots \\oplus G_m, \\quad \\textrm{where } \\; G_i = \\langle a_i\\mid a_i^{d_i} = 1\\rangle \\quad (1\\leq i\\leq m).\\] Pick any alement \\(\\omega_i\\) of order \\(d_i\\) in \\(\\mathbb{C}^*\\), i.e., a primitive \\(d_i\\)-the root of \\(1\\). Define \\[\\theta_i: G \\to \\mathbb{C}^* \\quad (a_1^{\\varepsilon_1}\\cdots a_m^{\\varepsilon_m} \\mapsto \\omega_i^{\\varepsilon_i} \\quad \\textrm{where }\\; 0\\leq \\varepsilon_i &lt; d_i, 1\\leq i\\leq m).\\] Then \\(\\theta_i\\in X(G)\\). (Exercise) Claim: There exists an isomorphism of groups \\(G \\to X(G)\\) that sends \\(a_i\\) to \\(\\theta_i\\). Observe: \\(\\theta_i^{d_i} = 1\\). For every \\(g = a_1^{\\varepsilon_1}\\cdots a_m^{\\varepsilon_m} \\in G\\), \\[\\theta_i^{d_i}(g) = (\\theta_i(g))^{d_i} = (\\omega_i^{\\varepsilon_i})^{d_i} = (\\omega_i^{d_i})^{\\varepsilon_i} = 1.\\] Observe: If \\(\\theta_1^{\\varepsilon_1}\\theta_2^{\\varepsilon_2}\\cdots \\theta_m^{\\varepsilon_m} = 1\\) for some \\(0\\leq \\varepsilon_i &lt; d_i, 1\\leq i\\leq m\\). Then \\(\\varepsilon_1 = \\varepsilon_2 = \\cdots = \\varepsilon_m = 0\\). Pf. \\(1 = \\theta_1^{\\varepsilon_1}\\theta_2^{\\varepsilon_2}\\cdots \\theta_m^{\\varepsilon_m}(a_i) = \\omega_i^{\\varepsilon_i}\\), Since \\(\\omega_i\\) is a primitive \\(d_i\\)-th root of \\(1\\), \\(\\varepsilon_i = 0\\) for \\(1\\leq i\\leq m\\). Observe: \\(\\theta_1, \\ldots, \\theta_m\\) generate \\(X(G)\\). Pick \\(\\theta\\in X(G)\\). Since \\(a_i^{d_i} = 1\\), \\(1 = \\theta(a_i^{d_i}) = \\theta(a_i)^{d_i}\\). Hence \\(\\theta(a_i) = \\omega^{\\varepsilon_i}\\) for some \\(\\varepsilon_i\\) with \\(0\\leq \\varepsilon_i &lt; d_i\\). Now \\(\\theta = \\theta_1^{\\varepsilon_1}\\cdots \\theta_m^{\\varepsilon_m}\\), since these are both equal to \\(\\omega_i^{\\varepsilon_i}\\) at \\(a_i\\) for \\(1\\leq i \\leq m\\). Therefore, \\[G \\to X(G) \\quad (a_i \\mapsto \\theta_i)\\] is an isomorphism of groups. Note. The correspondence above is clearly a group homomorphism. "],["lec4.html", "Chapter 4 Examples", " Chapter 4 Examples Wednesday, January 27, 1993 Theorem 4.1 Given a Cayley graph \\(\\Gamma = \\Gamma(G, \\Delta)\\). View the standard module \\(V \\equiv \\mathbb{C}G\\) (the group algebra), so \\[\\left\\langle \\sum_{g\\in G}\\alpha_g g, \\;\\sum_{g\\in G}\\beta_g g\\right\\rangle = \\sum_{g\\in G}\\alpha_g\\overline{\\beta_g}, \\quad \\textrm{with}\\; \\alpha_g, \\beta_g\\in \\mathbb{C}.\\] For any \\(\\theta\\in X(G)\\), write \\[\\hat{\\theta} = \\sum_{g\\in G}\\theta(g^{-1})g.\\] Then the following hold.   \\((i)\\) \\(\\langle \\hat{\\theta_1}, \\hat{\\theta_2}\\rangle = |G|\\) if \\(\\theta_1 = \\theta_2\\) and \\(0\\) othewise for \\(\\theta_1, \\theta_2\\in X(G)\\). In particular, \\(\\{\\hat{\\theta}\\mid \\theta\\in X(G)\\}\\) forms a basis for \\(V\\).   \\((ii)\\) \\(A\\hat{\\theta} = \\Delta_\\theta \\hat{\\theta}\\) for \\(\\theta \\in X(G)\\), where \\(A\\) is the adjacency matrix and \\[\\Delta_\\theta = \\sum_{g\\in \\Delta}\\theta(g).\\] In particular, the eigenvalues of \\(\\Gamma\\) are precisely \\[\\Delta_\\theta \\mid \\theta\\in X(G)\\}.\\] Proof. \\((i)\\) Claim: For every \\(\\theta \\in X(G)\\), let \\[s:= \\sum_{g\\in G}\\theta(g^{-1}) = \\begin{cases} |G| &amp; \\text{if }\\;\\theta = 1\\\\ 0 &amp; \\text{if } \\;\\theta \\neq 1. \\end{cases}\\] Pf. Clear if \\(\\theta =1\\). Let \\(\\theta \\neq 1\\). Then \\(\\theta(h)\\neq 1\\) for some \\(h\\in G\\). \\[s\\cdot \\theta(h) = \\left(\\sum_{g\\in G}\\theta(g^{-1})\\right)\\theta(h) = \\sum_{g\\in G}\\theta(g^{-1}h) = \\sum_{g&#39;\\in G}\\theta(g&#39;^{-1}) = s.\\] Since \\(\\theta(h)\\neq 1\\), \\(s = 0\\). Claim. \\(\\theta(g^{-1}) = \\overline{\\theta(g)}\\) for every \\(\\theta\\in X(G)\\) and every \\(g\\in G\\). Since \\(\\theta(g)\\in \\mathbb{C}\\) is a root of \\(1\\), \\[|\\theta(g)|^2 = \\theta(g)\\overline{\\theta(g)} = 1.\\] On the other hand, since \\(\\theta\\) is a homomorphism, \\[\\theta(g)\\theta(g^{-1}) = \\theta(1) = 1.\\] Hence \\(\\theta(g^{1}) = \\overline{\\theta(g)}\\). Now \\[\\begin{align} \\langle \\widehat{\\theta_1}, \\widehat{\\theta_2}\\rangle &amp; = \\sum_{g\\in G}\\theta_1(g^{-1})\\overline{\\theta_2(g^{-1})}\\\\ &amp; = \\sum_{g\\in G}\\theta_1(g^{-1})\\theta_2(g)\\\\ &amp; = \\sum_{g\\in G}\\theta_1\\theta_2^{-1}(g^{-1})\\\\ &amp; = \\begin{cases} |G| &amp; \\text{if}\\quad \\theta_1\\theta_2^{-1} = 1\\\\ 0 &amp; \\text{if} \\quad \\theta_1\\theta_2^{-1}\\neq 1. \\end{cases} \\end{align}\\] Since \\(|G| = |X(G)|\\) by Lemma 3.1, and \\(\\widehat{\\theta_i}\\)’s are orthogonal nonzero elements in \\(V\\), thet form a basis of \\(V\\).  \\((ii)\\) Let \\(\\Delta = \\{g_1, \\ldots, g_r\\}\\). Then \\[\\begin{align} A\\hat{\\theta} &amp; = A\\left(\\sum_{g\\in G}\\theta(g^{-1}g)\\right)\\\\ &amp; = \\sum_{g\\in G}\\theta(g^{-1})(gg_1 + \\cdots + gg_r) \\quad (\\Gamma(g) = \\{gg_1, \\ldots, gg_r\\})\\\\ &amp; = \\sum_{i = 1}^r \\left(\\sum_{g\\in G}\\theta(g^{-1})(gg_i)\\right)\\\\ &amp; = \\sum_{i=1}^r\\left(\\sum_{g\\in G}\\theta(g_ig_i^{-1}g^{-1})(gg_i)\\right)\\\\ &amp; = \\sum_{i = 1}^r\\left(\\sum_{g\\in G}\\theta(g_i)\\theta((gg_i)^{-1})gg_i\\right)\\\\ &amp; = \\sum_{i = 1}^r\\theta(g_i)\\sum_{h\\in G}\\theta(h^{-1})h \\\\ &amp; = \\Delta_\\theta\\cdot \\hat{\\theta}. \\end{align}\\] Since \\(\\{\\hat{\\theta}\\mid \\theta\\in X(G)\\}\\) forms a basis, the eigenvalues of \\(\\Gamma\\) are precisely, \\[\\{\\Delta_\\theta\\mid \\theta\\in X(G)\\}.\\] This completes the proof. Example 4.1 Let \\(G = \\langle a\\mid a^6 = 1\\rangle\\), and \\(\\Delta = \\{a, a^{-1}\\}\\). Pick a primitive 6-th root of 1, \\(\\omega\\). Then \\[X(G) = \\{\\theta^i\\mid 0\\leq i\\leq 5\\} \\quad \\text{such that }\\quad \\theta(a) = \\omega, \\; \\omega + \\omega^{-1} = 1.\\] \\[\\begin{array}{c | c | c} \\varphi\\in X(G) &amp; \\varphi(a) &amp; \\Delta_\\varphi = \\theta(a) + \\theta(a)^{-1}\\\\ \\hline 1 &amp; 1 &amp; 2\\\\ \\theta &amp; \\omega &amp; \\omega+\\omega^{-1} = 1\\\\ \\theta^2 &amp; \\omega^2 &amp; -1\\\\ \\theta^3 &amp; \\omega^3 = -1 &amp; -2\\\\ \\theta^4 &amp; \\omega^4 &amp; -1\\\\ \\theta^5 &amp; \\omega^5 &amp; 1 \\end{array}\\] \\[\\text{Spec}(\\Gamma) = \\begin{pmatrix} 2 &amp; 1 &amp; -1 &amp; -2\\\\ 1 &amp; 2 &amp; 2 &amp; 1\\end{pmatrix}.\\] Example 4.2 \\(D\\)-cube, \\(H(D,2)\\). Let \\[X = \\{(a_1, \\ldots, a_D)\\mid a_i\\in \\{1,-1\\}, \\; 1\\leq i\\leq D\\},\\] \\[E = \\{xy\\mid x, y\\in X, \\; x, y \\text{: different in exactly one coordinate}\\}.\\] Also \\(H(D,2)\\) is a Cayley graph \\(\\Gamma(G, \\Delta)\\), where \\[G = G_1\\oplus G_2 \\oplus \\cdots \\oplus G_D, \\] \\[G_i = \\langle a_i\\mid a_i^2 = 1\\rangle,\\quad \\Delta = \\{a_1, \\ldots, a_D\\}.\\] Homework: The spectrum of \\(H(D,2)\\) is \\[\\begin{pmatrix} \\theta_0 &amp; \\theta_1 &amp; \\cdots &amp; \\theta_D\\\\ m_0 &amp; m_1 &amp; \\cdots &amp; m_D\\end{pmatrix},\\] where \\[\\theta_i = D-2i \\quad (0\\leq i\\leq D), \\quad m_i = \\binom{D}{i}.\\] Remark. Let \\(\\theta \\in X(G)\\). Then \\(\\theta: X \\to \\{\\pm 1\\}\\). If \\[\\nu(\\theta) = |\\{i\\mid \\theta(a_i) = -1\\}|, \\] then \\(\\Delta_\\theta = D-2i\\). Since there are \\(\\binom{D}{i}\\) such \\(\\theta\\), we have te assertion. We want to compute the subconstituent algebra for \\(H(D,2)\\). First, we make a few observations about arbitrary graphs. Let \\(\\Gamma = (X,E)\\) be any graph, \\(A\\), the adjacemcy matrix of \\(\\Gamma\\), and \\(V\\), the standard module over \\(K = \\mathbb{C}\\). Fix a base \\(x\\in X\\). Write \\(E_i^* = E_i^*(x)\\), and \\[T \\equiv T(x) = \\text{the algebra generated by}\\; A, E_0^*, E_1^*, \\ldots .\\] Definition 4.1 Let \\(W\\) be any orreducible \\(T\\)-module (\\(\\subseteq V\\)). Then the endpoint \\(r \\equiv r(W)\\) satisfied \\[r = \\min\\{i\\mid E_i^*W \\neq 0\\}.\\] The diameter \\(d = d(W)\\) satisfied \\[d = |\\{i\\mid E_i^*W \\neq 0\\}| - 1.\\] Lemma 4.1 With the above notation, let \\(W\\) be an irreducible \\(T\\)-module. Then \\((i)\\) \\(E_i^*AE_j^* = 0 \\; \\text{ if }\\; |i-j|=1, \\quad \\neq 0 \\; \\text{ if }\\; |i-j| = 1, \\quad 0\\leq i,j\\leq d(x)\\). \\((ii)\\) \\(AE_j^*W \\subseteq E_{j-1}^*W + E_j^*W + E^*_{j+1}W\\), \\(0\\leq j \\leq d(x)\\). \\((E_i^*W = 0 \\; \\text{ if } i&lt;j\\) or \\(i &gt; d(x)\\).) \\((iii)\\) \\(E^*_jW \\neq 0\\) if \\(r\\leq j \\leq r+d\\), \\(=0\\) if \\(0\\leq j\\leq r\\) or \\(r+d &lt; j \\leq d(x)\\). \\((iv)\\) \\(E_i^*AE^*_jW \\neq 0\\), if \\(|i-j| = 1\\) \\((r \\leq i,j \\leq r+d)\\). Proof. \\((i)\\) Pick \\(y\\in X\\) with \\(\\partial(x,y) = j\\). We want to find \\(E_i^*AE^*_j \\hat{y}\\). Note, \\[E_j^*\\hat{y} = \\begin{cases} 0 &amp; \\text{if }\\; \\partial(x.y)\\neq j\\\\ \\hat{y} &amp; \\text{if }\\; \\partial(x,y) = j.\\end{cases}.\\] \\[\\begin{align} E_i^*AE_j^*\\hat{y} &amp;= E_i^*A\\hat{y} \\\\ &amp; = E_i^*\\sum_{z\\in X, yz\\in E}\\hat{z}\\\\ &amp; = \\sum_{z\\in X, yz\\in E, \\partial(x, z) = i}\\hat{z} \\qquad (*)\\\\ &amp; = 0 \\; \\text{ if }\\; |i-j|&gt;1 \\; \\text{by triangle inequality.} \\end{align}\\] If \\(|i-j| = 1\\), there exist \\(y, y&#39;\\in X\\) such that \\(\\partial(x,y) = j\\), \\(\\partial(x,y&#39;) = i\\), \\(yy&#39;\\in E\\) by connectivity of \\(\\Gamma\\). Hence (*) contains \\(\\widehat{y&#39;}\\) and \\(* \\neq 0\\) \\((ii)\\) We have \\[\\begin{align} AE_j^*W &amp; = \\left(\\sum_{i=0}^{d(x)}E_i^*\\right)AE_j^*W\\\\ &amp; = E_{j-1}^*AE^*_jW + E^*_jAE_j^*W + E^*_{j+1}AE_j^*W\\\\ &amp; \\subseteq E^*_{j-1}W + E^*_jW + E^*_{j+1}W. \\end{align}\\] \\((iii)\\) Suppose \\(E_j^*W = 0\\) for some \\(j\\) \\((r\\leq j \\leq r+d)\\). Then \\(r &lt; j\\) by the definition of \\(r\\). Set \\[\\tilde{W} = E^*_rW + E^*_{r+1}W + \\cdots + E^*_{j-1}W.\\] Observe \\(0\\subsetneq \\tilde{W} \\subsetneq W\\). Also \\(A\\tilde{W} \\subseteq \\tilde{W}\\) by \\((ii)\\) and \\(E_i^*\\tilde{W} \\subseteq \\tilde{W}\\) for every \\(i\\) by construction. Thus \\(T\\tilde{W} \\subseteq \\tilde{W}\\), contradicting \\(W\\) beging irreducible. "],["lec5.html", "Chapter 5 \\(T\\)-Modules of \\(H(D,2)\\), I", " Chapter 5 \\(T\\)-Modules of \\(H(D,2)\\), I Friday, January 29, 1993 Let \\(\\Gamma = (X, E)\\) be a graph, \\(A\\) the adjacency matrix, and \\(V\\) the standard module over \\(K = \\mathbb{C}\\). Fix a base \\(x\\in X\\) and write \\(E_I^* \\equiv E_i^*(x)\\), and \\(T \\equiv T(x)\\). Let \\(W\\) be an irreducible \\(T\\)-module with endpoint \\(r:= \\min\\{i\\mid E_i^*W \\neq 0\\}\\) and diameter \\(d:=|\\{i\\mid E_i^*W\\neq 0\\}|-1\\). We have \\[\\begin{align} E_i^*W &amp; \\neq 0 &amp; r\\leq i \\leq r+d\\\\ &amp; = 0 &amp; 0 \\leq i &lt; r \\;\\text{ or }\\; r+d &lt; i \\leq d(x). \\end{align}\\] Claim: \\(E_i^*AE_j^*W \\neq 0\\) if \\(|i-j| = 1\\) for \\(r\\leq i,j\\leq r+d\\). (See Lemma 4.1.) Suppose \\(E_{j+1}^*AE_j^*W = 0\\) for some \\(j\\) with \\(r \\leq j &lt; r+d\\). Observe that \\[\\tilde{W} = E^*_rW + \\cdot E^*_jW\\] is \\(T\\)-invariant with \\[0 \\subsetneq \\tilde{W} \\subsetneq W.\\] Becase \\(A\\tilde{W} \\subseteq \\tilde{W}\\) since \\(AE_j^*W \\subseteq E^*_{j-1}W + E^*_jW\\), \\[E_k^*\\tilde{W} \\subseteq \\tilde{W} \\quad\\text{for all }\\; k,\\] we have \\(T\\tilde{W} \\subseteq{W}\\). Suppose \\(E_{i-1}^*AE_i^*W = 0\\) for some \\(i\\) with \\(r \\leq i &lt; r+d\\). Similarly, \\[\\tilde{W} = E^*_iW + \\cdot E^*_{r+d}.W\\] is a \\(T\\)-module with \\(0\\subsetneq \\tilde{W} \\subsetneq W\\). Definition 5.1 Let \\(\\Gamma\\), \\(E^*_i\\), and \\(T\\) be as above. Irreducible \\(T\\)-modules \\(W\\) and \\(W&#39;\\) are isomorphic whenever there is an isomorphism \\(\\sigma: W \\to W&#39;\\) of vector spaces such that \\(a\\sigma = \\sigma a\\) for all \\(a\\in T\\). Recall that the standard module \\(V\\) is an orthogonal direct sum of irreducible \\(T\\)-modules \\(W_1 \\oplus W_2 \\oplus \\cdots\\). Given \\(W\\) in this list, the multiplicity of \\(W\\) in \\(V\\) is \\[|\\{j \\mid W_j \\simeq W\\}|.\\] Remark. It is known that the multiplicity does not depend on the decomposition. Now assume that \\(\\Gamma\\) is the \\(D\\)-cube, \\(H(D,2)\\) with \\(D\\geq 1\\). Vew \\[\\begin{align} X &amp; = \\{a_1\\cdots a_D\\mid a_i\\in \\{1, -1\\}, 1\\leq i\\leq D\\},\\\\ E &amp; = \\{xy\\mid x, y\\in X, \\; x, y \\;\\text{ differ in exactly 1 coordinate.}\\}. \\end{align}\\] Find \\(T\\)-modules. Claim: \\(H(D,2)\\) is bipartite with a partition \\(X = X^+ \\cup X^-\\), where \\[\\begin{align} X^+ &amp; = \\{a_1\\cdots a_D\\in X\\mid \\prod a_i &gt; 0\\}\\\\ X^- &amp; = \\{a_1\\cdots a_D \\in X \\mid \\prod a_i &lt; 0\\} \\end{align}\\] Observe: for all \\(y, z\\in X\\), \\[\\partial(y,z) = i \\Leftrightarrow y, z \\; \\text{ differ in exactly in }\\; i\\; \\text{ coorinates with }\\; 0\\leq i\\leq D.\\] Here, the diameter of \\(H(D, 2) = D = d\\) for all \\(x\\in X\\). Theorem 5.1 Let \\(\\Gamma = H(D,2)\\) be as above. Fix \\(x\\in X\\), and write \\(E_i^* = E^*_i(x)\\), and \\(T = T(x)\\). Let \\(W\\) be an irreducible \\(T\\)-module with endpoint \\(r\\), and diameter \\(d\\) with \\(0\\leq r \\leq r+d\\leq D\\). \\((i)\\) \\(W\\) has a basis \\(w_0, w_1, \\ldots, w_d\\) with \\(w_i\\in E^*_{i+r}W\\) for \\(0\\leq i\\leq d\\). With respect to which the matrix representing \\(A\\) is \\[ \\begin{pmatrix} 0 &amp; d &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; 0\\\\ 1 &amp; 0 &amp; d-1 &amp; \\cdots &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 2 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 3 &amp; \\cdots &amp; 0 &amp; 0 &amp; 0\\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 2 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; d-1 &amp; 0 &amp; 1\\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; d &amp; 0 \\end{pmatrix} \\] \\((ii)\\) $ d= D - 2r$. In particular, \\(0\\leq r\\leq D/2\\). \\((iii)\\) Let \\(W&#39;\\) denote an irreducible \\(T\\)-module with endpoint \\(r&#39;\\). Then \\(W\\) and \\(W&#39;\\) are isormorphic as \\(T\\)-modules if and only if \\(r = r&#39;\\). \\((iv)\\) The multiplicity of the irreducible \\(T\\)-module with endpoint \\(r\\) is \\[\\binom{D}{r} - \\binom{D}{r-1} \\quad \\text{if } 1\\leq r \\leq R/2,\\] and \\(1\\) if \\(r = 0\\). Proof. Recall that \\(\\Gamma\\) is vertex transitive. It is a Cayley graph. Hence without loss of generality, we may assume that \\(x = \\overbrace{11\\cdots 1}^{D}\\). Notation: Set \\(\\Omega = \\{1, 2, \\ldots, D\\}\\). For every subset \\(S \\subseteq \\Omega\\), let \\[\\hat{S} = a_1\\cdot a_d \\in X \\quad a_i = \\begin{cases} -1 &amp; \\text{if }\\; i\\in S\\\\ 1 &amp; \\text{if } i\\not\\in S.\\end{cases}\\] In particular, \\(\\hat{emptyset} = x\\) and \\[|S| = i \\Leftrightarrow \\partial(x, \\hat{S}) = i \\Leftrightarrow \\hat{S}\\in E^*_iV.\\] For all \\(S, T\\subseteq \\Omega\\), we say \\(S\\) covers T$ if and only if \\(S\\supseteq T\\) and \\(|S| = |T| +1\\). Observe that \\(\\hat{S}, \\hat{T}\\) are adjacent in \\(\\Gamma\\) if and only if either \\(T\\) coverse \\(S\\) or \\(S\\) coverr \\(T\\). Define the ‘raising matrix’ \\[R = \\sum_{i=0}^D E^*_{i+1}AE^*_i.\\] Observe that \\[RE_i^*V \\subseteq E^*_{i+1}V \\; \\text{ for }\\; 0\\leq i \\leq D, \\; \\text{ and }E^*_{D+1}V = 0.\\] Indeed for any \\(S\\subseteq \\Omega\\) with \\(|S| = i\\), \\[\\begin{align} R\\hat{S} &amp; = RE^*_i\\hat{S} \\\\ &amp; = E^*_{i+1}A\\hat{S} \\\\ &amp; = \\sum_{T_1 \\subseteq \\Omega, S \\text{ covers }T_1} E^*_{i+1}\\widehat{T_1} + \\sum_{T \\subseteq \\Omega, T \\text{ covers }S} E^*_{i+1}\\hat{T}\\\\ &amp; = \\sum_{T \\subseteq \\Omega, T \\text{ covers }S} E^*_{i+1}\\hat{T}. \\end{align}\\] Define the ‘lowering matrix’ \\[L = \\sum_{i=0}^D E^*_{i-1}AE^*_i.\\] Observe that \\[LE_i^*V \\subseteq E^*_{i-1V} \\; \\text{ for }\\; 0\\leq i \\leq D, \\; \\text{ and }E^*_{-1}V = 0.\\] Indeed for any \\(S\\subseteq \\Omega\\), \\[L\\hat{S} = \\sum_{T\\subseteq \\Omega, S \\text{ covers }T} \\hat{T}.\\] Observe that \\(A = L + R\\). For convenience, set \\[A^* = \\sum_{i=0}^D (D-2i)E_i^*.\\] Claim: The following hold. \\((a)\\) \\(LR - RL = A^*\\). \\((b)\\) \\(A*L - LA^* = 2L\\). \\((c)\\) \\(A^*R - RA^* = -2R\\). In particular \\(\\mathrm{Span}(R,L, A^*)\\) is a ’representation of Lie algebra \\(\\mathrm{sl}_2(\\mathbb{C})\\). Remark (Lie Algebra sl2(C)). \\[\\mathrm{sl}_2(\\mathbb{C}) = \\{X\\mid \\mathrm{Mat}(\\mathbb{C} \\mid \\mathrm{tr}(X) = 0\\}.\\] For \\(X, Y\\in \\mathrm{sl}_2(\\mathbb{C})\\), define a binary operation \\([X, Y] = XY - YX\\). \\[A^*\\sim \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; -1\\end{pmatrix}, \\quad L\\sim \\begin{pmatrix} 0 &amp; 1 \\\\ 0 &amp; 0\\end{pmatrix}, \\quad R\\sim \\begin{pmatrix} 0 &amp; 0 \\\\ 1 &amp; 0\\end{pmatrix}.\\] Then these satisfy the relations \\((a)\\) - \\((c)\\) above. Proof of Claim. Apply both sides to \\(\\hat{S}\\) \\((S\\subseteq \\Omega)\\). Say \\(|S| = i\\). Proof of \\((a)\\): \\[\\begin{align} (LR - RL)\\hat{S} &amp; = L\\left(\\sum_{\\substack{T \\subseteq \\Omega, T \\text{ covers }S\\\\(D-i \\text{ of them})}}\\hat{T}\\right) - R \\left(\\sum_{\\substack{U \\subseteq \\Omega, S \\text{ covers }U\\\\(i \\text{ of them})}}\\hat{T}\\right)\\\\ &amp; = (D-i)\\hat{S} + \\sum_{V \\subseteq \\Omega, |V| = i, |S\\cap V| = i-1}\\hat{V} - \\left(i\\hat{S} + \\sum_{V \\subseteq \\Omega, |V| = i, |S\\cap V| = i-1}\\hat{V}\\right)\\\\ &amp; = (D-2i)\\hat{S}\\\\ &amp; = A^*\\hat{S}. \\end{align}\\] Proof of \\((b)\\): \\[\\begin{align} (A^*L - LA^*)\\hat{S} &amp; = (D-2(i-1))L\\hat{S} - (D-2i)L\\hat{S} \\quad (\\text{since} \\; L\\hat{S}\\in E^*_{i-1}V)\\\\ &amp; = 2L\\hat{S}. \\end{align}\\] Proof of \\((c)\\): \\[\\begin{align} (A^*R - RA^*)\\hat{S} &amp; = (D-2(i+1))R\\hat{S} - (D-2i)R\\hat{S} \\quad (\\text{since} \\; R\\hat{S}\\in E^*_{i+1}V)\\\\ &amp; = 2R\\hat{S}. \\end{align}\\] Let \\(W\\) be an irreducible \\(T\\)-module with endpoint \\(r\\) and diameter \\(d\\) \\((0\\leq r \\leq r+d \\leq D)\\). Proof of \\((i)\\) and \\((ii)\\): Pick \\(0\\neq w \\in E^*_rW\\). Claim: \\(LRw = (D-2r)w\\). Pf. \\[\\begin{align} LRw &amp; = (A^*+RL)w \\quad (\\text{by Claim }(a))\\\\ &amp; = A^*w \\quad (Lw \\in E^*_{r-1}W = 0)\\\\ (D-2r)w. \\end{align}\\] Define \\[w_i = \\frac{1}{i!}R^iw \\in E^*_{r+i}W \\quad (0\\leq i \\leq d).\\] Then, \\[\\begin{align} Rw_i &amp; = (i+1)w_{i+1}\\quad (0\\leq i \\leq d)\\\\ Rw_d &amp; = 0 \\quad (\\text{by definition of }d) \\end{align}\\] Claim: \\(Lw_0 = 0\\) and \\[Lw_i = (D-2r-i+1)w_{i-1} \\quad (1\\leq i\\leq d).\\] Pf. We prove by induction on \\(i\\). The case \\(i=0\\) is trivial, and the case \\(i=1\\) follows from above claim. Let \\(i\\geq 2\\), \\[\\begin{align} Lw_i &amp; = \\frac{1}{i}LRw_{i-1} = \\frac{1}{i}(A^*+RL)w_{i-1} \\quad (\\text{by Claim (a)})\\\\ &amp; \\quad \\text{(by induction hypothesis)}\\\\ &amp; = \\frac{1}{i}((D-2(r+i-1))w_{i-1} + (D-2r-(i-1)+1)Rw_{i-2} \\quad (Rw_{i-2} = (i-1)w_{i-1})\\\\ &amp; = \\frac{1}{i}i(D-2r-i+1)w_{i-1}\\\\ &amp; = (D-2r-i+1)w_{i-1}. \\end{align}\\] Claim: \\(w_0, \\ldots, w_d\\) is a basis for \\(W\\). Pf. Let \\(W&#39; = \\mathrm{Span}\\{w_0, \\ldots, w_d\\}\\). Then \\(W&#39;\\) is \\(R\\) and \\(L\\) invariant. So it is \\(A = R+L\\) invariant. Also it is \\(E^*_i\\)-invariant for every \\(i\\). Hence \\(W&#39;\\) is a \\(T\\)-module. Since \\(W\\) is irreducible, \\(W&#39; = W\\). As \\(w_i\\)’s are orthogonal, theyy are linearly independent. Note that \\(w_i\\neq 0\\) by the definition of \\(d\\) and Lemma 4.1 \\((iv)\\). Claim: \\(d = D-2r\\). Pf. By \\((a)\\), \\[\\begin{align} 0 &amp; = (LR - RL - A^*)w_d \\\\ &amp; = 0 - (D-2r-d+1)Rw_{d-1} - (D-2(r+d))w_d\\\\ &amp; = -d(D-2r-d+1)w_d - (D-2(r+d))w_d\\\\ &amp; = (-dD + 2rd + d^2 - d - D + 2r + 2d)w_d\\\\ &amp; = (d^2 + (2r-D+1)d + 2r - D)w_d\\\\ &amp; = (d+2r-D)(d+1)w_d. \\end{align}\\] Hence \\(d = D-2r\\). Therefore, with respect to a bais \\(w_0, w_1, \\ldots, w_d\\), \\(A = L+R\\), \\(w_{-1} = w_{d+1} = 0\\), \\[Lw_i = (d-i+1)w_{i-1}, \\quad Rw_i = (i+1)w_{i+1}.\\] \\[L = \\begin{pmatrix} 0 &amp; d &amp; 0 &amp; \\cdots &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; d-1 &amp; \\cdots &amp; 0 &amp; 0\\\\ &amp; &amp; \\cdots &amp; \\cdots &amp; &amp; \\\\ &amp; &amp; &amp; &amp; 0 &amp; 1\\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\end{pmatrix}, \\qquad R = \\begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0\\\\ 1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0\\\\ 0 &amp; 2 &amp; 0 &amp; \\cdots &amp; &amp; \\\\ &amp; &amp; &amp; &amp; 0 &amp; 1\\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; d &amp; 0 \\end{pmatrix}.\\] This completes the proof of \\((i)\\) and \\((ii)\\). "],["lec6.html", "Chapter 6 \\(T\\)-Modules of \\(H(D,2)\\), II", " Chapter 6 \\(T\\)-Modules of \\(H(D,2)\\), II Monday, February 1, 1993 Proof (Proof of Theorem 5.1 Continued). \\((iii)\\) Let \\(r = r&#39;\\), \\(w_0,\\ldots, w_d\\): a basis for \\(W\\) with \\(w_i\\in E^*_iW\\), and \\(w_0&#39;, \\ldots, w_d&#39;\\): a basis for \\(W&#39;\\) with \\(w_i&#39;\\in E^*_iW&#39;\\). Then \\(d = D-2r = D-2r&#39; = d&#39;\\), and \\[\\sigma: W \\to W&#39; \\quad (w_i\\mapsto w_i&#39;)\\] is an isomorsphism of \\(T\\)-modules by \\((i)\\). If \\(r\\neq r&#39;\\), then \\[d = D-2r \\neq D-2r&#39; = d&#39;,\\] hence, \\(\\dim W \\neq \\dim W&#39;\\). \\((iv)\\) Let \\(W_i\\) be the irreducible \\(T\\)-module with endpoint \\(i\\). Then \\[\\dim E_r^*V = \\binom{D}{r} = \\sum_{i=0}^r \\mathrm{mult}(W_i).\\] Hence, we have that \\[\\mathrm{mult}(W_r) = \\binom{D}{r} - \\binom{D}{r-1}\\] by induction on \\(r\\). Theorem 6.1 Let \\(\\Gamma = H(D,2)\\) with \\(D\\geq 1\\). Fix a vertex \\(x\\in X\\) and write \\[E^*_i \\equiv E^*_i(x), \\quad T = T(x), and A^* \\equiv \\sum_{i=0}^D(D-2i)E^*_i.\\] Let \\(W\\) be an irreducible \\(T\\)-module with endpoint \\(r\\) with \\(0\\leq r\\leq D/2\\). Then, \\((i)\\) \\(W\\) has a basis \\[w_0^*, w_1^*, \\ldots, w_d^* \\quad(d = D-2r), \\; \\text{ such that }\\; w_i^*\\in E_{i+r}W\\; (0\\leq i \\leq d)\\] with respect to which the matrix corresponding to \\(A^*\\) is \\[\\begin{pmatrix} 0 &amp; d &amp; 0 &amp; &amp; &amp; &amp; \\\\ 1 &amp; 0 &amp; d-1 &amp; &amp; &amp; &amp; \\\\ 0 &amp; 2 &amp; 0 &amp; &amp; &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; &amp; \\\\ &amp; &amp; &amp; &amp; 0 &amp; 2 &amp; 0 \\\\ &amp; &amp; &amp; &amp; d-1 &amp; 0 &amp; 1\\\\ &amp; &amp; &amp; &amp; 0 &amp; d &amp; 0 \\end{pmatrix}.\\] In particular, | \\((ii)\\) \\(E_iA^*E_j = 0\\) if \\(|i-j|\\neq 1\\) for \\(0 \\leq i, j\\leq D\\). Proof. We use the notation, \\[[\\alpha, \\beta] = \\alpha\\beta - \\beta\\alpha \\; (=-[\\beta, \\alpha]).\\] Recall that \\((a)\\) \\([L, R] = A^*\\), \\((b)\\) \\([A^*, L] = wL\\), \\((c)\\) \\([A^*, R] = -2R\\), and \\(A = L + R\\). Write \\((a) - (c)\\) in terms of \\(A\\) and \\(A^*\\), we have, \\[[A, A^*] = [L, A^*]+ [R, A^*] = 2(R-L).\\] \\[\\begin{cases} R + L &amp; = A\\\\ R-L &amp; = [A,A^*]/2. \\end{cases}.\\] Hence, \\[\\begin{align} R &amp; = \\frac{1}{4}(2A + [A, A^*]) \\quad \\text{ and }\\\\ L &amp; = \\frac{1}{4}(2A - [A, A^*]). \\end{align}\\] Now \\((a)\\), \\((b)\\) become \\[\\begin{align} A^2A^* - 2AA^*A + A^*A^2 - 4A^* &amp; = 0 \\tag{6.1}\\\\ {A^*}^2A - 2A^*AA^* + A{A^*}^2 - 4A &amp; = 0 \\tag{6.2} \\end{align}\\] Pf. By \\((b)\\), \\[\\begin{align} 2A - AA^* + A^*A &amp; = 4L\\\\ &amp; = 2[A^*, L]\\\\ &amp; = A^* \\frac{2A - [A,A^*]}{2} - \\frac{2A - [A, A^*]}{2}A^* \\end{align}\\] So we have ((6.2)) \\[ {A^*}^2A - 2A^*AA^* + A{A^*}^2 - 4A = 0. \\] By \\((a)\\), \\[\\begin{align} -16A^* &amp; = [2A + [A, A^*], 2A - [A, A^*]]\\\\ &amp; (2A + [A, A^*])(2A - [A, A^*]) - (2A - [A,A^*])(2A + [A, A^*])\\\\ &amp; = [4A^2 - 2A[A, A^*] + [A, A^*](2A) - [A,A^*]^2\\\\ &amp; \\quad - 4A^2 - 2A[A, A^*] + [A, A^*](2A) + [A, A^*]^2\\\\ &amp; = -4A^2A^* + 4AA^*A + 4AA^*A - 4A^*A^2. \\end{align}\\] So, \\[A^2A^* - 2AA^*A + A^*A^2 - 4A^* = 0.\\] Claim: \\(E_i^*A^*E_j = 0\\) if \\(|i-j| \\neq 1\\) for \\(0\\leq i, j\\leq D\\). Pf. We have, \\[\\begin{align} 0 &amp; = E_i(A^2A^* - 2AA^*A + A^*A^2 - 4A^*)E_j\\\\ &amp; = E_iA^*E_j(\\theta_i^2 - 2\\theta_i\\theta_j + \\theta_j^2 - 4)\\\\ &amp; \\quad (AE_j = \\theta_jE_j, \\; E_iA = (AE_j)^\\top = (\\theta_iE_i)^\\top = \\theta_iE_i)\\\\ &amp; = E_iA^*E_j(\\theta_i - \\theta_j -2)(\\theta_i - \\theta_j + 2)\\\\ &amp; = E_iA^*E_j(D-2i - (D-2j)-2)(D-2i - (D-2j) + 2)\\\\ &amp; \\quad (\\theta_k = D-2k)\\\\ &amp; = E_iA^*E_j \\cdot 4(i-j+1)(i-j-1) \\end{align}\\] and \\(i-j+1 \\neq 0\\), \\(i-j-1\\neq 0\\). Hence, \\(E_i^*A^*E_j = 0\\). Now define “dual raising matrix”, \\[R^* = \\sum_{i=0}^D E_{i+1}A^*E_i.\\] So, \\[R^*E_iV \\subseteq E_{i+1}V, \\quad (0\\leq i\\leq D, \\; E_{D+1}V = 0).\\] Define “dual lowering matrix” \\[L^* = \\sum_{i=0}^D E_{i-1}A^*E_i.\\] Then \\[L^*E_iV \\subseteq E_{i-1}V \\quad (0\\leq i\\leq D, \\; E_{-1}V = 0).\\] Observe that \\[A^* = \\left(\\sum_{i=0}^DE_i\\right)A^*\\left(\\sum_{j=0}^DE_j\\right) = L^* + R^*\\] by Claim 1. Claim 2. We have | \\((a)\\) \\([L^*, R^*] = A\\), | \\((b)\\) \\([A, L^*] = 2L^*\\), | \\((c)\\) \\([A, R^*] = -2R^*\\). Pf. \\((b)\\) \\[\\begin{align} AL^* - L^*A &amp; = \\sum_{i=0}^D(AE_{i-1}A^*E_i - E_{i-1}A^*E_iA)\\\\ &amp; = \\sum_{i=0}^D E_{i-1}A^*E_i (\\theta_{i-1} - \\theta_i)\\\\ &amp; \\quad (\\theta_k = D-2k, \\quad \\theta_{i-1}- \\theta_i = 2I - 2(i-1) = 2\\\\ &amp; = 2L^*. \\end{align}\\] \\((c)\\) Similar. Remark. \\[\\begin{align} AR^* - R^*A &amp; = \\sum_{i=0}^D (AE_{i+1}A^*E_i - E_{i+1}A^*E_iA)\\\\ &amp; = \\sum_{i=0}^D E_{i+1}A^*E_i (\\theta_{i+1} - \\theta_i)\\\\ &amp; = 2R^*. \\end{align}\\] \\((a)\\) We have, by \\((b)\\), \\((c)\\) \\[\\begin{equation} [A, A^*] = [A, L^*] + [A, R^*] = 2(L^* - R^*). \\end{equation}\\] Since \\(A^* = L^* + R^*\\), \\[R^* = \\frac{2A^* + [A^*, A]}{4}, \\quad L^* = \\frac{2A^* - [A^* - A]}{4}.\\] Now \\((a)\\) is seen to be equivalent to ((6.2)) upon evaluation. This proves Claim 2. Remark. \\[\\begin{align} [L^*,R^*] &amp; = \\frac{1}{16}((2A^*-[A^*,A])(2A^*+[A^*,A]) - (2A^*+[A^*,A])(2A^*- [A,A^*]))\\\\ &amp; = \\frac{1}{16}(4{A^*}^2 + 2A^*[A^*,A] - [A^*,A]2A^* - [A^*,A]^2 - 4{A^*}^2 + 2A^*[A^*,A] - [A^*,A]2A^* + [A^*,A]^2)\\\\ &amp; = \\frac{1}{4}({A^*}^2A - 2A^*AA^* + A{A^*}^2)\\\\ &amp; = A, \\end{align}\\] by ((6.2)). Now apply same argument as for ((6.1)), ((6.2)) of Theorem 5.1 and observe \\(A^*\\) has \\(D+1\\) distinct eigenvalues. So, \\[A^* = \\sum_{i=0}^D(D-2i)E^*_i\\] generates \\[M^* = \\mathrm{Span}(E^*_0, \\ldots, E^*_D).\\] Hence, \\(E_0, \\ldots, E_D, \\; A^*\\) generates \\(T\\). Take an irreducible \\(T\\)-module \\(W\\) with endpoint \\(r\\) with \\(0\\leq r \\leq D/2\\). Set \\(t = \\min\\{i\\mid E_iW\\}\\). Pick \\(0\\neq w_0^*\\in E_tW\\). Set \\[w_i^* = \\frac{1}{i!}{R^*}^i w_0^* \\in E_{t+i}W \\quad \\text{for all }i.\\] Then, \\[R^*w_i^* = (i+1)w_{i+1}^* \\quad \\text{for all }i.\\] By \\((a)\\), we get by induction, \\(L^*w_i^* = (D-2t-i+1)w^*_{i-1}\\), \\[\\begin{align} L^*w_i^* &amp; = \\frac{1}{i}L^*R^*w_{i-1}^* \\\\ &amp; = \\frac{1}{i}(A + R^*L^*)w_{i-1}^* \\\\ &amp; = \\frac{1}{i}((D-2(t+i-1))w^*_{i-1} + (i-1)(D-2t-i+2)w_{i-1}^*)\\\\ &amp; = (D-2t - i + 1)w_{i-1}^*. \\end{align}\\] So \\(\\mathrm{Span}(w_0^*, w_1^*, \\ldots )\\) is \\(L^*\\), \\(R^*\\), \\(A^*\\)-invariant. Hence, \\(W = \\mathrm(Span)(w_0^*, w_1^*, \\ldots, w_d^*)\\), \\(w_0^*, w_1^*, \\ldots, w_d^* \\neq 0\\), \\(w^*_i = 0\\) for every \\(i&gt;d\\) by dimension. Thus \\(d = D-2t\\). Pf. \\[\\begin{align} (D -2(t+d))w^*_d &amp; = Aw_d^* \\\\ &amp; = (L^*R^* - R^*L^*)w_d^*\\\\ &amp; = -(D-2t - d + 1)R^*w_{d-1}^*\\\\ &amp; = -(D-2t - d +1)dw^*_d. \\end{align}\\] Hence, \\[0 = d^2 + (2t - D - 1 + 2)d - (D-2t) = (d-D+2t)(d+1)\\] So \\(d = D-2t\\). Definition 6.1 For any graph \\(\\Gamma = (X, E)\\), pick a vertex \\(x\\in X\\) and set \\(E^*_i \\equiv E^*_i(x)\\) and \\(T \\equiv T(x)\\). \\((i)\\) an irreducible \\(T\\)-module \\(W\\) is thin if \\(\\dim E_i^*W \\leq 1\\) for every \\(i\\), \\((ii)\\) \\(\\Gamma\\) is thin with respet to \\(x\\), if every irreducible \\(T(x)\\)-module is thin, \\((iii)\\) an irreducible \\(T\\)-module \\(W\\) is dual thin if \\(\\dim E_iW \\leq 1\\) for every \\(i\\), \\((iv)\\) \\(\\Gamma\\) is dual thin with respect to \\(x\\), if every irreducible \\(T(x)\\)-module is dual thin. Observe: \\(H(D,2)\\) is thin, dual thin with respect to each \\(x\\in X\\). With above notation, write \\(D \\equiv D(x)\\). \\((i)\\) an ordering \\(E_0, E_1, \\ldots, E_R\\) of primitive idempotents of \\(\\Gamma\\) is restricted if \\(E_0\\) corresponds to the maximal eigenvalue. Fix a restricted ordering, \\((ii)\\) \\(\\Gamma\\) is \\(Q\\)-polynomial with respect to \\(x\\), above ordering if there exists \\(A^* \\equiv A^*(x)\\) such that \\(\\quad (a)\\) \\(E_0^*V, \\ldots, E_D^*V\\) are the maximal eigenspaces for \\(A^*\\). \\(\\quad (b)\\) \\(E_iA^*E_j = 0\\) if \\(|i-j| &gt; 1\\) for \\(0\\leq i,j\\leq R\\). Observe \\(H(D,2)\\) is \\(Q\\)-polynomial with respect to the natural ordering of the idempotents and every vetex. Program. Study graphs that are thin and \\(Q\\)-polynomial with respect to each vertex. (In fact, thin with respect to \\(x\\) implies dual thin with respect to \\(x\\).) Get a situation like \\(H(D,2)\\), where \\(T\\) is generated by \\(A\\), \\(A^*\\). Except \\(\\mathrm{sl}_s(\\mathbb{C})\\) is repalaced by a quantum Lie algebra. "],["lec7.html", "Chapter 7 The Johnson Graph \\(J(D,N)\\)", " Chapter 7 The Johnson Graph \\(J(D,N)\\) Wednesday, February 3, 1993 Definition 7.1 The Johnson graph, \\(\\Gamma = J(D,N)\\) \\((1\\leq D\\leq N-1)\\) satisfies \\[\\begin{align} X &amp; = \\{S\\mid S\\subset \\Omega, \\; |S| = D\\} \\quad\\text{where }\\; \\Omega = \\{1, 2, \\ldots, N\\}\\\\ E &amp; = \\{ST\\mid S, T\\in X, \\quad |S\\cap T| = D-1\\}. \\end{align}\\] Example 7.1 \\(J(2,4)\\) Note 1. The symmetric group \\(S_N\\) acts on \\(\\Omega\\). \\(S_N \\subseteq \\mathrm{Aut}(\\Gamma)\\) acts vertex transitively on \\(\\Gamma\\). Note 2. \\(\\Gamma = J(D,N)\\) is isomorphic to \\(\\Gamma&#39; = J(N-D,N)\\). \\[\\begin{align} \\Gamma = (X, E) &amp; &amp; \\Gamma&#39; = (X&#39;, E&#39;)\\\\ X\\ni S &amp; \\qquad \\longrightarrow &amp; \\bar{S} = \\Omega\\setminus S \\in X&#39; \\end{align}\\] This correspondence induces an isomorphism of graphs. Pf. \\[\\begin{align} ST\\in E &amp; \\Leftrightarrow |S\\cap T| = D-1\\\\ &amp; \\Leftrightarrow |\\Omega - (S\\cup T)| = N-D-1\\\\ &amp; \\Leftrightarrow |\\bar{S} \\cap \\bar{T}| = N-D-1\\\\ &amp; \\Leftrightarrow \\bar{S}\\bar{T} \\in E&#39; \\end{align}\\] Hence, without loss of generality, assume \\[D\\leq N/2 \\quad \\text{for } J(D,N).\\] We sill need the eigenvalues of \\(J(D,N)\\) for certain problem later in the course. We can get these eigenvalues from our study of \\(H(D,2)\\). Lemma 7.1 The eigenvalues for \\(J(D,N)\\) with \\(1\\leq D \\leq N/2\\) are give by \\[\\begin{align} \\theta_i &amp; = (N-D-i)(D-i) - i \\quad (0\\leq i\\leq D)\\\\ m_i &amp; = \\binom{D}{i} - \\binom{N}{i-1}. \\end{align}\\] Proof. Let \\[\\begin{align} \\Gamma_J &amp; \\equiv J(D,N) = (X_J, E_J)\\\\ \\Gamma_H &amp; \\equiv H(N,2) = (X_H, E_H). \\end{align}\\] Set \\(x \\equiv 11\\cdots 1 \\in X_H\\). Define \\(\\tilde{\\Gamma} \\equiv (\\tilde{X}, \\tilde{E})\\), where \\[\\begin{align} \\tilde{X} &amp; = \\{y\\in X_H \\mid \\partial_H(x,y) = D\\} \\quad \\partial_H:\\text{distance in }\\Gamma_H\\\\ \\tilde{E} &amp; = \\{yz\\in X_H \\mid \\partial_H(y,z) = 2\\}. \\end{align}\\] Observe \\[\\begin{align} X_J &amp; \\to &amp; \\tilde{X}\\\\ S &amp; \\mapsto &amp; \\hat{S}, \\end{align}\\] where \\[\\hat{S} = a_1\\cdots a_N, \\quad a_i = \\begin{cases} -1 &amp; \\text{if }i\\in S\\\\ 1 &amp; \\text{if }i\\not\\in S \\end{cases}\\] induces an isomorphism of graphs \\(\\Gamma_J \\to \\tilde{\\Gamma}\\). Pf. \\[\\begin{align} ST \\in E_J &amp;\\Leftrightarrow |S\\cap T| = D-1\\\\ &amp; \\Leftrightarrow \\partial_H(\\hat{S}, \\hat{T}) = 2\\\\ &amp; \\Leftrightarrow (\\hat{S}, \\hat{T})\\in \\tilde{E}. \\end{align}\\] Identify, \\(\\Gamma_J\\) with \\(\\tilde{\\Gamma}\\). Then the standard module \\(V_J\\) of \\(\\Gamma_J\\) becomes \\(\\tilde{V} = E^*_DV_H\\), where \\(V_H\\) is the standard module of \\(\\Gamma_H\\), and \\(E^*_D \\equiv E^*_D(x)\\). Let \\(R\\) be the raising matrix with respect to \\(x\\) in \\(\\Gamma_H\\), and let \\(L\\) be the lowering matrix with respect to \\(x\\) in \\(\\Gamma_H\\). Recall \\[(RL - DE^*_D) |_{\\tilde{V}}\\] is the adjacency map in \\(\\tilde{\\Gamma}\\). To find eigenvalues of \\(\\tilde{A}\\), pick any irreducible \\(T(x)\\)-module \\(W\\) with the endpoint \\(r\\leq D\\). Then by Theorem 5.1 \\[\\text{diam}(W) = N-2r+1.\\] Let \\(w_0, w_1, \\ldots, w_{N-2r}\\) denote a basis for \\(W\\) as in Theorem 5.1. Then, \\[w_{D-r} \\in E^*_DW \\subseteq \\tilde{V}.\\] Observe: \\[\\begin{align} \\tilde{A}w_{D-r} &amp; = RLw_{D-r} - DE_D^*w_{D-r}\\\\ &amp; = R(N-2r-D+r+1)w_{D-r-1} - Dw_{D-r}\\\\ &amp; = ((N-D-r+1)(D-r) - D)w_{D-r}. \\end{align}\\] Note that this is valid for \\(D = r\\) as well. Hence, \\[\\tilde{A}w_{D-r} = ((N-D-r)(D-r)-r)w_{D-r}.\\] Let \\[V_H = \\sum W \\quad (\\text{direct sum of irreducible }T(x)-\\text{modules}.)\\] Then, \\[\\begin{align} V_J &amp; = E_D^*V_H\\\\ &amp; = \\sum_{W:r(W)\\leq D} E_D^*W\\\\ &amp; = \\text{a direct sum of 1 dimensional eigenspaces for }\\tilde{A}. \\end{align}\\] The eigenspace for eigenvalue \\[(N-D-r)(D-r)-r \\quad (\\text{monotonously decreasing with respec to }r)\\] appears with multiplicity \\[\\binom{N}{r} - \\binom{N}{r-1}\\] in this sum by Theorem 5.1 \\((iv)\\). Theorem 7.1 Let \\(\\Gamma = (X, E)\\) be any graph. For a fixed vertex \\(x\\in X\\), let \\[E_i^*\\equiv E_i^*(x), \\quad T\\equiv T(x), \\quad D \\equiv D(x), \\text{ and } K = \\mathbb{C}.\\] Then we have the following implications of conditions: \\[\\text{TH} \\Leftrightarrow \\text{C} \\Leftarrow \\text{S} \\Leftarrow \\text{G}.\\] where (TH) \\(\\Gamma\\) is thinn with respect to \\(x\\). (C) \\(E^*_iTE^*_i\\) is commutative for every \\(i\\), \\((0\\leq i \\leq D)\\). (S) \\(E^*_iTE^*_i\\) is symmetric for every \\(i\\), \\((0\\leq i \\leq D)\\). (G) For every \\(y, z\\in X\\) with \\(\\partial(x,y) = \\partial(x,z)\\), there exists \\(g\\in \\mathrm{Aut}(\\Gamma)\\) such that \\[gx = x, \\; gy = z, \\; gz = y.\\] Proof. (TH) \\(\\Rightarrow\\) (C) Fix \\(i\\) with \\(0\\leq i\\leq D\\). Let \\[V = \\sum W. \\; \\text{The standard module written as a direct sum of irreducible $T$-modules}.\\] The, \\[E_i^*V = \\sum E_i^*W. \\; \\text{The direct sum of 1-dimensional $E_i^*TE_i^*$-modules}.\\] Since \\(\\dim E_i^*W = 1\\), for \\(a, b\\in E_i^*TE_i^*\\), \\({ab - ba}_{| E_i^*W} = 0\\). Hence \\(ab - ba = 0\\). (C) \\(\\Rightarrow\\) (TH) Suppose \\(\\dim E_i^*W \\geq 2\\) for some irreducible \\(T\\)-module \\(W\\) with some \\(i\\) with \\(1\\leq i\\leq D\\). Claim: \\(E_i^*W\\) is an irreducible \\(E_i^*TE_i^*\\)-module. Pf. Suppose \\[0 \\subsetneq U \\subsetneq E_i^*W,\\] where \\(U\\) is a \\(E_i^*TE_i^*\\)-module. Then by the irreducibility, \\[TU = W.\\] So \\[U \\supseteq E_i^*TE_i^*U = E_i^*TU = E^*_iW.\\] This is a contradiction. Claim 2: Each irreducible \\(S = E_i^*TE_i^*\\)-module \\(U\\) has dimension \\(1\\). In particular, \\(\\Gamma\\) is thin with respect to \\(x\\). Pf. Pick \\[0\\neq a \\in E_i^*TE_i^*.\\] Since \\(\\mathbb{C}\\) is algebraicallt closed, \\(a\\) has an eigenvector \\(w\\in U\\) with eigenvalue \\(\\theta\\). Then, \\[\\begin{align} (a- \\theta I)U &amp; = (a-\\theta I)Sw\\\\ &amp; = S(a-\\theta I)w\\\\ &amp; = 0. \\end{align}\\] Hence, \\[a_{|U} = \\theta I_{|U} \\quad \\text{for all }\\; a\\in S.\\] Thus each \\(1\\) dimensional subspace of \\(U\\) is an \\(S\\)-module. We have \\[\\dim U = 1.\\] By Claim 1 and Claim 2, we hat (TH). "],["lec8.html", "Chapter 8 Thin Graphs", " Chapter 8 Thin Graphs Friday, February 5, 1993 Proof (Proof of Theorem 7.1 continued). (S) \\(\\Rightarrow\\) (C) Fix \\(i\\) and pick \\(a, b\\in E_i^*TE_i^*\\). Since \\(a\\), \\(b\\) and \\(ab\\) are symmetric, \\[ab = (ab)^\\top = b^\\top a^\\top = ba.\\] Hence \\(E_i^*TE_i^*\\) is commutative. (G) \\(\\Rightarrow\\) (S) Fix \\(i\\) and pick \\(a \\in E_i^*TE_i^*\\). Pick vertices \\(y, z\\in X\\). We want to show that \\[a_{yz} = a_{zy}.\\] We may assume that \\[\\partial(x, y) = \\partial(x,z) = i,\\] othewise \\[a_{yz} = a_{zy} = 0.\\] By our assumption, there exists \\(g\\in G\\) such that \\[g(y) = z, \\quad g(z) = y, \\quad g(x) = x.\\] Let \\(\\hat{g}\\) denote the permutation matrix representing \\(g\\), i.e., \\[\\hat{g}\\hat{y} =\\widehat{g(y)} \\quad \\text{for all }\\ y\\in X, \\quad \\hat{y} = \\begin{pmatrix}0\\\\\\vdots \\\\ 1 \\\\\\vdots \\\\0\\end{pmatrix}\\begin{matrix} \\\\ \\\\ \\leftarrow y \\\\ \\\\ \\text{ } \\end{matrix}.\\] If \\(g\\in \\mathrm{Aut}(\\Gamma)\\), then \\[\\hat{g}A = A\\hat{g} \\quad \\text{Exercise}.\\] Also we have \\[\\hat{g}E_j^* = E^*_j\\hat{g} \\quad (0\\leq j\\leq D),\\] since \\[\\partial(x,y) = \\partial(g(x), g(y)) = \\partial(x, g(y)).\\] Hence \\(\\hat{g}\\) commutes with each element of \\(T\\). We have \\[\\begin{align} a_{yz} &amp; = (\\hat{g}^{-1}a\\hat{g})_{yz}, \\quad (\\hat{g})_{yz} = \\begin{cases} 1 &amp; g(z) = y\\\\ 0 &amp; \\text{else.}\\end{cases}\\\\ &amp; = \\sum_{y&#39;, z&#39;}(\\hat{g}^{-1})_{yy&#39;}a_{y&#39;z&#39;}\\hat{g}_{z&#39;z}\\\\ &amp; \\quad (\\text{zero except for $g^{-1}(y&#39;) = y, \\; g(z) = z&#39;$}.)\\\\ &amp; = a_{g(y)g(z)}\\\\ &amp; a_{zy}. \\end{align}\\] This proves Theorem 7.1. Open Problem: Find all the graphs that satisfy the condition (G) for every vertex \\(x\\). \\(H(N, 2)\\) is one example, because \\[\\mathrm{Aut}\\Gamma_{1\\cdots 1} \\simeq S_\\Omega, \\quad x = (1\\cdots 1), \\Gamma_i(x) = \\{\\hat{S} \\mid |S| = i\\}.\\] Property (G) is clearly related to the distance-transitive property. Definition 8.1 Let \\(\\Gamma = (X, E)\\) be any graph. \\(\\Gamma\\) with \\(G\\subseteq \\mathrm{Aut}(\\Gamma)\\) is said to be distance-transitive (or two-point homogeneous), whenever \\[\\text{for all } x, x&#39;, y, y&#39;\\in X \\; \\text{ with } \\partial(x,y) = \\partial(x&#39;,y&#39;),\\] there exists \\(g\\in G\\) such that \\[g(x) = y,\\quad g(x&#39;) = y&#39;.\\] (This means \\(G\\) is as close to being doubly transitive as possible.) Lemma 8.1 Suppose a graph \\(\\Gamma = (X, E)\\) satisfies the property for every \\(x\\in X\\). Then, \\((i)\\) either \\(\\quad (ia)\\) \\(\\Gamma\\) is vertex transitive; or \\(\\quad (iia)\\) \\(\\Gamma\\) is bipartite \\((X = X^+ \\cup X^-)\\) with \\(X^+\\), \\(X^-\\) each an orbit of \\(\\mathrm{Aut}(\\Gamma)\\). \\((ii)\\) if \\((ia)\\) holds, then \\(\\Gamma\\) is distance-transitive. Proof. \\((i)\\) Claim. Suppose \\(y, z\\in X\\) are conneced by a path of even length. Then \\(y, z\\) are in the same orbit of \\(\\mathrm{Aut}(\\Gamma)\\). Pf. It suffices to assume that the path has lenght \\(2\\), \\(y \\sim w\\sim z\\). Now \\(\\partial(y,w) = \\partial(w,z) = 1\\). So there exits \\(g\\in \\mathrm{Aut}(\\Gamma)\\) such that $\\(gw = w, \\quad gy = z, \\quad gz = y.\\) This proves Claim. Fix \\(x\\in X\\). Now suppose that \\(\\Gamma\\) is not vertex transitive, and we shall show \\((ib)\\). Observe that \\(X = X^+ \\cup X^-\\), where \\[\\begin{align} X^+ &amp; = \\{y\\in X\\mid \\text{there exists a path of even length connecting $x$ and $y$}\\}\\\\ X^- &amp; = \\{y\\in X\\mid \\text{there exists a path of odd length connecting $x$ and $y$}\\} \\end{align}\\] Asi \\(X^+\\) is contained in an orbit \\(O^+\\) of \\(\\mathrm{Aut}(\\Gamma)\\), and \\(X^-\\) is contained in an orbit \\(O^-\\) of \\(\\mathrm{Aut}(\\Gamma)\\). Now \\(O^+\\cap O^- = \\emptyset\\) (else \\(O^+ = O^- = X\\) and vertex transitive). So, \\(X = O^+\\), and \\(X^- = O^-\\). Also \\(X^+ \\cup X^- = X\\) is a bipartition by construction. \\((ii)\\) Fix \\(x, y, x&#39;, y&#39;\\) with \\(\\partial(x,y) = \\partial(x&#39;,y&#39;)\\). By vertex transitivity, there exists an element \\[g_1\\in G \\text{ such that } g_1x = x&#39;.\\] Observe that \\[\\partial(x&#39;, y&#39;) = \\partial(x,y) = \\partial(g_1x, g_1y) = \\partial(x&#39;, g_1y).\\] Hence, there exisits an element \\[g_2\\in G \\text{ such that } g_1x&#39; = x&#39;, g_2y&#39; = g_1y&#39;, g_2g_1y = y&#39;\\] by (G(\\(x&#39;\\))) property. Set \\(g = g_2g_1\\). Then \\[gx = x&#39;, gy = y&#39;\\] by construction. The following graphs \\(\\Gamma = (X, E)\\) are vertex transitive, and satisfy the property (G(\\(x\\))) for all \\(x\\in X\\). \\[J(D, N), \\quad H(D, r), \\quad J_q(D,N),\\] where \\(H(D,r)\\): \\[\\begin{align} X &amp; = \\{a_1\\cdots a_D\\mid a_i\\in F, 1\\leq i\\leq D\\}\\\\ &amp; \\quad F: \\text{ any set of cardinality $r$}\\\\ E &amp; = \\{xy\\mid y, x\\in X, \\; \\text{$x$ and $y$ differ in exactly one coordiate}\\}. \\end{align}\\] \\(J_q(D, N)\\): \\[\\begin{align} X &amp; = \\text{the set of all $D$-dimensional subspaces of $N$-dimensional vector space over $GF(q)$.}\\\\ &amp; \\quad F: \\text{ any set of cardinality $r$}\\\\ E &amp; = \\{xy\\mid y, x\\in X, \\; \\dim (x\\cap y) = D-1\\}. \\end{align}\\] The following graph is distance-transitive but does not satisify (G(\\(x\\))) for any \\(x\\in G\\). \\(H_q(D,N)\\): \\[\\begin{align} X &amp; = \\text{the set of all $D\\times N$ matrices with entries in $GF(q)$}.\\\\ E &amp; = \\{xy\\mid y, x\\in X, \\; \\mathrm{rank}(x-y) = 1.\\}. \\end{align}\\] Remark. \\(H(D,r)\\): \\(G = S_r \\mathrm{wr} S_D\\), \\(G_x = S_{r-1} \\mathrm{wr} S_D\\), For \\(x, y\\in X\\) with \\(\\partial(x, y) = \\partial(x,z) = i\\), \\[\\begin{align} Y = \\{j\\in \\Omega \\mid x_j\\neq y_j\\} &amp; \\leftrightarrow Z = \\{j\\in \\Omega \\mid x_j\\neq z_j\\}\\\\ (y_{j_1}, \\ldots, y_{j_i}) &amp; \\leftrightarrow (z_{\\ell_1}, \\ldots, z_{\\ell_i}) \\end{align}\\] \\(J(D, N)\\): \\(G = S_N\\), \\(G_x = S_D \\times S_{N-D}\\). \\[\\begin{align} X\\cap Y &amp; \\leftrightarrow X \\cap Z\\\\ (\\Omega \\setminus X)\\cap Y &amp; \\leftrightarrow (\\Omega \\setminus X)\\cap Z. \\end{align}\\] The following graph is distance-transitive but does not satisify (G(\\(x\\))) for any \\(x\\in G\\). \\(J_q(D,N)\\): \\[X\\cap Y \\leftrightarrow X \\cap Z.\\] The theory of single thin irreducible \\(T\\)-module. Let \\(\\Gamma = (X, E)\\) be any graph. \\[\\begin{align} M &amp; = \\text{Bose-Mesner algebra over $K/\\mathbb{C}$ generated by the adjacency matrix $A$.}\\\\ &amp; = \\mathrm{Span}(E_0, \\ldots, E_R). \\end{align}\\] \\(M\\) acts on the standard module \\(V = \\mathbb{C}^{|X|}\\). Fix \\(x\\in X\\), let \\(D \\equiv D(x)\\) be the \\(x\\)-diameter, and \\(k = k(x)\\) be the valency of \\(x\\). "],["lec9.html", "Chapter 9 Thin \\(T\\)-Module, I", " Chapter 9 Thin \\(T\\)-Module, I Monday, February 8, 1993 Let \\(\\Gamma = (X, E)\\) be any graph. \\(M\\): Bose-Mesner algebra over \\(K/\\mathbb{C}\\) generated by the adjacency matrix \\(A\\). \\(\\quad M = \\mathrm{Span}(E_0, \\ldots, E_R)\\). \\(M\\) acts on the standard module \\(V = \\mathbb{C}^{|X|}\\). Fix \\(x\\in X\\), let \\(D \\equiv D(x)\\) be the \\(x\\)-diameter, and \\(k = k(x)\\) be the valency of \\(x\\). Definition 9.1 Pick \\(x\\in X\\) and write \\(E_i^* \\equiv E_i^*(x)\\) and \\(T \\equiv T(x)\\). Let \\(W\\) be an irreducible thin \\(T\\)-module with endpoint \\(r\\), diameter \\(d\\). Let \\(a_i = a_i(W)\\in \\mathbb{C}\\) satisfying \\[E_{r+i}^*A{E^*_{r+i}|}_{E_{r+i}^*W} = a_i1|_{E_{r+i}^*} \\quad (0\\leq i\\leq d).\\] Let \\(x_i = x_i(W)\\in \\mathbb{C}\\) satisfying \\[E_{r+i-1}^*A{E^*_{r+i}AE^*_{r+i-1}|}_{E_{r+i}^*W} = x_i1|_{E_{r+i}^*} \\quad (0\\leq i\\leq d).\\] Lemma 9.1 With above notation, the following hold. \\((i)\\) \\(a_i\\in \\mathbb{R} \\quad (0\\leq i\\leq d)\\). \\((ii)\\) \\(x_i\\in \\mathbb{R}^{&gt;0} \\quad (0\\leq i\\leq d)\\). \\((iii)\\) Pick \\(0\\neq w_0\\in E^*_rW\\). Set \\(w_i = E^*_{r+i}A^iw_0\\) for all \\(i\\). Then \\(\\quad (iiia)\\) \\(w_0, w_1, \\ldots, w_d\\) is a basis for \\(W\\), \\(w_{-1} = w_{d+1} = 0\\). \\(\\quad (iiib)\\) \\(Aw_i = w_{i+1} + a_iw_{i} + x_iw_{i-1} \\quad (0\\leq i\\leq d)\\). \\((iv)\\) Define \\(p_0, p_1, \\ldots, p_{d+1}\\in \\mathbb{R}[\\lambda]\\) by \\[p_0 = 1, \\quad \\lambda p_i = p_{i+1} + a_i p_i + x_i p_{i-1} \\quad (0\\leq i\\leq d),\\quad p_{-1} = 0.\\] \\(\\quad (iva)\\) \\(p_i(A)w_0 = w_i, \\quad (0\\leq i\\leq d+1)\\). \\(\\quad (ivb)\\) \\(p_{d+1}\\) is the minimal polynomial of \\(A|_W\\). Proof. \\((i)\\) \\(a_i\\) is an eigenvalue of a real symmetric matrix \\(E_{r+i}^*AE^*_{r+i}\\). \\((ii)\\) \\(x_i\\) is an eigenvalue of a real symmetrix matrix \\(B^\\top B\\), where \\[B = E^*_{r+i}AE^*_{r+i-1}.\\] Hence, \\(x_i\\in \\mathbb{R}\\). Since \\(B^\\top B\\) is positive semidefinite, \\[x_i \\geq 0.\\] Pf. If \\(B^\\top Bv = \\sigma v\\) for some \\(\\sigma \\in \\mathbb{R}\\), \\(v\\in \\mathbb{R}^m \\setminus \\{0\\}\\), then \\[0\\leq \\|Bv\\|^2 = v^\\top B^\\top Bv = \\sigma v^\\top v = \\sigma \\|v\\|^2, \\quad \\|v\\|^2 &gt;0.\\] Hence, \\(\\sigma \\geq 0\\). Moreover, \\(x_i\\neq 0\\) by Lemma 4.1 \\((iv)\\). \\((iiia)\\) Observe \\[w_i = E^*_{r+i}AE^*_{r+i-1}w_{i-1} \\quad (1\\leq i \\leq d).\\] So \\(w_i \\neq 0 \\quad (1\\leq i \\leq d)\\) by Lemma 4.1 \\((iv)\\). Hence, \\[W = \\mathrm{Span}(w_0, \\ldots, w_d)\\] by Lemma 4.1. \\((iii)\\). \\((iiib)\\) We have that \\[\\begin{align} Aw_i &amp; = E^*_{r+i+1}Aw_i + E_{r+i}^*Aw_i + E^*_{r+i-1}Aw_i\\\\ &amp; = w_{i+1} + E^*_{r+i}AE^*_{r+i}w_i + E^*_{r+i-1}AE^*_{r+i}AE^*_{r+i-1}w_{i-1}\\\\ &amp; = w_{i+1} + a_iw_{i} + x_iw_{i-1} \\end{align}\\] \\((iva)\\) Clear for \\(i=0\\). Assume it is valid for \\(0, \\ldots, i\\). \\[p_{i+1}(A)w_0 = (A-a_iI)w_i - x_iw_{i-1} = w_{i+1}.\\] \\((ivb)\\) By definition, \\[p_{d+1}(A)w_0 = 0.\\] Moreover, \\(p_{d+1}(A)W = 0\\). For every \\(w\\in W\\), write \\[\\begin{align} w &amp; = \\sum_{i=0}^d \\alpha_i w_i \\\\ &amp; = \\sum_{i=0}^d \\alpha_i p_i(A)w_0 &amp;&amp; \\text{for some }\\alpha_i\\in\\mathbb{C}\\\\ &amp; = p(A)w_0 &amp;&amp; \\text{for some }p\\in \\mathbb{C}[\\lambda] \\end{align}\\] Hence, \\[\\begin{align} p_{d+1}(A)w &amp; = p_{d+1}(A)p(A)w_0\\\\ &amp; = p(A)p_{d+1}(A)w_0\\\\ &amp; = 0. \\end{align}\\] Note that \\(p_{d+1}\\) is the minimal polynomial. Pf. Suppose \\(q(A)W = 0\\) for some \\(0\\neq q\\in \\mathbb{C}[\\lambda]\\) with \\(\\deg q &lt; \\deg p_{d+1} = d+1\\). Then, \\[q = \\sum_{i=0}^d\\beta_ip_i \\quad \\text{for some }\\beta_i\\in \\mathbb{C}.\\] We have, \\[0 = q(A)w_0 = \\sum_{i=0}^d \\beta_iw_i.\\] Hence \\(\\beta_0 = \\cdots = \\beta_d = 0\\) by \\((iiia)\\). Thus \\(q = 0\\) and a contradiction. Corollary 9.1 Let \\(\\Gamma\\), \\(W\\), \\(r\\), \\(d\\) be as above. Then \\((i)\\) \\(W\\) is dual thin, that is, \\[\\dim E_iW \\leq 1 \\quad (1\\leq i \\leq d).\\] \\((ii)\\) \\(d = |\\{i \\mid E_iW \\neq 0\\}| - 1.\\) Proof. \\((i)\\) Set as in Lemma 9.1, \\[w_i = p_i(A)w_0\\in E^*_{r+i}W.\\] Then \\(w_0, w_1, \\ldots, w_d\\) is a basis for \\(W\\). We have \\[W = Mw_0.\\] So, \\[E_iW = E_iMw_0 = \\mathrm{Span}(E_iw_0).\\] Thus, \\[\\dim E_iW = \\begin{cases}1 &amp; \\text{if } E_iw_0\\neq 0\\\\ 0 &amp; \\text{if }E_iw_0 = 0.\\end{cases}\\] In particular, \\[\\dim E^*_iW \\leq 1.\\] \\((ii)\\) Immediate as \\[\\dim W = d+1.\\] This proves the lemma. Lemma 9.2 Given an irreducible \\(T(x)\\)-module \\(W\\) with endpoint \\(r = r(W)\\), diameter \\(d = d(W)\\). Write \\[x_i = x_i(W) \\; (0\\leq i\\leq d), \\quad w_i = p_i(A)w_0\\in E^*_{r+i}W \\; (0\\leq i\\leq d), \\quad 0\\neq w_0 \\in E^*_rW.\\] Then, \\[\\frac{\\|w_i\\|^2}{\\|w_0\\|^2} = x_1x_2\\cdots x_i \\quad (1\\leq i\\leq d).\\] Proof. It suffices to show that \\[\\|w_i\\|^2 = x_i\\|w_i\\|^2 \\quad (1\\leq i\\leq d).\\] Recall by Lemma 9.1 \\((iiib)\\) that \\[Aw_j = w_{j+1} + a_jw_j + x_jw_{j-1} \\quad (0\\leq j\\leq d), \\quad w_{-1} = w_{d+1} = 0.\\] Now observe, \\[\\begin{align} \\langle w_{i-1}, Aw_i\\rangle &amp; = \\langle w_{i-1}, w_{i+1}+ a_iw_i + x_iw_{i-1}\\rangle\\\\ &amp; = \\overline{x_i}\\|w_{i-1}\\|^2\\\\ &amp; = x_i\\|w_{i-1}\\|^2. \\end{align}\\] by Lemma 9.1 \\((ii)\\). Also, \\[\\begin{align} \\langle w_{i-1}, Aw_i\\rangle &amp; = \\langle Aw_{i-1}, w_i\\rangle \\quad (text{since}\\; \\bar{A}^\\top = A)\\\\ &amp; = \\langle x_i + a_{i-1}w_{i-1} + x_{i-1}x_{i-2}, w_i\\rangle\\\\ &amp; = \\|w_i\\|^w. \\end{align}\\] This proves the lemma. Definition 9.2 Let \\(W\\) be an irreducible thin \\(T(x)\\) module with endpoint \\(r\\), \\(E^*_i \\equiv E_i^*(x)\\). The measure \\(m = m_W\\) is the function \\[m: \\mathbb{R} \\to \\mathbb{R}\\] such that \\[ m(\\theta) = \\begin{cases}\\frac{\\|E_iw\\|^2}{\\|w\\|^2} &amp; \\text{where } 0\\neq w \\in E^*_rW\\\\ &amp; \\text{ if $\\theta = \\theta_i$ is an eigenvalue for $\\Gamma$}\\\\ 0 &amp; \\text{if $\\theta$ is not an eigenvalue for $\\Gamma$.} \\end{cases}\\] "],["lec10.html", "Chapter 10 Thin \\(T\\)-Module, II", " Chapter 10 Thin \\(T\\)-Module, II Wednesday, February 10, 1993 Let \\(\\Gamma = (X, E)\\) be any graph. Fix a vertex \\(x\\in X\\). Let \\(E^*_i \\equiv E^*_i(x)\\), \\(T\\equiv T(x)\\), the subconstituent algebra over \\(\\mathbb{C}\\), and \\(V = \\mathbb{C}^{|X|}\\) the standard module. Lemma 10.1 With above notation, let \\(W\\) denote a thin irreducible \\(T(x)\\)-module with endpoint \\(r\\) and diameter \\(d\\). Let \\[\\begin{align} a_i &amp; = a_i(W) \\quad (0\\leq i \\leq d)\\\\ x_i &amp; = x_i(W) \\quad (1\\leq i \\leq d)\\\\ p_i &amp; = p_i(W) \\quad (0\\leq i \\leq d+1) \\end{align}\\] be from Lemma 9.1, and measure \\(m = m_W\\). Then, \\((i)\\) \\(p_0, \\ldots, p_{d+1}\\) are orthogonal with respect to \\(m\\), i.e., \\[\\sum_{\\theta\\in \\mathbb{R}}p_i(\\theta)p_j(\\theta)m(\\theta) = \\delta_{ij}x_1x_2\\cdots x_i \\quad (0\\leq i,j\\leq d+1) \\text{ with }\\; x_{d+1}=0.\\] \\(\\quad (ia)\\) \\({\\displaystyle \\sum_{\\theta\\in \\mathbb{R}}p_i(\\theta)^2m(\\theta) = x_1\\cdots x_i \\quad (0\\leq i\\leq d)}\\). \\(\\quad (iia)\\) \\({\\displaystyle \\sum_{\\theta\\in \\mathbb{R}}m(\\theta) = 1}\\). \\(\\quad (iiia)\\) \\({\\displaystyle \\sum_{\\theta\\in \\mathbb{R}}p_i(\\theta)^2\\theta m(\\theta) = x_1\\cdots x_ia_i \\quad (0\\leq i\\leq d)}\\). Proof. Pick \\(0\\neq w_0\\in E^*_rW\\). Set \\[w_i = p_i(A)w_0 \\in E^*_{r+i}W.\\] Since \\(E^*_iW\\) and \\(E^*_jW\\) are orthogonal if \\(i\\neq j\\), \\[\\begin{align} \\delta_{ij}\\|w_i\\|^2 &amp; = \\langle w_i, w_j\\rangle\\\\ &amp; = \\langle p_i(A)w_0, p_j(A)w_0\\rangle\\\\ &amp; = \\left\\langle p_i(A)\\left(\\sum_{\\ell=0}^R E_\\ell\\right)w_0, p_j(A)\\left(\\sum_{\\ell=0}^R E_\\ell\\right)w_0\\right\\rangle\\\\ &amp; = \\left\\langle \\sum_{\\ell=0}^R p_i(\\theta_\\ell)E_\\ell w_0, \\sum_{\\ell=0}^R p_j(\\theta_\\ell)E_\\ell w_0\\right\\rangle &amp;&amp; (\\text{as } AE_j = \\theta_jE_j)\\\\ &amp; = \\sum_{\\ell=0}^R p_i(\\theta_\\ell)\\overline{p_j(\\theta_\\ell)}\\|E_\\ell w_0\\|^2\\\\ &amp; \\qquad\\qquad (\\text{as } \\; p_j\\in \\mathbb{R}[\\lambda], \\quad \\theta_\\ell\\in \\mathbb{R}, \\quad m(\\theta_i)\\|w_0\\|^2 = \\|E_iw_0\\|^2)\\\\ &amp; = \\sum_{\\theta\\in \\mathbb{R}} p_i(\\theta)p_j(\\theta)m(\\theta)\\|w_0\\|^2. \\end{align}\\] Now we are done by Lemma 9.2 as \\[\\|w_i\\|^2 = \\|w_0\\|^2 x_1x_2\\ldots x_i.\\] For \\((ia)\\), set \\(i = j\\), and for \\((ib)\\), set \\(i=j=0\\). \\((ii)\\) We have \\[\\begin{align} \\langle w_i,Aw_i\\rangle &amp; = \\langle w_i, w_{i+1} + a_iw_i + x_i w_{i-1}\\rangle\\\\ &amp; = \\overline{a_i}\\|w_i\\|^2\\\\ &amp; = a_i x_1\\cdots x_i\\|w_0\\|^2, \\end{align}\\] as \\(a_i\\in \\mathbb{R}\\) by Lemma 9.1. Also, \\[\\begin{align} \\langle w_i, Aw_i\\rangle &amp; = \\langle p_i(A)w_0, Ap_i(A)w_0\\rangle \\\\ &amp; = \\left\\langle p_i(A)\\left(\\sum_{\\ell=0}^R E_\\ell\\right)w_0, A p_i(A)\\left(\\sum_{\\ell=0}^R E_\\ell\\right)w_0\\right\\rangle &amp;&amp; (\\text{ as in $(i)$})\\\\ &amp; = \\sum_{\\ell = 0}^D p_i(\\theta_\\ell)^2 \\theta_\\ell \\|E_\\ell w_0\\|^2\\\\ &amp; = \\sum_{\\theta\\in \\mathbb{R}}p_i(\\theta)^2\\theta m(\\theta)\\|w_0\\|^2. \\end{align}\\] Thus, we have \\((ii)\\). Lemma 10.2 With above notation, let \\(W\\) be a thin irreducible \\(T(x)\\)-module with measure \\(m\\). Then \\(m\\) determines diameter \\(d(W)\\), \\[\\begin{align} a_i &amp; = a_i(W) \\quad (0\\leq i\\leq d)\\\\ x_i &amp; = x_i(W) \\quad (1\\leq i\\leq d)\\\\ p_i &amp; = p_i(W) \\quad (0\\leq i\\leq d+1). \\end{align}\\] Proof. Note that \\(d+1\\) is the number of \\(\\theta\\in \\mathbb{R}\\) such that \\(m(\\theta)\\neq 0\\). Hence \\(m\\) determines \\(d\\). Apply \\((ia)\\), \\((ii)\\) of Lemma 10.1. \\[\\begin{align} &amp; \\sum_{\\theta\\in\\mathbb{R}}m(\\theta) = 1 &amp;&amp; p_0 =1.\\\\ &amp; \\sum_{\\theta\\in\\mathbb{R}}\\theta m(\\theta) = a_0 &amp;&amp; p_1 = \\lambda - a_0\\\\ &amp; \\sum_{\\theta\\in\\mathbb{R}}p_1(\\theta)^2 m(\\theta) = x_1 \\\\ &amp; \\sum_{\\theta\\in\\mathbb{R}}p_1(\\theta)^2\\theta m(\\theta) = x_1a &amp;&amp; \\to a_1\\\\ &amp; \\qquad p_2 = (\\lambda - a_1)p_1 - x_1p_0\\\\ &amp; \\sum_{\\theta\\in\\mathbb{R}}p_2(\\theta)^2 m(\\theta) = x_1x_2 &amp;&amp; \\to x_2\\\\ &amp; \\sum_{\\theta\\in\\mathbb{R}}p_2(\\theta)^2\\theta m(\\theta) = x_1x_2a_2 &amp;&amp; \\to a_2\\\\ &amp; \\qquad p_3 = (\\lambda-a_2)p_2 - x_2p_1\\\\ &amp; \\qquad\\qquad \\vdots\\\\ &amp; \\sum_{\\theta\\in\\mathbb{R}}p_d(\\theta)^2 m(\\theta) = x_1x_2\\cdots x_d &amp;&amp; \\to x_d\\\\ &amp; \\sum_{\\theta\\in\\mathbb{R}}p_d(\\theta)^2\\theta m(\\theta) = x_1x_2\\cdots x_da_d &amp;&amp; \\to a_d\\\\ &amp; \\qquad p_{d+1} = (\\lambda-a_d)p_d - x_dp_{d-1}.\\\\ \\end{align}\\] This proves the assertions. Corollary 10.1 With above notation, let \\(W\\), \\(W&#39;\\) denote thin irreducible \\(T(x)\\)-modules. The following are equivalent. \\((i)\\) \\(W\\), \\(W\\) are isomorpphic as \\(T\\)-modoles. \\((ii)\\) \\(r(W) = r(W&#39;)\\) and \\(m_W = m_{W&#39;}\\). \\((iii)\\) \\(r(W) = r(W&#39;)\\), \\(d(W) = d(W&#39;)\\), \\(a_i(W) = a_i(W&#39;)\\) amd \\(x_i(W) = x_i(W&#39;)\\) \\(\\quad (0\\leq i\\leq d)\\). Proof. \\((i)\\Rightarrow (iii)\\) Write \\(r\\equiv r(W)\\), \\(r&#39; \\equiv r(W&#39;)\\), \\(d = d(W)\\), \\(d&#39; = d(W&#39;)\\), \\(a_i = a_i(W)\\), \\(a_i&#39; = a_i(W&#39;)\\), \\(x_i = x_i(W)\\) and \\(x_i&#39; = x_i(W&#39;)\\). Let \\(\\sigma: W\\to W&#39;\\) denote an isomorphism of \\(T\\)-modules. (See Definition 5.1.) For every \\(i\\), \\[\\sigma E^*_iW = E^*_i\\sigma W = E^*_iW&#39;.\\] So, \\(r = r&#39;\\) and \\(d = d&#39;\\). To show \\(a_i = a_i&#39;\\), pick \\(w\\in E^*_{r+i}W \\setminus \\{0\\}\\). Then, \\[E^*_{r+i}AE^*_{r+i}\\sigma (W) = \\sigma(E^*_{r+i}AE^*_{r+i}w) = \\sigma(a_iw) = a_i\\sigma(w), \\] and \\(\\sigma w\\neq 0\\). So, \\[\\begin{align} a_i &amp; = \\text{eigenvalue of $E^*_{r+i}AE^*_{r+i}$ on $E^*_{r+i}W$}\\\\ &amp; = a_i&#39; \\end{align}\\] It is similar to show \\(x = x&#39;\\). Remark. Pick \\(w\\in E^*_{r+i-1}W \\setminus \\{0\\}\\) \\[E^*_{r+i-1}AE^*_{r+i}AE^*_{r+i-1}\\sigma(W) = \\sigma(E^*_{r+i-1}AE^*_{r+i}AE^*_{r+i-1}w) = x_i\\sigma(w).\\] Hence, \\(x_i\\) is the eigenvalue of \\(E^*_{r+i-1}AE^*_{r+i}AE^*_{r+i-1}\\) on \\(E^*_{r+i-1}W = x_i&#39;\\). \\((iii)\\Rightarrow (i)\\) Pick \\(0\\neq w_0\\in E^*_rW\\), \\(0\\neq w_0&#39;\\in E^*_rW&#39;\\). Let \\(p_i\\) be in Lemma 9.1, and set \\[\\begin{align} w_i &amp; = p_i(A)w_0\\in E^*_{r+i}W \\quad (0\\leq i\\leq d) \\\\ w_i&#39; &amp; = p_i&#39;(A)w_0&#39; \\in E^*_{r+i}W \\quad (0\\leq i\\leq d) \\end{align}\\] Define a linear transformation, \\[\\sigma: W \\to W&#39; \\quad (w_i \\mapsto w_i&#39;).\\] Since \\(\\{w_i\\}\\) and \\(\\{w_i&#39;\\}\\) are bases with \\(d = d&#39;\\), \\(\\sigma\\) is an isomorphism of vector spaces. We need to show \\[a\\sigma = \\sigma a \\quad (\\text{for all }\\; a\\in T).\\] Take \\(a = E^*_j\\) for some \\(j\\) \\((0\\leq j\\leq d(x))\\). Then for all \\(i\\), we have \\[E^*_j \\sigma w_i = E^*_jw_i&#39; = \\delta_{ij}w_i&#39;,\\] \\[\\sigma E^*_jw_i = \\delta_{ij}\\sigma(w_i) = \\delta_{ij}w_i&#39;.\\] \\[E^*_j \\sigma w_i = \\sigma E^*_jw_i?\\] Take an adjacency matrix \\(A\\) of \\(a\\). Then, \\[A\\sigma w_i = Aw_i&#39; = w_{i+1}&#39; + a_i&#39;w_i&#39; + x_i&#39;w_{i-1}&#39; = \\sigma(w_{i+1} + a_iw_i + x_iw_{i-1}) = \\sigma Aw_i.\\] \\((ii)\\Rightarrow (iii)\\) Lemma 10.2. \\((iii)\\Rightarrow (ii)\\) Given \\(d\\), \\(a_i\\), \\(x_i\\), we can compute the polynomial sequence \\[p_0, p_1, \\ldots, p_{d+1}\\] for \\(W\\). Show \\(p_0, p_1, \\ldots, p_{d+1}\\) determines \\(m = m_W\\). Set \\[\\Delta = \\{\\theta\\in \\mathbb{R}\\mid p_{d+1}(\\theta) = 0\\}.\\] Observe: \\(|\\Delta| = d+1\\). See ‘An Introcuction to Interlacing’. \\(m(\\theta) = 0\\) if \\(\\theta\\not\\in\\Delta\\quad (\\theta\\in \\mathbb{R})\\). So it suffices to find \\(m(\\theta)\\), \\(\\theta\\in \\Delta\\). By Lemma 10.1 \\((i)\\), \\[ \\begin{cases} \\sum_{\\theta\\in\\Delta} m(\\theta)p_0(\\theta) &amp; = 1\\\\ \\sum_{\\theta\\in\\Delta} m(\\theta)p_1(\\theta) &amp; = 0\\\\ \\qquad \\vdots &amp; \\\\ \\sum_{\\theta\\in\\Delta} m(\\theta)p_d(\\theta) &amp; = 0 \\end{cases} \\] \\(d+1\\) linear equation with \\(d+1\\) unknowns \\(m(\\theta)\\) (\\(\\theta\\in \\Delta\\)). But the coefficient matrix is essentially Vander Monde (since \\(\\deg p_i = i\\)). Hence the system is nonsingular and there are unique values for \\(m(\\theta)\\) \\((\\theta\\in \\Delta)\\). Remark. \\[\\begin{pmatrix} \\theta-a_0 &amp; -1 &amp; \\cdots &amp; 0 &amp; 0 \\\\ -x_1 &amp; \\theta - a_1 &amp; \\cdots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\theta-a_{d-1} &amp; -1\\\\ 0 &amp; 0 &amp; \\cdots &amp; -x_d &amp; \\theta - a_d \\end{pmatrix} \\begin{pmatrix} p_0(\\theta)\\\\ \\vdots\\\\ \\vdots\\\\ \\vdots\\\\ p_d(\\theta) \\end{pmatrix} = 0,\\] where \\(\\theta\\) is an eigenvalue of a diagonalizable matrix \\[ L = \\begin{pmatrix} a_0 &amp; 1 &amp; \\cdots &amp; 0 &amp; 0 \\\\ x_1 &amp; a_1 &amp; \\cdots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; a_{d-1} &amp; 1\\\\ 0 &amp; 0 &amp; \\cdots &amp; x_d &amp; \\theta a_d \\end{pmatrix} \\] with multiplicity \\(\\dim (\\mathrm{Ker}(\\theta I - L) = 1)\\). "],["lec11.html", "Chapter 11 Examples of \\(T\\)-Module", " Chapter 11 Examples of \\(T\\)-Module Friday, February 12, 1993 Let \\(\\Gamma = (X, E)\\) be a connected graph. Let \\(\\theta_0\\) be the maximal eigenvalue of \\(\\Gamma\\), and \\(\\delta\\) its corresponding eigenvector. \\[\\delta = \\sum_{y\\in X}\\delta_y \\hat{y}.\\] Without loss of generality, we may assume that \\(\\delta_y\\in \\mathbb{R}^*\\) for all \\(y\\in X\\). Lemma 11.1 Fix a vertex \\(x\\in X\\). Write \\(T \\equiv T(x)\\), \\(E^*_i\\equiv E^*_i(x)\\). \\((i)\\) \\(T\\delta = T\\hat{x}\\) is an irreducible \\(T\\)-module. \\((ii)\\) Given any irreducible \\(T\\)-module \\(W\\), the following are equivalent: \\(\\quad (iia)\\) \\(W = T\\delta\\). \\(\\quad (iib)\\) The diameter \\(d(W) = d(x)\\). \\(\\quad (iic)\\) The endpoint \\(r(W) = 0\\). Proof. \\((i)\\) Observe: there exists an irreducible \\(T\\)-module \\(W\\) that contains \\(\\delta\\). Let \\(V = \\sum_{i}W_i\\) be a direct sum decomposition of the standard module. Then \\[\\mathrm{Span}(\\delta) = E_0V = \\sum_{i}E_0W_i.\\] So, \\(E_0W_i \\neq 0\\) for some \\(i\\). Then, \\[\\delta \\in E_0W_i \\subseteq W_i.\\] Observe: \\(T\\delta\\) is an irreducible \\(T\\)-module. Since \\(\\delta\\in W\\), where \\(W\\) is a \\(T\\)-module. As \\(T\\delta \\subseteq W\\) and \\(W\\) is irreducible, \\(T\\delta = W\\). Observe: \\(T\\delta = T\\hat{x}\\). Since \\(\\hat{x} = \\delta_x^{-1}E^*_0\\delta \\in T\\delta\\), \\(T\\hat{x} \\subseteq T\\delta\\). Since \\(T\\delta\\) is irreducible, \\(T\\hat{x} = T\\delta\\). \\((ii)\\) \\((a)\\to (b)\\): \\[E^*_i\\delta = \\sum_{y\\in X, \\partial(x,y) = i}\\delta_y\\hat{y} \\neq 0, \\quad (0\\leq i\\leq d(x)), \\] because \\(\\delta_y &gt;0\\) for every \\(y\\in X\\). Hence, \\[E^*_iT\\delta \\neq 0, \\quad (0\\leq i\\leq d(x)).\\] Thus, \\(d(x) = d(W)\\). \\((b)\\to (c)\\): Immediate. \\((c)\\to (a)\\): Since \\(r(W) = 0\\), \\(E^*_0W \\neq 0\\). Hence, \\(\\hat{x}\\in W\\) and \\(T\\hat{x} \\subseteq W\\). By the irreduciblity, we have \\(T\\hat{x} = W\\). Lemma 11.2 Assume \\(\\Gamma\\) is bipartite \\((X = X^+ \\cup X^-)\\) (\\(X^+\\) and \\(X^-\\) are nonempty). Then the following are equivalent. \\((i)\\) There exist \\(\\alpha^+\\) and \\(\\alpha^-\\in \\mathbb{R}\\) such that \\[\\delta_x = \\begin{cases} \\alpha^+ &amp; \\text{if }\\: x\\in X^+\\\\ \\alpha^- &amp; \\text{if } x\\in X^-. \\end{cases}\\] | \\((ii)\\) There exist \\(k+\\) and \\(k^-\\in \\mathbb{Z}^{&gt;0}\\) such that \\[k(x) = \\begin{cases} k^+ &amp; \\text{if }\\: x\\in X^+\\\\ k^- &amp; \\text{if } x\\in X^-. \\end{cases}\\] In this xase, \\(k^+k^- = \\theta_0^2\\), and \\(\\Gamma\\) is called bi-regular. Proof. \\((i)\\to(ii)\\) \\[\\begin{align} A\\delta &amp; = A\\left(\\alpha^+\\sum_{x\\in X^+}\\hat{x} + \\alpha^-\\sum_{y\\in X^-}\\hat{y}\\right)\\\\ &amp; = \\alpha^+\\sum_{y\\in X^-}k(y)\\hat{y} + \\alpha^-\\sum_{x\\in X^+}k(x)\\hat{x}\\\\ &amp; = \\theta_0\\delta. \\end{align}\\] So, \\[k(x)\\alpha^- = \\theta_0\\alpha^+, \\quad k(y)\\alpha^+ = \\theta_0\\alpha^-.\\] As \\(\\alpha^+\\neq =\\) and \\(\\alpha^- \\neq 0\\), \\[\\begin{align} k^+ &amp; := k(x) \\; \\text{ is independent of the choice of $x\\in X^+$, and}\\\\ k^- &amp; := k(y) \\; \\text{ is independent of the choice of $y\\in X^-$.} \\end{align}\\] Moreover, \\(k^+k^- = \\theta_0^2\\). \\((ii)\\to(i)\\) Set \\[\\delta&#39; = \\sum_{y\\in X}\\alpha_y\\hat{y} \\quad \\text{where}\\; \\alpha = \\begin{cases} 1/\\sqrt{k^-} &amp; \\text{if }\\; y\\in X^+\\\\ 1/\\sqrt{k^+} &amp; \\text{if }\\; y\\in X^-.\\end{cases}\\] Then one checks \\[\\begin{align} A\\delta&#39; &amp; = A\\left(\\frac{1}{\\sqrt{k^-}}\\sum_{y\\in X^+}\\hat{y} + \\frac{1}{\\sqrt{k^+}}\\sum_{y\\in X^-}\\hat{y}\\right)\\\\ &amp; = \\frac{k^-}{\\sqrt{k^-}}\\sum_{y\\in X^-}\\hat{y} + \\frac{k^+}{\\sqrt{k^+}}\\sum_{y\\in X^+}\\hat{y}\\\\ &amp; = \\sqrt{k^+k^-}\\delta&#39; \\end{align}\\] Since \\(\\delta&#39; &gt;0\\), \\(\\delta&#39;\\in \\mathrm{Span}(\\delta)\\), and \\(\\theta_0 = \\sqrt{k^+k^-}\\). Definition 11.1 For any graph \\(\\Gamma = (X, E)\\), fix a vertex \\(x\\in X\\). Set \\(d = d(x)\\). \\(\\Gamma\\) is distance-regular with respect to \\(x\\), if for all \\(i\\) : (0id), and all \\(y\\in X\\) such that \\(\\partial(x,y) = i\\): \\[\\begin{align} c_i(x) &amp; := |\\{z\\in X \\mid \\partial(x,z) = i-1, \\; \\partial(y,z) = 1\\}|\\\\ a_i(x) &amp; := |\\{z\\in X \\mid \\partial(x,z) = i, \\; \\partial(y,z) = 1\\}|\\\\ b_i(x) &amp; := |\\{z\\in X \\mid \\partial(x,z) = i+1, \\; \\partial(y,z) = 1\\}| \\end{align}\\] depends only on \\(i\\), \\(x\\), and not on \\(y\\). (In this case, \\(c_0(x) = a_0(x) = b_d(x) = 0\\), \\(c_1(x) = 1\\), \\(b_0(x) = k(x)\\) is the valency of \\(x\\).) We call \\(c_i(x)\\), \\(a_i(x)\\) and \\(b_i(x)\\) the intersection numbers with respect to \\(x\\). Example 11.1 \\[\\begin{align} c_0 &amp;= 1 &amp; c_1 &amp;= 1 &amp; c_2 &amp;= 1\\\\ a_0 &amp;= 0 &amp; a_1 &amp;= 1 &amp; a_2 &amp;= 1\\\\ b_0 &amp;= 2 &amp; b_1 &amp;= 1 &amp; b_2 &amp;= 0 \\end{align}\\] "],["lec12.html", "Chapter 12 Distance-Regular", " Chapter 12 Distance-Regular Monday, February 15, 1993 Lemma 12.1 For any connected graph \\(\\Gamma = (X, E)\\), the following are equivalent. \\((i)\\) The trivial \\(T(x)\\)-module is thin for all \\(x\\in X\\). \\((ii)\\) \\(\\displaystyle{\\left\\{\\sum_{y\\in X, \\partial(x,y) = i}\\hat{y} \\left| 0\\leq i\\leq d(x)\\right.\\right\\}}\\) is a basis for the trivial \\(T(x)\\)-module for every \\(x\\in X\\). \\((iii)\\) \\(\\Gamma\\) is distance-regular with respect to \\(x\\) for all \\(x\\in X\\). Note. Let \\(\\Gamma = (X, E)\\) be a graph, with \\(X = \\{x, y_1, y_2, y_3, z_1, z_2, z_3\\}\\), \\(E = \\{xy_1, xy_2, xy_3, y_1z_1, y_1z_2, y_2z_3, y_3z_3\\}\\). Then \\((i)\\), \\((ii)\\) are not equivalent for a single vertex \\(x\\). \\[\\begin{align} E^*_0T\\hat{x} &amp; = \\langle \\hat{x}\\rangle, \\\\ E^*_1T\\hat{x} &amp; = \\langle y_1 + y_2 + y_3\\rangle, \\\\ E^*_2T\\hat{x} &amp; = \\langle z_1 + z_2 + 2z_3\\rangle. \\end{align}\\] Proof (Proof of Lemma 12.1). \\((i)\\to (ii)\\) Let \\(\\delta = \\sum_{y\\in X}\\delta_y\\hat{y}\\) be an eigenvector for the maximal eigenvalue \\(\\theta_0\\). Then, \\[\\begin{align} \\sum_{y\\in X, \\partial(x,y) = 1}\\hat{y} &amp; = A\\hat{x} \\in T(x)\\hat{x} = T(x)\\delta \\ni E^*_1\\delta\\\\ &amp; = \\sum_{y\\in X, \\partial(x,y)=1}\\delta_h\\hat{y} \\end{align}\\] If the trivial \\(T(x)\\)-module is thin, \\[\\delta_y = \\delta_z \\; \\text{ for }\\; y, z\\in X, \\; \\partial(x,y) = \\partial(x,z) = 1.\\] Hence, \\(\\delta_y = \\delta_z\\) if \\(y\\) and \\(z\\) in \\(X\\) are connected by a path of even length. So, \\(\\Gamma\\) is regular or bipartite biregular by Lemma 11.2. In particular, \\(\\delta_y = \\delta_z\\) if \\(\\partial(x,y) = \\partial(x,z)\\), as there is a path of length \\(2\\cdot \\partial(x,y)\\); \\[y\\sim \\cdots \\sim x \\sim \\cdots \\sim z.\\] Hence, \\[E^*_i\\delta \\in \\mathrm{Span}\\left(\\sum_{y\\in X, \\partial(x,y) = i}\\hat{y}\\right).\\] Since \\(E^*_0\\delta, E^*_1\\delta, \\ldots, E^*_d\\delta\\) forms a basis for \\(T(x)\\delta\\), we have \\((ii)\\). \\((ii)\\to (iii)\\) Fix \\(x\\in X\\), and let \\(T \\equiv T(x)\\), \\(E^*_i\\equiv E^*_i(x)\\), and \\(d \\equiv d(x)\\). \\[\\begin{align} A\\sum_{y\\in X, \\partial(x.y)=i}\\hat{y} &amp; = \\sum_{z\\in X} |\\{y\\in X \\mid \\partial(y,z) = 1, \\; \\partial(x,y) = i\\}|\\hat{z}\\\\ &amp; = \\sum_{z\\in X, \\partial(x,y) = i-1}b_{i-1}(x,z)\\hat{z} \\\\ &amp; \\qquad + \\sum_{z\\in X, \\partial(x,y) = i} a_{i}(x,z)\\hat{z} \\\\ &amp; \\qquad + \\sum_{z\\in X, \\partial(x,y) = i+1} c_{i+1}(x,z)\\hat{z}\\\\ &amp; \\in \\mathrm{Span}\\left\\{\\left.\\sum_{z\\in X, \\partial(x,z) = j}\\hat{z} \\; \\right| \\; j = 0, 1, \\ldots, d \\right\\}. \\end{align}\\] Hence, \\(b_{i-1}(x,z)\\), \\(a_i(x,z)\\) and \\(c_{i+1}(x,z)\\) depend only on \\(i\\) and \\(x\\), and not on \\(z\\). Therefore, \\(\\Gamma\\) is distance-regular with respect to \\(x\\). \\((iii)\\to (i)\\) Fix \\(x\\in X\\), and let \\(T \\equiv T(x)\\), \\(E^*_i\\equiv E^*_i(x)\\), and \\(d \\equiv d(x)\\). By defintion of distance-regularity, for every \\(i\\) \\((0\\leq i\\leq d)\\), \\[\\begin{align} A\\left(\\sum_{y\\in X, \\partial(x,y)=i}\\hat{y}\\right) &amp; = b_{i-1}(x)\\sum_{y\\in X, \\partial(x,y) = i-1}\\hat{y} \\\\ &amp; \\qquad + a_{i}(x)\\sum_{y\\in X, \\partial(x,y) = i}\\hat{y}\\\\ &amp; \\qquad + c_{i+1}(x)\\sum_{y\\in X, \\partial(x,y) = i+1}\\hat{y}. \\end{align}\\] Hence, \\[W = \\left\\{\\left.\\sum_{y\\in X, \\partial(x,y)=i}\\hat{y} \\; \\right| \\; 0\\leq i\\leq d\\; \\right\\}\\] is \\(A\\)-invariant and so \\(T\\)-invariant. Since \\(\\hat{x}\\in W\\), \\(T\\hat{x} = W\\) is the trivial module and \\(T\\hat{x}\\) is thin. Next, we show more is true if \\((i)-(iii)\\) hold in Lemma 12.1. In fact, \\(d(x)\\), \\(a_i(x)\\), \\(c_i(x)\\), and \\(b_i(x)\\) are \\[\\begin{cases} \\text{independent of $X$} &amp; \\text{if $\\Gamma$ is regular; or}\\\\ \\text{constant over $X^+$ and $X^-$} &amp; \\text{if $\\Gamma$ is biregular.} \\end{cases}\\] Let \\(\\Gamma = (X, E)\\) be any (connected) graph. Pick vertices \\(x, y\\in X\\). Let \\(W\\) be a thin, irreducible \\(T(x)\\)-module, and measure \\(m: \\mathbb{R} \\to \\mathbb{R}\\) determined by \\(W\\). Let \\(W&#39;\\) be a thin, irreducible \\(T(y)\\)-module, and measure \\(m: \\mathbb{R} \\to \\mathbb{R}\\) determined by \\(W&#39;\\). Recall \\(W\\), \\(W&#39;\\) are orthogonal if \\[\\langle w, w&#39;\\rangle = 0 \\quad \\text{for all }w\\in W, w&#39;\\in W&#39;.\\] We shall show if \\(W\\) and \\(W&#39;\\) are note orthogonal, then \\(m\\) and \\(m&#39;\\) are related: \\[m\\cdot \\mathrm{poly}_1 = m&#39;\\cdot \\mathrm{poly}_2\\] for some polynomials with \\[\\deg \\mathrm{poly}_1 + \\deg \\mathrm{poly}_2 \\leq 2\\cdot \\partial(x,y).\\] Notation. \\(V\\): standard module of \\(\\Gamma\\). \\(H\\): any subspace of \\(V\\). \\[V = H + H^\\bot \\quad \\text{orthogonal direct sum},\\] and for \\(v = v_1 + v_2\\) \\(\\mathrm{proj}_H: V\\to H \\; (v\\mapsto v_1)\\): linear transformation. Observe: For every \\(v\\in V\\), \\[v - \\mathrm{proj}_H v \\in H^\\bot.\\] So, \\[\\langle v - \\mathrm{proj}_H v, h\\rangle = 0 \\quad \\text{for all }\\;h\\in H \\text{ or},\\] \\[\\langle v, h\\rangle = \\langle \\mathrm{proj}_H v, h\\rangle \\quad \\text{for all }\\;v\\in V, \\;\\text{ and for all }\\: h\\in H.\\] Theorem 12.1 Let \\(\\Gamma = (X,E)\\) be any graph. Pick vertices \\(x,y\\in X\\) and set \\(\\Delta = \\partial(x,y)\\). Assume \\(W\\): thin irreducible \\(T(x)\\)-module with endpoint \\(r\\), diameter \\(d\\), and measure \\(m\\). \\(W&#39;\\): thin irreducible \\(T(y)\\)-module with endpoint \\(r&#39;\\), diameter \\(d&#39;\\), and measure \\(m&#39;\\). \\(W\\) and \\(W&#39;\\) are not orghotonal. Now ppick \\[0\\neq w\\in E^*_r(x)W, \\quad 0\\neq w\\in E^*_{r&#39;}(x)W&#39;.\\] Then, \\((i)\\) \\({\\displaystyle \\mathrm{proj}_{W&#39;}w = p(A)\\frac{\\|w\\|}{\\|w&#39;\\|}w&#39;}\\) for some \\(0\\neq p\\in \\mathbb{C}[\\lambda]\\) with \\(\\deg p \\leq \\Delta - r&#39; + r, d&#39;\\), \\({\\displaystyle \\mathrm{proj}_{W}w&#39; = p&#39;(A)\\frac{\\|w&#39;\\|}{\\|w\\|}w}\\) for some \\(0\\neq p&#39;\\in \\mathbb{C}[\\lambda]\\) with \\(\\deg p \\leq \\Delta - r + r&#39;, d\\). \\((ii)\\) For all eigenvalues \\(\\theta_i\\) of \\(\\Gamma\\), \\[\\frac{\\langle E_iw, E_iw&#39;\\rangle}{\\|w\\|\\|w&#39;\\|} = m(\\theta_i)\\overline{p&#39;(\\theta_i)}.\\] \\((iii)\\) For all eigenvalues \\(\\theta_i\\) of \\(\\Gamma\\), \\[p(\\theta_i)p&#39;(\\theta_i)\\] is in a real number in interval \\([0,1]\\). Proof. \\((i)\\) Since \\(W\\), \\(W&#39;\\) are not orthogonal, there exist \\[v\\in W, \\; v&#39;\\in W&#39; \\; \\text{ sich that }\\; \\langle v, v&#39;\\rangle \\neq 0.\\] Then there exists \\(a\\in M\\) such that \\[v&#39; = aw&#39;.\\] (This is becase \\(w&#39;_i = p&#39;_i(A)w_0&#39;\\) and hence for every \\(v&#39;\\in W&#39;\\), there is a polynomial \\(q\\in \\mathbb{C}[\\lambda]\\), \\(q(A)w_0&#39; = v\\).) We have \\[0\\neq \\langle v&#39;, v\\rangle = \\langle aw&#39;, v\\rangle = \\langle w&#39;, a^*v\\rangle\\] and \\(a^*v\\in W\\). Hence, \\(\\mathrm{proj}_{W} w&#39; \\neq 0\\). Let \\(p_0, \\ldots, p_d\\in \\mathbb{C}[\\lambda]\\) be from Lemma 9.1. Then, \\(w_i = p_i(A)w\\) is a basis for \\(E^*_{r+i}(x)W \\quad (0\\leq i\\leq d)\\). Hence, \\[\\mathrm{proj}_{W}w&#39; = \\alpha_0w_0 + \\cdots + \\alpha_dw_d \\quad \\text{for some }\\; \\alpha_j\\in \\mathbb{C}.\\] Set \\[p&#39; := \\frac{\\|w\\|}{\\|w&#39;\\|}\\sum_{i=0}^d \\alpha_ip_i.\\] Then \\(0\\neq p&#39;\\in \\mathbb{C}[\\lambda]\\) and \\(\\deg p&#39; \\leq d\\). Claim: \\(\\alpha_i = 0\\) \\((\\Delta - r + r&#39; &lt; i\\leq d)\\). In particular, \\(\\deg p&#39; \\leq \\Delta - r + r&#39;\\). Pf. Obseve: \\[w&#39;\\in E^*_{r&#39;}(y)V, \\quad w \\in E^*_r(x)V,\\] for \\(\\partial(x,y) = \\Delta\\). \\[E^*_{r&#39;}(y)V \\cap E^*_{r+i}(x)V = 0\\] by triangle inequality. (\\(\\Delta = \\partial(x,y) &lt; r+i - r&#39;\\) or \\(\\Delta + r&#39; &lt; r + i\\) by our choice of \\(i\\).) Hence, \\[E^*_{r&#39;}(y)V \\bot E^*_{r+i}(x)V,\\] or \\[\\begin{align} 0 &amp; = \\langle w&#39;, w_i\\rangle \\\\ &amp; = \\langle \\mathrm{proj}_{W}w&#39;, w_i\\rangle\\\\ &amp; = \\sum_{j=0}^d\\alpha_j\\langle w_j, w_i\\rangle\\\\ &amp; = \\alpha_i\\|w_i\\|^2. \\end{align}\\] Hence, \\(\\alpha_i = 0\\). Thus, \\[\\begin{align} \\mathrm{proj}_{W}w&#39; &amp; = \\sum_{i=0}^{\\Delta + r&#39; - r}\\alpha_iw_i\\\\ &amp; = \\sum_{i=0}^{\\Delta + r&#39; - r}\\alpha_ip_i(A)w_0\\\\ &amp; = p&#39;(A)\\frac{\\|w&#39;\\|}{\\|w\\|}w. \\end{align}\\] \\((ii)\\) We have \\[\\begin{align} \\frac{\\langle E_iw, E_iw&#39;\\rangle}{\\|w\\|\\|w&#39;\\|} &amp; = \\frac{\\langle E_iw, w&#39;\\rangle}{\\|w\\|\\|w&#39;\\|}\\\\ &amp; = \\frac{\\langle E_iw, \\mathrm{proj}_W w&#39;\\rangle}{\\|w\\|\\|w&#39;\\|} &amp;&amp; \\text{as }\\; \\mathrm{proj}_Ww&#39; = p&#39;(A)\\frac{\\|w\\|}{\\|w&#39;\\|}w\\\\ &amp; = \\frac{\\langle E_iw, p&#39;(A) w\\rangle}{\\|w\\|^2}\\\\ &amp; = \\frac{\\langle E_iw, E_ip&#39;(A) w\\rangle}{\\|w\\|^2}\\\\ &amp; = \\overline{p&#39;(\\theta_i)}\\frac{\\|E_iW\\|^2}{\\|w\\|^2}\\\\ &amp; = \\overline{p&#39;(\\theta_i)}m(\\theta_i). \\end{align}\\] Moreover, as \\(m(\\theta_i)\\), \\(m&#39;(\\theta_i)\\in \\mathbb{R}\\), \\[\\frac{\\langle E_iw, E_iw&#39;\\rangle}{\\|w\\|\\|w&#39;\\|} = \\frac{\\overline{\\langle E_iw, E_iw&#39;\\rangle}}{\\|w&#39;\\|\\|w\\|} = \\overline{\\overline{p(\\theta_i)}m&#39;(\\theta_i)} = p(\\theta_i)m&#39;(\\theta_i).\\] \\((iii)\\) Sicne, \\[\\frac{|\\langle E_iw, E_iw&#39;\\rangle\\|^2}{\\|w\\|^2\\|w&#39;\\|^2} = p(\\theta_i)p&#39;(\\theta_i)m(\\theta_i)m&#39;(\\theta_i),\\] \\[\\begin{align} p(\\theta_i)p&#39;(\\theta_i) &amp; = \\frac{|\\langle E_iw, E_iw&#39;\\rangle\\|^2}{m(\\theta_i)m&#39;(\\theta_i)\\|w\\|^2\\|w&#39;\\|^2} \\in \\mathbb{R}\\\\ &amp; = \\frac{|\\langle E_iw, E_iw&#39;\\rangle\\|^2}{\\frac{\\|E_iw\\|^2}{\\|w\\|^2}\\frac{\\|E_iw&#39;\\|^2}{\\|w&#39;\\|^2}\\|w\\|^2\\|w&#39;\\|^2}. \\end{align}\\] By Cauchy-Schwartz inequality, \\[(|\\langle a, b\\rangle | \\leq \\|a\\|\\|b\\|,)\\] \\[\\frac{|\\langle E_iw, E_iw&#39;\\rangle\\|^2}{\\|E_iw\\|^2\\|E_iw&#39;\\|^2} \\leq 1.\\] Hence, we have the assertion. "],["lec13.html", "Chapter 13 Modules of a DRG", " Chapter 13 Modules of a DRG Wednesday, February 17, 1993 "],["lec55.html", "Chapter 14 Title of the Chapter", " Chapter 14 Title of the Chapter Wednesday, February 17, 1993 # Edit Date "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
