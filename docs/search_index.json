[["index.html", "Lecture Note on Terwilliger Algebra About this lecturenote Setting Another Host", " Lecture Note on Terwilliger Algebra P. Terwilliger, edited by H. Suzuki 2022-11-14 About this lecturenote Setting This note is created by bookdown package on RStudio. Log-in to my GitHub Account Go to RStudio/bookdown-demo repository: https://github.com/rstudio/bookdown-demo Use This Template Input Repository Name Select Public - default Create repository from template From Code download ZIP Move the extracted folder into a favorite directory Open RStudio Project in the folder Use Terminal in the buttom left pane confirm that the current directory is the home directry of the project by pwd (failed to proceed by ssh) Use Console library(usethis) use_git() use_github() — Error gh_token_help() create_github_token(): create a token in the github page. Copy the token gitcreds::gitcreds_set(): paste the token, the token is to be expired in 30 days Use Terminal git remote add origin https://github.com/icu-hsuzuki/t-alagebra.git git push -u origin main type in the password of the computer Use GIT in R Studio Another Host library(usethis) use_git() create_github_token() gitcreds::gitcreds_set(): Replace these credentials "],["lec1.html", "Chapter 1 Lecture 1", " Chapter 1 Lecture 1 Wednesday, January 20, 1993 A graph (undirected, without loops or multiple edges) is a pair \\(\\Gamma = (X, E)\\), where \\[\\begin{align} X &amp;= \\textrm{finite set (of vertices)}\\\\ E &amp; = \\textrm{set of (distinct) 2-element subsets of }X \\textrm{ (= edges of ) }\\Gamma. \\end{align}\\] vertices \\(x\\) and \\(y\\in X\\) are adjacent if and only if \\(xy\\in E\\). Example 1.1 Let \\(\\Gamma\\) be a graph. \\(X = \\{a, b, c, d\\}\\), \\(E = \\{ab, ac, bc, bd\\}\\). Set \\(n = |X|\\), the order of \\(\\Gamma\\). Pick a field \\(K\\) (\\(=\\mathbb{R}\\) or \\(\\mathbb{C}\\)). Then \\(\\mathrm{Mat}_X(K)\\) denotes the \\(K\\) algebra of all \\(n\\times n\\) matrices with entries in \\(K\\). (rows and columns are indexed by \\(X\\)) Adjacency matrix \\(A\\in \\mathrm{Mat}_X(K)\\) is defined by \\[\\begin{align} A_{xy} &amp; = \\left\\{\\begin{array}{cl} 1 &amp; \\textrm{ if } \\; xy\\in E\\\\ 0 &amp; \\textrm{ else } . \\end{array}\\right. \\end{align}\\] Example 1.2 Let \\(a, b, c, d\\) be labels of rows and columns. Then \\[A = \\begin{matrix} \\\\ a\\\\ b\\\\c\\\\d\\end{matrix}\\begin{matrix}\\begin{matrix} a &amp; b &amp; c &amp; d \\end{matrix}\\\\\\begin{pmatrix} 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\end{pmatrix}\\end{matrix}\\] The subalgebra \\(M\\) of \\(\\mathrm{Mat}_X(K)\\) generated by \\(A\\) is called the Bose-Mesner algebra of \\(\\Gamma\\). Set \\(V = K^n\\), the set of \\(n\\)-dimensional column vectors, the coorinates are indexed by \\(X\\). Let \\(\\langle\\; , \\;\\rangle\\) denote the Hermitean inner product: \\[\\langle u, v\\rangle = u^\\top\\cdot v \\quad (u, v\\in V)\\] \\(V\\) with \\(\\langle\\; , \\;\\rangle\\) is the standard module of \\(\\Gamma\\). \\(M\\) acts on \\(V\\): For every \\(x\\in X\\), write \\[\\hat{x} = \\begin{pmatrix} 0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}\\] where \\(1\\) is at the \\(x\\) position. Then \\[A\\hat{x} = \\sum_{y\\in X, xy\\in E}\\hat{y}.\\] Since \\(A\\) is a real symmetrix matrix, \\[V = V_0 + V_1 + \\cdots + V_r \\quad \\textrm{ some } r\\in \\mathbb{Z}^{\\geq0},\\] the orthogonal direct sum of maximal \\(A\\)-eigenspaces. Let \\(E_i\\in\\mathrm{Mat}_X(K)\\) denote the orthogonal projection, \\[E_i: V \\longrightarrow V_i.\\] Then \\(E_0, \\ldots, E_r\\) are the primitive idempotents of \\(M\\). \\[M = \\mathrm{Span}_K(E_0, \\ldots, E_r),\\] \\[E_iE_j = \\delta_{ij}E_i \\quad \\textrm{for all }\\; i, j, \\quad E_0 + \\cdots + E_r = I.\\] Let \\(\\theta_i\\) denote the eigenvalue of \\(A\\) for \\(V_i\\) in \\(\\mathbb{R}\\). Without loss of generality we may assume that \\[\\theta_0 &gt; \\theta_1 &gt; \\cdots &gt; \\theta_r.\\] Let \\[m_i = \\textrm{the multiplicity of }\\: \\theta_i = \\mathrm{dim} V_i = \\mathrm{rank} E_i.\\] Set \\[\\mathrm{Spec}(\\Gamma) = \\begin{pmatrix} \\theta_0, &amp; \\theta_1, &amp; \\cdots, &amp; \\theta_r\\\\m_0, &amp; m_1, &amp; \\cdots, &amp; m_r\\end{pmatrix}.\\] Problem. What can we say about \\(\\Gamma\\) when \\(\\mathrm{Spec}(\\Gamma)\\) is given? The following Lemma1.1, is an example of Problem. For every \\(x\\in X\\), \\[k(x) \\equiv \\textrm{ valency of }x \\equiv \\textrm{ degree of }x \\equiv |\\{y\\mid y\\in X, \\: xy\\in E\\}|.\\] Definition 1.1 The graph \\(\\Gamma\\) is regular of valency \\(k\\) if \\(k = k(x)\\) for every \\(x\\in X\\). Lemma 1.1 With the above notation, 1. \\(\\theta_0\\leq \\max\\{k(x) \\mid x\\in X\\} = k^{\\max}\\). 2. If \\(\\Gamma\\) is regular of valency \\(k\\), then \\(\\theta_0 = k\\). Proof. Without loss of generality we may assume that \\(\\theta_0&gt;0\\), else done. Let \\(v:=\\sum_{x\\in X}\\alpha_x\\hat{x}\\) denote the eivenvector for \\(\\theta_0\\). Pick \\(x\\in X\\) with \\(|\\alpha_x|\\) maximal. Then \\(|\\alpha_x|\\neq 0\\). Since \\(Av = \\theta_0v\\), \\[\\theta_0\\alpha_x = \\sum_{y\\in X, xy\\in E}\\alpha_y.\\] So, \\[\\theta_0 |\\alpha_x| = |\\theta_0\\alpha_x| \\leq \\sum_{y\\in X, xy\\in E}|\\alpha_y| \\leq k(x)|\\alpha_x| \\leq k^{\\max}|\\alpha_x|.\\] 2. All 1’s vector \\(v = \\sum_{x\\in X}\\hat{x}\\) satisfies \\(Av = kv\\). Subconstituent Algebra Let \\(x, y\\in X\\) and \\(\\ell \\in \\mathbb{Z}^{\\geq 0}\\). Definition 1.2 A path of length \\(\\ell\\) connecting \\(x, y\\) is a sequence \\[x = x_0, x_1, \\ldots, x_{\\ell} = y, \\quad x_i\\in X, \\; 0\\leq i\\leq \\ell\\] such that \\(x_ix_{i+1}\\in E\\) for \\(0\\leq i \\leq \\ell-1\\). Definition 1.3 The distance \\(\\partial(x,y)\\) is the length of a shortest path connecting \\(x\\) and \\(y\\). \\[\\partial(x,y) \\in \\mathbb{Z}^{\\geq 0} \\cup \\{\\infty\\}.\\] Definition 1.4 The graph \\(\\Gamma\\) is connected if and only if \\(\\partial(x,y) &lt; \\infty\\) for all \\(x, y\\in X\\). From now on, assume that \\(\\Gamma\\) is connected with \\(|X|\\geq 2\\). Set \\[d_\\Gamma = d = \\max\\{\\partial(x,y)\\mid x, y\\in X\\} \\equiv \\textrm{the diameter of }\\Gamma.\\] Fix a ‘base’ vertex \\(x\\in X\\). Definition 1.5 \\[d(x) = \\textrm{the diameter with respect to }x = \\max\\{\\partial(x,y)\\mid y\\in X\\} \\leq d.\\] Observe that \\[V = V_0^* + V_1^* + \\cdots + V_{d(x)}^* \\quad \\textrm{(orthogonal direct sum)},\\] where \\[V_i^* = \\mathrm{Span}_K(\\hat{y}\\mid \\partial(x,y) = i) \\equiv V_i*(x)\\] and \\(V_i^* = V_i^*(x)\\) is called the \\(i\\)-the subconstituent with respect to \\(x\\). Let \\(E_i^* = E_i^*(x)\\) denote the orthogonal projection \\[E_i^*: V \\longrightarrow V_i^*(x).\\] View \\(E_i^*(x) \\in \\mathrm{Mat}_X(K)\\). So, \\(E_i^*(x)\\) is diagonal with \\(yy\\) entry \\[(E_i^*(x))_{yy} = \\begin{cases} 1 &amp; \\textrm{if } \\: \\partial(x,y) = i\\\\ 0 &amp; \\textrm{else,}\\end{cases} \\quad \\textrm{ for } y\\in X.\\] Set \\[M^* = M^*(x) \\equiv \\textrm{Span}_K(E_0^*(x), \\ldots, E_{d(x)}^*(x)).\\] Then \\(M^*(x)\\) is a commutative subalgebra of \\(\\mathrm{Mat}_X(K)\\) and is calle the dual Bose-Mesner algbara with respect to \\(x\\). Definition 1.6 (Subconstituent Algebra) Let \\(\\Gamma = (X, E)\\), \\(x\\), \\(M\\), \\(M^*(x)\\) be as above. Let \\(T = T(x)\\) denote the subalgebra of \\(\\mathrm{Mat}_X(K)\\) generated by \\(M\\) and \\(M^*(x)\\). \\(T\\) is the subconstituent algebra of \\(\\Gamma\\) with respect to \\(x\\). Definition 1.7 A \\(T\\)-module is any subspace \\(W\\subset V\\) such that \\(aw\\in W\\) for all \\(a\\in T\\) and \\(w\\in W\\). \\(T\\)-module \\(W\\) is irreducible if and only if \\(W\\neq 0\\) and \\(W\\) does not properly contain a nonzero \\(T\\)-module. For any \\(a\\in \\mathrm{Mat}_X(K)\\), let \\(a^*\\) denbote the conjugate transpose of \\(a\\). Observe that \\[\\langle au, v\\rangle = \\langle u, a^*v\\rangle \\quad \\textrm{for all }\\; a\\in \\mathrm{Mat}_X(K), \\textrm{ and for all } \\; u,v\\in V.\\] Lemma 1.2 Let \\(\\Gamma = (X,E)\\), \\(x\\in X\\) and \\(T \\equiv T(x)\\) be as above. If \\(a\\in T\\), then \\(a^*\\in T\\). For any \\(T\\)-module \\(W\\subset V\\), \\[W^\\bot := \\{v\\in V\\mid \\langle w, v\\rangle = 0, \\textrm{ for all }w\\in W\\}\\] is a \\(T\\)-module. \\(V\\) decomposes as an orthogonal direct sum of irreducible \\(T\\)-modules. Proof. It is becase \\(T\\) is generated by symmetric real matrices \\[A, E^*_0(x), E^*_1(x), \\ldots, E^*_{d(x)(x)}.\\] Pick \\(v\\in W^\\bot\\) and \\(a\\in T\\), it suffices to show that \\(av\\in W^\\bot\\). For all \\(w\\in W\\), \\[\\langle w, av\\rangle = \\langle a^*w, v\\rangle = 0\\] as \\(a^*\\in T\\). This is proved by the induction on the dimension of \\(T\\)-modules. If \\(W\\) is an irreducible \\(T\\)-module of \\(V\\), then \\[V = W + W^\\bot \\quad \\textrm{(orthogonal direct sum)}.\\] Problem. What does the structure of the \\(T(x)\\)-module tell us about \\(\\Gamma\\)? Study those \\(\\Gamma\\) whose modules take ‘simple’ form. The \\(\\Gamma\\)’s involved are highly regular. Remark. The subconstituent algebra \\(T\\) is semisimple as the left regular representation of \\(T\\) is completely reducible. See Curtis-Reiner 25.2. The inner product \\(\\langle a, b\\rangle_T = \\mathrm{tr}(a^\\top\\bar{b})\\) is nondegenerate on \\(T\\). In general, \\[\\begin{align*} T\\textrm{: Semisimple and Artinian} &amp; \\Leftrightarrow T\\textrm{: Artinian with } J(T) = 0 \\\\ &amp; \\Leftarrow T\\textrm{: Artinian with nonzero nilpotent element} \\\\ &amp; \\Leftarrow T \\subset \\mathrm{Mat}_X(K) \\textrm{ such that for all } a\\in T \\textrm{ is normal.} \\end{align*}\\] "],["lec2.html", "Chapter 2 Lecture 2", " Chapter 2 Lecture 2 Friday, January 22, 1993 In this lecture we use the Perron Frobenius theory of nonnegative matrices to obtain informaiton on eigenvalues of a graph. Let \\(K = \\mathbb{R}\\). For \\(n\\in \\mathbb{Z}\\){&gt; 0}$, pick a symmetrix matrix \\(C\\in \\mathrm{Mat}_n(\\mathbb{R})\\). Definition 2.1 The matrix \\(C\\) is reducible if and only if there is a bipartition \\(\\{1, 2, \\ldots, n\\} = X^+ \\cup X^-\\) (disjoint union of nonempty sets) such that \\(C_{ij} = 0\\) for all \\(i\\in X^+\\), and for all \\(j\\in X^-\\), and for all \\(i\\in X^-\\), and for all \\(j\\in X^+\\),i.e., \\[ C \\sim \\begin{pmatrix} \\ast &amp; O \\\\ O &amp; \\ast \\end{pmatrix}.\\] Definition 2.2 The matrix \\(C\\) is bipartite if and only if there is a bipartition \\(\\{1, 2, \\ldots, n\\} = X^+ \\cup X^-\\) (disjoint union of nonempty sets) such that \\(C_{ij} = 0\\) for all \\(i,j\\in X^+\\), and for all \\(i,j\\in X^-\\), i.e., \\[ C \\sim \\begin{pmatrix} O &amp; \\ast \\\\ \\ast &amp; O \\end{pmatrix}.\\] Note. If \\(C\\) is bipatite, for every eigenvalue \\(\\theta\\) of \\(C\\), \\(-\\theta\\) is an eigenvalue of \\(C\\) such that \\(\\mathrm{mult}(\\theta) = \\mathrm{mult}(-\\theta)\\). Indeed, let \\(C = \\begin{pmatrix} O &amp; A \\\\ B &amp; O \\end{pmatrix}\\), \\[\\begin{pmatrix} O &amp; A \\\\ B &amp; O \\end{pmatrix} \\begin{pmatrix}x\\\\y\\end{pmatrix} = \\theta \\begin{pmatrix}x\\\\y\\end{pmatrix}\\Leftrightarrow \\begin{pmatrix} O &amp; A \\\\ B &amp; O \\end{pmatrix} \\begin{pmatrix}x\\\\-y\\end{pmatrix} = -\\theta \\begin{pmatrix}x\\\\-y\\end{pmatrix}, \\] where \\(Ay = \\theta x\\) and \\(Bx = \\theta y\\). If \\(C\\) is bipartite, \\(C^2\\) is reducible. The matrix \\(C\\) is irreducible and \\(C^2\\) is reducible, if \\(C_{ij} \\geq 0\\) for all \\(i,j\\) and \\(C\\) is reducible. (Exercise) Remark. Note 1. Even if \\(C\\) is not symmetric \\[\\begin{pmatrix} O &amp; A \\\\ B &amp; O \\end{pmatrix} \\begin{pmatrix}x\\\\y\\end{pmatrix} = \\theta \\begin{pmatrix}x\\\\y\\end{pmatrix}\\Leftrightarrow \\begin{pmatrix} O &amp; A \\\\ B &amp; O \\end{pmatrix} \\begin{pmatrix}x\\\\-y\\end{pmatrix} = -\\theta \\begin{pmatrix}x\\\\-y\\end{pmatrix}\\] holds. So the geometrix multiplicities coincide. How about the algebraic multiplicities? Note 3. Set \\(x \\sim y\\) if and only if \\(C_{xy}&gt;0\\). So the graph may have loops. Then \\[(C^2)_{xy} &gt; 0 \\Leftrightarrow \\textrm{ if there exists } z\\in X \\textrm{ such that } x\\sim z \\sim y.\\] Note that \\(C\\) is irreducible if and only if \\(\\Gamma(C)\\) is connected. Let \\[\\begin{align} X^+ &amp; = \\{y\\mid \\textrm{there is a path of even length from }x \\textrm{ to }y\\}\\\\ X^- &amp; = \\{y\\mid \\textrm{there is no path of even length from }x \\textrm{ to }y\\} \\neq \\emptyset. \\end{align}\\] If there is an edge \\(y\\sim z\\) in \\(X^+\\) and \\(w\\in X^-\\). Then there would be a path from \\(x\\) to \\(y\\) of even length. So \\(\\mathrm{e}(X^+, X^+) = \\mathrm{e}(X^-, X^-) = 0.\\). Theorem 2.1 (Perron-Frobenius) Given a matrix \\(C\\) in \\(\\mathrm{Mat}_n(\\mathbb{R})\\) such that \\(C\\) is symmetric. \\(C\\) is irreducible. \\(C_{ij} \\geq 0\\) for all \\(i,j\\). Let \\(\\theta_0\\) be the maximal eigenvalue of \\(C\\) with eigenspace \\(V_0 \\subseteq \\mathbb{R}^n\\), and let \\(\\theta_r\\) be the maximal eigenvalue of \\(C\\) with eigenspace \\(V_r \\subseteq \\mathbb{R}^n\\). Then the following hold. Suppose \\(0\\neq v = \\begin{pmatrix}\\alpha_1\\\\\\alpha_2\\\\\\vdots\\\\\\alpha_n\\end{pmatrix} \\in V_0\\). Then \\(\\alpha_0 &gt; 0\\) for all \\(i\\), or \\(\\alpha_i &lt; 0\\) for all \\(i\\). \\(\\mathrm{dim}V_0 = 1\\). \\(\\theta_r \\geq -\\theta_0\\). \\(\\theta_r = \\theta_0\\) if and only if \\(C\\) is bipartite. First, we prove the following lemma. Lemma 2.1 Let \\(\\langle , \\rangle\\) be the dot product in \\(V = \\mathbb{R}^n\\). Pick a symmetric matrix \\(B \\in \\mathrm{Mat}_n(\\mathbb{R})\\). Suppose all eigenvalues of \\(B\\) are nonnegative. (i.e., \\(B\\) is positive semidefinite.) Then there exist vectors \\(v_1, v_2, \\ldots, v_n\\in V\\) such that \\(B_{ij} = \\langle v_i, v_j\\rangle\\) for \\((1\\leq i, j \\leq n)\\). :::{.proof} By elementary linear algebra, there exists an orthonormal basis \\(w_1, w_2, \\ldots, w_n\\) of \\(V\\) consisting of eigenvectors of \\(B\\). Set the \\(i\\)-th column of \\(P\\) is \\(w_i\\) and \\(D = \\mathrm{diag}(\\theta_1,\\ldots, \\theta_n)\\). Then \\(P^\\top P = I\\) and \\(BP = PD\\). Hence, \\[B = PDP^{-1} = PDP^\\top = QQ^\\top,\\] where \\[Q = P\\cdot \\mathrm{diag}(\\sqrt{\\theta_1}, \\sqrt{\\theta_2}, \\ldots, \\sqrt{\\theta_n}) \\in \\mathrm{Mat}_n(\\mathbb{R}).\\] Now, let \\(v_i\\) be the \\(i\\)-th column of \\(Q^\\top\\). Then \\[B_{ij} = v_i^\\top\\cdot v_j^- = \\langle v_i, v_j\\rangle.\\] Now we start the proof of Theorem 2.1. Proof of Theorem 2.1 Let \\(\\langle , \\rangle\\) denote the dot product on \\(V = \\mathbb{R}^n\\). Set \\[\\begin{align} B &amp; = \\theta I - C\\\\ &amp; = \\textrm{symmetric matrix with eigenvalues }\\theta_0 - \\theta_i \\geq 0\\\\ &amp; = (\\langle v_i, v_j\\rangle)_{1\\leq i,j\\leq n} \\end{align}\\] with the same \\(v_1, \\ldots, v_n \\in V\\) by Lemma 2.1. Observe: \\(\\sum_{i = 1}^n \\alpha_iv_i = 0\\). Pf. \\[\\begin{align} \\|\\sum_{i = 1}^n \\alpha_iv_i\\|^2 &amp; = \\langle \\sum_{i=1}^n\\alpha_iv_i, \\sum_{i=1}^n\\alpha_iv_i\\rangle\\\\ &amp; = \\begin{pmatrix} \\alpha_1 &amp;\\ldots &amp;\\alpha_n\\end{pmatrix}B\\begin{pmatrix} \\alpha_1\\\\\\vdots\\\\\\alpha_n\\end{pmatrix}\\\\ &amp; = v^\\top Bv\\\\ &amp; = 0, \\end{align}\\] since \\(Bv = (\\theta_0 I - C)v = 0\\). Now set \\[s = \\textrm{the number of indices} i, \\textrm{ where } \\alpha_i &gt;0.\\] Replacing \\(v\\) by \\(-v\\) if necessary, without loss of generality we may assume that \\(s\\geq 1\\). We want to show \\(s = n\\). Assume \\(s &lt; n\\). Without loss of generality, we may assume that \\(\\alpha_i &gt;0\\) for \\(1\\leq s\\leq s\\) and \\(\\alpha_i = 0\\) for \\(s+1 \\leq i \\leq n\\). Set \\[ \\rho = \\alpha_1v_1 + \\cdots + \\alpha_sv_s = -\\alpha_{s+1}v_{s+1} - \\cdots - \\alpha_nv_n.\\] Then, for \\(i = 1,\\ldots, s\\), \\[\\begin{align} \\langle v_i, \\rho \\rangle &amp; = \\sum_{j = s+1}^n -\\alpha_j\\langle v_i, v_j\\rangle \\quad (\\langle v_i, v_j\\rangle = B_{ij}, B = \\theta_0I - C)\\\\ &amp; = \\sum_{j = s+1}^n (-\\alpha_{ij})(-C_{ij})\\\\ &amp; \\leq 0. \\end{align}\\] Hence \\[0\\leq \\langle \\rho, \\rho\\rangle = \\sum_{i=1}^s \\alpha_i \\langle v_i, \\rho\\rangle \\leq 0,\\] as \\(\\alpha &gt; 0\\) and \\(\\langle v_i, \\rho\\rangle \\leq 0\\). Thus, we have \\(\\langle, \\rho, \\rho \\rangle = 0\\) and \\(\\rho = 0\\). For \\(j = s+1, \\ldots, n\\), \\[0 = \\langle \\rho, v_j\\rangle = \\sum_{i=1}^s \\alpha_i\\langle v_i, v_j\\rangle \\leq 0,\\] as \\(\\langle v_i, v_j\\rangle = -C_{ij}\\). Therefore, \\[0 = \\langle v_i, v_j \\rangle = -C_{ij} \\textrm{ for } 1\\leq i, \\leq s, \\: s+1 \\leq j \\leq n.\\] Since \\(C\\) is symmetric, \\[C = \\begin{pmatrix} \\ast &amp; O \\\\ O &amp; \\ast\\end{pmatrix}\\] Thus \\(C\\) is reducible, which is not the case. Hence \\(s = n\\). Proof of Theorem 2.1 2. Suppose \\(\\dim V_0 \\geq 2\\). Then, \\[\\dim\\left(V_0 \\cap \\begin{pmatrix}1\\\\0\\\\\\vdots\\\\0\\end{pmatrix}^\\bot\\right) \\geq 1.\\] So, there is a vector \\[0\\neq v = \\begin{pmatrix}\\alpha_1\\\\\\vdots\\\\\\alpha_n\\end{pmatrix} \\in V_0\\] with \\(\\alpha_1 = 0\\). This contradicts 1. Now pick \\[0\\neq w = \\begin{pmatrix}\\beta_1\\\\\\vdots\\\\\\beta_n\\end{pmatrix} \\in V_r.\\] Proof of Theorem 2.1 3. Suppose \\(\\theta_r &lt; -\\theta_0\\). Since the eigenvalues of \\(C^2\\) are the squares of those of \\(C\\), \\(\\theta_r^2\\) is the maximal eigenvalue of \\(C^2\\). Also we have \\(C^2w = \\theta_r^2w\\). Observe that \\(C^2\\) is irreducible. (As otherwise, \\(C\\) is bipartite by Note 3, and we must have \\(\\theta_r = -\\theta_0\\).) Therefore, \\(\\beta_i &gt; 0\\) for all \\(i\\) or \\(\\beta_i &lt; 0\\) for all \\(i\\). We have \\[\\langle v, w\\rangle = \\sum_{i=1}^n\\alpha_i\\beta_j \\neq 0.\\] This is a contradiction, as \\(V_0 \\bot V_r\\). Proof of Theorem 2.1 4. \\(\\Rightarrow\\): Let \\(\\theta_r = -\\theta_0\\). Then \\(\\theta = \\theta_1^2 = \\theta_0^2\\) is the maximal eigenvalue of \\(C^2\\), and \\(v\\) and \\(w\\) are linearly independent eigenvalues for \\(\\theta\\) for \\(C^2\\). Hence, for \\(C^2\\), \\(\\mathrm{mult}(\\theta) \\geq 2\\). Thus by 2, \\(C^2\\) must be reducible. Therefore, \\(C\\) is bipartite by Note 3. \\(\\Leftarrow\\): This is Note 1. \\(\\Box\\) Let \\(\\Gamma = (X, E)\\) be any graph. Definition 2.3 \\(\\Gamma\\) is said to be bipartite if the adjacency matrix \\(A\\) is bipartite. That is, \\(X\\) can be written as a disjoint union of \\(X^+\\) and \\(X^-\\) such that \\(X^+, X^-\\) contain no edges of \\(\\Gamma\\). Corollary 2.1 For any (connected) graph \\(\\Gamma\\) with \\[\\mathrm{Spec}(\\Gamma) = \\begin{pmatrix}\\theta_0 &amp; \\theta_1 &amp;\\cdots &amp; \\theta_r\\\\m_1 &amp; m_1 &amp; \\cdots &amp; m_r\\end{pmatrix} \\:\\textrm{ with }\\: \\theta_0 &gt; \\theta_1 &gt; \\cdots &gt; \\theta_r.\\] Let \\(V_i\\) be the eigenspace of \\(\\theta_i\\). Then the following holds. Supppose \\(0\\neq v = \\begin{pmatrix} \\alpha_1\\\\\\vdots \\\\\\alpha_n \\end{pmatrix} \\in V_0\\in \\mathbb{R}^n\\). Then \\(\\alpha_i &gt; 0\\) for all \\(i\\) or \\(\\alpha_i &lt; 0\\) for all \\(i\\). \\(m_0 = 1\\). \\(\\theta_r \\geq -\\theta_0\\) if and only if \\(\\Gamma\\) is bipartite. In this case, \\[-\\theta_i = \\theta_{r-i} \\; \\textrm{and} \\; m_i = m_{r-i} \\quad (0\\leq i\\leq r)\\] Proof. This is a direct consequences of Theorem 2.1 and Note 3. "],["lec3.html", "Chapter 3 Lecture 3", " Chapter 3 Lecture 3 Monday, January 25, 1993 Given graphs \\(\\Gamma = (X, E)\\) and \\(\\Gamma&#39; = (X&#39;, E&#39;)\\). Example 3.1 \\(\\Delta = \\{a, a^{-1}\\}\\) Example 3.2 \\(\\Delta = \\{a, a^{-1}, a^2, a^{-2}\\}\\) Example 3.3 \\(G = \\langle a, b \\mid a^6 = 1, b^2, ab = ba\\rangle\\), \\(\\Delta = \\{a, a^{-1}, b\\}\\) "],["applications.html", "Chapter 4 Applications 4.1 Example one 4.2 Example two", " Chapter 4 Applications Some significant applications are demonstrated in this chapter. 4.1 Example one 4.2 Example two "],["final-words.html", "Chapter 5 Final Words", " Chapter 5 Final Words We have finished a nice book. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
