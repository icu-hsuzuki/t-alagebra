% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\title{Lecture Note on Terwilliger Algebra}
\author{P. Terwilliger, edited by H. Suzuki}
\date{2022-12-06}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Lecture Note on Terwilliger Algebra},
  pdfauthor={P. Terwilliger, edited by H. Suzuki},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm, amsmath, amssymb, amsbsy, amsopn, amsxtra, amscd, amstext}
\usepackage{makeidx}
\makeindex
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}


\newcommand{\bN}{\mathbb{N}}
\newcommand{\bZ}{\mathbb{Z}}
\newcommand{\bR}{\mathbb{R}}
\newcommand{\bC}{\mathbb{C}}
\newcommand{\bQ}{\mathbb{Q}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\as}{\cX = (X,\{R_{i}\}_{0\leq i\leq d})}
\newcommand{\tas}{\cX = (X,\{R_{i,j}\}_{(i,j)\in I})}
\newcommand{\gas}{\cG = (G,\{R_i\}_{0\leq i\leq d})}
\newcommand{\edge}{\mbox{{\rm edge}}}
\newcommand{\lp}{\mbox{{\rm loop}}}
\newcommand{\nmat}{\textrm{Mat}_n(\bC)}
\newcommand{\xmat}{\textrm{Mat}_X(\bC)}
\newcommand{\xrmat}{\textrm{Mat}_X(\bR)}
\newcommand{\mat}{\textrm{Mat}}
\newcommand{\tr}{\textrm{tr}}
\newcommand{\rank}{\textrm{rank}}
\newcommand{\spn}{\textrm{Span}}
\newcommand{\spec}{\textrm{Spec}}


\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{definition}
\newtheorem{hypothesis}{Hypothesis}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{about-this-lecturenote}{%
\chapter*{About this lecturenote}\label{about-this-lecturenote}}
\addcontentsline{toc}{chapter}{About this lecturenote}

\hypertarget{setting}{%
\section*{Setting}\label{setting}}
\addcontentsline{toc}{section}{Setting}

sudo
This note is created by \texttt{bookdown} package on RStudio.

For \texttt{bookdown} See \citep{xie2015}, \citep{xie2017}, \citep{xie2018}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Log-in to my GitHub Account
\item
  Go to RStudio/bookdown-demo repository: \url{https://github.com/rstudio/bookdown-demo}
\item
  Use This Template
\item
  Input Repository Name
\item
  Select Public - default
\item
  Create repository from template
\item
  From Code download ZIP
\item
  Move the extracted folder into a favorite directory
\item
  Open RStudio Project in the folder
\item
  Use Terminal in the buttom left pane

  \begin{itemize}
  \tightlist
  \item
    confirm that the current directory is the home directry of the project by pwd
  \end{itemize}
\item
  (failed to proceed by ssh)
\item
  Use Console

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    library(usethis)
  \item
    use\_git()
  \item
    use\_github() --- Error
  \item
    gh\_token\_help()
  \item
    create\_github\_token(): create a token in the github page. Copy the token
  \item
    gitcreds::gitcreds\_set(): paste the token, the token is to be expired in 30 days
  \end{enumerate}
\item
  Use Terminal

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    git remote add origin \url{https://github.com/icu-hsuzuki/t-alagebra.git}
  \item
    git push -u origin main
  \item
    type in the password of the computer
  \end{enumerate}
\item
  Use GIT in R Studio
\end{enumerate}

\hypertarget{another-host}{%
\section*{Another Host}\label{another-host}}
\addcontentsline{toc}{section}{Another Host}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  create a project by version control git
\item
  git init
\item
  git remote add origin \href{mailto:git@github.com}{\nolinkurl{git@github.com}}:/.git
\item
  git branch -r
\item
  git fetch
\item
  git pull origin main
\end{enumerate}

\hypertarget{lec1}{%
\chapter{Subconstituent Algebra of a Graph}\label{lec1}}

\textbf{Wednesday, January 20, 1993}

A graph \index{graph} (undirected, without loops or multiple edges) is a pair \(\Gamma = (X, E)\), where
\begin{align}
X &= \textrm{finite set (of vertices)}\\
E & = \textrm{set of (distinct) 2-element subsets of }X \textrm{ (= edges of ) }\Gamma.
\end{align}
vertices \(x\) and \(y\in X\) are adjacent if and only if \(xy\in E\).

\begin{example}

Let \(\Gamma\) be a graph. \(X = \{a, b, c, d\}\), \(E = \{ab, ac, bc, bd\}\).

\begin{center}\includegraphics{t-algebra_files/figure-latex/unnamed-chunk-1-1} \end{center}

\end{example}

Set \(n = |X|\), the order of \(\Gamma\).

Pick a field \(K\) (\(=\mathbb{R}\) or \(\mathbb{C}\)). Then \(\mathrm{Mat}_X(K)\) denotes the \(K\) algebra of all \(n\times n\) matrices with entries in \(K\). (rows and columns are indexed by \(X\))

\emph{Adjacency matrix} \(A\in \mathrm{Mat}_X(K)\) is defined by
\begin{align}
A_{xy} & = \left\{\begin{array}{cl} 1 & \textrm{ if } \; xy\in E\\
0 & \textrm{ else } . \end{array}\right.
\end{align}

\begin{example}
Let \(a, b, c, d\) be labels of rows and columns. Then
\[A = \begin{matrix} \\ a\\ b\\c\\d\end{matrix}\begin{matrix}\begin{matrix} a & b & c & d \end{matrix}\\\begin{pmatrix} 0 & 1 & 1 & 0 \\ 1 & 0 & 1 & 1 \\
1 & 1 & 0 & 0 \\ 0 & 1 & 0 & 0 \end{pmatrix}\end{matrix}\]
\end{example}

The subalgebra \(M\) of \(\mathrm{Mat}_X(K)\) generated by \(A\) is called the \emph{Bose-Mesner algebra} of \(\Gamma\).

Set \(V = K^n\), the set of \(n\)-dimensional column vectors, the coorinates are indexed by \(X\).

Let \(\langle\; , \;\rangle\) denote the Hermitean inner product:
\[\langle u, v\rangle = u^\top\cdot v \quad (u, v\in V)\]
\(V\) with \(\langle\; , \;\rangle\) is the \emph{standard module} of \(\Gamma\).

\(M\) acts on \(V\): For every \(x\in X\), write
\[\hat{x} = \begin{pmatrix} 0 \\ \vdots \\ 0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}\]
where \(1\) is at the \(x\) position.

Then
\[A\hat{x} = \sum_{y\in X, xy\in E}\hat{y}.\]
Since \(A\) is a real symmetrix matrix,
\[V = V_0 + V_1 + \cdots + V_r \quad \textrm{ some } r\in \mathbb{Z}^{\geq0},\]
the orthogonal direct sum of maximal \(A\)-eigenspaces.

Let \(E_i\in\mathrm{Mat}_X(K)\) denote the orthogonal projection,
\[E_i: V \longrightarrow V_i.\]
Then \(E_0, \ldots, E_r\) are the primitive idempotents of \(M\).
\[M = \mathrm{Span}_K(E_0, \ldots, E_r),\]
\[E_iE_j = \delta_{ij}E_i \quad \textrm{for all }\; i, j, \quad E_0 + \cdots + E_r = I.\]
Let \(\theta_i\) denote the eigenvalue of \(A\) for \(V_i\) in \(\mathbb{R}\). Without loss of generality we may assume that
\[\theta_0 > \theta_1 > \cdots > \theta_r.\]
Let
\[m_i = \textrm{the multiplicity of }\: \theta_i = \mathrm{dim} V_i = \mathrm{rank} E_i.\]
Set
\[\mathrm{Spec}(\Gamma) = \begin{pmatrix} \theta_0, & \theta_1, & \cdots, & \theta_r\\m_0, & m_1, & \cdots, & m_r\end{pmatrix}.\]
\textbf{Problem. }
What can we say about \(\Gamma\) when \(\mathrm{Spec}(\Gamma)\) is given?

The following Lemma\ref{lem:largestev}, is an example of Problem.

For every \(x\in X\),
\[k(x) \equiv \textrm{ valency of }x \equiv \textrm{ degree of }x \equiv |\{y\mid y\in X, \: xy\in E\}|.\]

\begin{definition}
\protect\hypertarget{def:regular}{}\label{def:regular}The graph \(\Gamma\) is regular\index{regular} of valency\index{valency} \(k\) if \(k = k(x)\) for every \(x\in X\).
\end{definition}

\begin{lemma}
\protect\hypertarget{lem:largestev}{}\label{lem:largestev}

With the above notation,

\((i)\) \(\theta_0\leq \max\{k(x) \mid x\in X\} = k^{\max}\).\\
\((ii)\) If \(\Gamma\) is regular of valency \(k\), then \(\theta_0 = k\).

\end{lemma}

\begin{proof}
\leavevmode

\((i)\) Without loss of generality we may assume that \(\theta_0>0\), else done. Let \(v:=\sum_{x\in X}\alpha_x\hat{x}\) denote the eivenvector for \(\theta_0\).

Pick \(x\in X\) with \(|\alpha_x|\) maximal. Then \(|\alpha_x|\neq 0\).

Since \(Av = \theta_0v\),
\[\theta_0\alpha_x = \sum_{y\in X, xy\in E}\alpha_y.\]
So,
\[\theta_0 |\alpha_x| = |\theta_0\alpha_x| \leq \sum_{y\in X, xy\in E}|\alpha_y| \leq k(x)|\alpha_x| \leq k^{\max}|\alpha_x|.\]

\((ii)\) All 1's vector \(v = \sum_{x\in X}\hat{x}\) satisfies \(Av = kv\).

\end{proof}

\textbf{Subconstituent Algebra}

Let \(x, y\in X\) and \(\ell \in \mathbb{Z}^{\geq 0}\).

\begin{definition}
A path\index{path} of length \(\ell\) connecting \(x, y\) is a sequence
\[x = x_0, x_1, \ldots, x_{\ell} = y, \quad x_i\in X, \; 0\leq i\leq \ell\]
such that \(x_ix_{i+1}\in E\) for \(0\leq i \leq \ell-1\).
\end{definition}

\begin{definition}
The distance\index{distance} \(\partial(x,y)\) is the length of a shortest path connecting \(x\) and \(y\).
\[\partial(x,y) \in \mathbb{Z}^{\geq 0} \cup \{\infty\}.\]
\end{definition}

\begin{definition}
The graph \(\Gamma\) is connected\index{connected} if and only if \(\partial(x,y) < \infty\) for all \(x, y\in X\).
\end{definition}

From now on, assume that \(\Gamma\) is connected with \(|X|\geq 2\).

Set
\[d_\Gamma = d = \max\{\partial(x,y)\mid x, y\in X\} \equiv \textrm{the diameter of }\Gamma.\]
Fix a `base' vertex \(x\in X\).

\begin{definition}
\index{distance}
\[d(x) = \textrm{the diameter with respect to }x = \max\{\partial(x,y)\mid y\in X\} \leq d.\]
\end{definition}

Observe that
\[V = V_0^* + V_1^* + \cdots + V_{d(x)}^* \quad \textrm{(orthogonal direct sum)},\]
where
\[V_i^* = \mathrm{Span}_K(\hat{y}\mid \partial(x,y) = i) \equiv V_i*(x)\]
and \(V_i^* = V_i^*(x)\) is called the \(i\)-the subconstituent with respect to \(x\).

Let \(E_i^* = E_i^*(x)\) denote the orthogonal projection
\[E_i^*: V \longrightarrow V_i^*(x).\]
View \(E_i^*(x) \in \mathrm{Mat}_X(K)\). So, \(E_i^*(x)\) is diagonal with \(yy\) entry
\[(E_i^*(x))_{yy} = \begin{cases} 1 & \textrm{if } \: \partial(x,y) = i\\ 0 & \textrm{else,}\end{cases} \quad \textrm{ for } y\in X.\]
Set
\[M^* = M^*(x) \equiv \textrm{Span}_K(E_0^*(x), \ldots, E_{d(x)}^*(x)).\]
Then \(M^*(x)\) is a commutative subalgebra of \(\mathrm{Mat}_X(K)\) and is calle the \emph{dual Bose-Mesner algbara with respect to \(x\)}.

\begin{definition}[Subconstituent Algebra]
Let \(\Gamma = (X, E)\), \(x\), \(M\), \(M^*(x)\) be as above. Let \(T = T(x)\) denote the subalgebra of \(\mathrm{Mat}_X(K)\) generated by \(M\) and \(M^*(x)\). \(T\) is the \emph{subconstituent algebra}\index{subconstituent algebra} of \(\Gamma\) with respect to \(x\).
\end{definition}

\begin{definition}
A \(T\)-module \index{module} is any subspace \(W\subset V\) such that \(aw\in W\) for all \(a\in T\) and \(w\in W\).

\(T\)-module \(W\) is \emph{irreducible} if and only if \(W\neq 0\) and \(W\) does not properly contain a nonzero \(T\)-module.
\end{definition}

For any \(a\in \mathrm{Mat}_X(K)\), let \(a^*\) denbote the conjugate transpose of \(a\).

Observe that
\[\langle au, v\rangle = \langle u, a^*v\rangle \quad \textrm{for all }\; a\in \mathrm{Mat}_X(K), \textrm{ and for all } \; u,v\in V.\]

\begin{lemma}
\protect\hypertarget{lem:complete-reducibility}{}\label{lem:complete-reducibility}

Let \(\Gamma = (X,E)\), \(x\in X\) and \(T \equiv T(x)\) be as above.

\((i)\) If \(a\in T\), then \(a^*\in T\).

\((ii)\) For any \(T\)-module \(W\subset V\),

\[W^\bot := \{v\in V\mid \langle w, v\rangle = 0, \textrm{ for all }w\in W\}\]
is a \(T\)-module.

\((iii)\) \(V\) decomposes as an orthogonal direct sum of irreducible \(T\)-modules.

\end{lemma}

\begin{proof}
\leavevmode

\((i)\) It is becase \(T\) is generated by symmetric real matrices

\[A, E^*_0(x), E^*_1(x), \ldots, E^*_{d(x)(x)}.\]

\((ii)\) Pick \(v\in W^\bot\) and \(a\in T\), it suffices to show that \(av\in W^\bot\). For all \(w\in W\),

\[\langle w, av\rangle = \langle a^*w, v\rangle = 0\]
as \(a^*\in T\).

\((iii)\) This is proved by the induction on the dimension of \(T\)-modules. If \(W\) is an irreducible \(T\)-module of \(V\), then

\[V = W + W^\bot \quad \textrm{(orthogonal direct sum)}.\]

\end{proof}

\textbf{Problem. }
What does the structure of the \(T(x)\)-module tell us about \(\Gamma\)?

Study those \(\Gamma\) whose modules take `simple' form. The \(\Gamma\)'s involved are highly regular.

\begin{remark}
\leavevmode

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The subconstituent algebra \(T\) is semisimple as the left regular representation of \(T\) is completely reducible. See Curtis-Reiner 25.2 \citep{cr}.
\item
  The inner product \(\langle a, b\rangle_T = \mathrm{tr}(a^\top\bar{b})\) is nondegenerate on \(T\).
\item
  In general,
  \begin{align*}
  T\textrm{: Semisimple and Artinian} & \Leftrightarrow T\textrm{: Artinian with } J(T) = 0 \\
  & \Leftarrow T\textrm{: Artinian with nonzero nilpotent element} \\
  & \Leftarrow T \subset \mathrm{Mat}_X(K) \textrm{ such that for all } a\in T \textrm{ is normal.}
  \end{align*}
\end{enumerate}

\end{remark}

\hypertarget{lec2}{%
\chapter{Perron-Frobenius Theorem}\label{lec2}}

\textbf{Friday, January 22, 1993}

In this lecture we use the Perron Frobenius theory of nonnegative matrices to obtain informaiton on eigenvalues of a graph.

Let \(K = \mathbb{R}\). For \(n\in \mathbb{Z}^{> 0}\), pick a symmetrix matrix \(C\in \mathrm{Mat}_n(\mathbb{R})\).

\begin{definition}
The matrix \(C\) is \emph{reducible}\index{reducible} if and only if there is a bipartition \(\{1, 2, \ldots, n\} = X^+ \cup X^-\) (disjoint union of nonempty sets) such that \(C_{ij} = 0\) for all \(i\in X^+\), and for all \(j\in X^-\), and for all \(i\in X^-\), and for all \(j\in X^+\),i.e.,
\[ C \sim \begin{pmatrix} \ast & O \\ O & \ast \end{pmatrix}.\]
\end{definition}

\begin{definition}
The matrix \(C\) is \emph{bipartite}\index{bipartite} if and only if there is a bipartition \(\{1, 2, \ldots, n\} = X^+ \cup X^-\) (disjoint union of nonempty sets) such that \(C_{ij} = 0\) for all \(i,j\in X^+\), and for all \(i,j\in X^-\), i.e.,
\[ C \sim \begin{pmatrix} O & \ast \\ \ast  & O \end{pmatrix}.\]
\end{definition}

\textbf{Note.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If \(C\) is bipatite, for every eigenvalue \(\theta\) of \(C\), \(-\theta\) is an eigenvalue of \(C\) such that \(\mathrm{mult}(\theta) = \mathrm{mult}(-\theta)\).
\end{enumerate}

Indeed, let \(C = \begin{pmatrix} O & A \\ B & O \end{pmatrix}\),
\[\begin{pmatrix} O & A \\ B & O \end{pmatrix} \begin{pmatrix}x\\y\end{pmatrix} = \theta \begin{pmatrix}x\\y\end{pmatrix}\Leftrightarrow \begin{pmatrix} O & A \\ B & O \end{pmatrix} \begin{pmatrix}x\\-y\end{pmatrix} = -\theta \begin{pmatrix}x\\-y\end{pmatrix}, \]
where \(Ay = \theta x\) and \(Bx = \theta y\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  If \(C\) is bipartite, \(C^2\) is reducible.
\item
  The matrix \(C\) is irreducible and \(C^2\) is reducible, if \(C_{ij} \geq 0\) for all \(i,j\) and \(C\) is reducible. (Exercise)
\end{enumerate}

\begin{remark}
Note 1. Even if \(C\) is not symmetric
\[\begin{pmatrix} O & A \\ B & O \end{pmatrix} \begin{pmatrix}x\\y\end{pmatrix} = \theta \begin{pmatrix}x\\y\end{pmatrix}\Leftrightarrow \begin{pmatrix} O & A \\ B & O \end{pmatrix} \begin{pmatrix}x\\-y\end{pmatrix} = -\theta \begin{pmatrix}x\\-y\end{pmatrix}\]
holds. So the geometrix multiplicities coincide. How about the algebraic multiplicities?

Note 3. Set \(x \sim y\) if and only if \(C_{xy}>0\). So the graph may have loops. Then
\[(C^2)_{xy} > 0 \Leftrightarrow \textrm{ if there exists } z\in X \textrm{ such that } x\sim z \sim y.\]
Note that \(C\) is irreducible if and only if \(\Gamma(C)\) is connected. Let
\begin{align}
X^+ & = \{y\mid \textrm{there is a path of even length from }x \textrm{ to }y\}\\
X^- & = \{y\mid \textrm{there is no path of even length from }x \textrm{ to }y\} \neq \emptyset.
\end{align}
If there is an edge \(y\sim z\) in \(X^+\) and \(w\in X^-\). Then there would be a path from \(x\) to \(y\) of even length.
So \(\mathrm{e}(X^+, X^+) = \mathrm{e}(X^-, X^-) = 0.\).
\end{remark}

\begin{theorem}[Perron-Frobenius]
\protect\hypertarget{thm:pf}{}\label{thm:pf}

Given a matrix \(C\) in \(\mathrm{Mat}_n(\mathbb{R})\) such that

\((a)\) \(C\) is symmetric.

\((b)\) \(C\) is irreducible.

\((c)\) \(C_{ij} \geq 0\) for all \(i,j\).

Let \(\theta_0\) be the maximal eigenvalue of \(C\) with eigenspace \(V_0 \subseteq \mathbb{R}^n\), and let \(\theta_r\) be the maximal eigenvalue of \(C\) with eigenspace \(V_r \subseteq \mathbb{R}^n\). Then the following hold.

\((i)\) Suppose \(0\neq v = \begin{pmatrix}\alpha_1\\\alpha_2\\\vdots\\\alpha_n\end{pmatrix} \in V_0\). Then \(\alpha_0 > 0\) for all \(i\), or \(\alpha_i < 0\) for all \(i\).

\((ii)\) \(\mathrm{dim}V_0 = 1\).

\((iii)\) \(\theta_r \geq -\theta_0\).

\((iv)\) \(\theta_r = \theta_0\) if and only if \(C\) is bipartite.

\end{theorem}

First, we prove the following lemma.

\begin{lemma}
\protect\hypertarget{lem:pfl}{}\label{lem:pfl}Let \(\langle \;,\; \rangle\) be the dot product in \(V = \mathbb{R}^n\). Pick a symmetric matrix \(B \in \mathrm{Mat}_n(\mathbb{R})\). Suppose all eigenvalues of \(B\) are nonnegative. (i.e., \(B\) is positive semidefinite.) Then there exist vectors \(v_1, v_2, \ldots, v_n\in V\) such that \(B_{ij} = \langle v_i, v_j\rangle\) for \((1\leq i, j \leq n)\).
\end{lemma}

\begin{proof}
By elementary linear algebra, there exists an orthonormal basis \(w_1, w_2, \ldots, w_n\) of \(V\) consisting of eigenvectors of \(B\). Set the \(i\)-th column of \(P\) is \(w_i\) and \(D = \mathrm{diag}(\theta_1,\ldots, \theta_n)\). Then \(P^\top P = I\) and \(BP = PD\).

Hence,
\[B = PDP^{-1} = PDP^\top = QQ^\top,\]
where
\[Q = P\cdot \mathrm{diag}(\sqrt{\theta_1}, \sqrt{\theta_2}, \ldots, \sqrt{\theta_n}) \in \mathrm{Mat}_n(\mathbb{R}).\]
Now, let \(v_i\) be the \(i\)-th column of \(Q^\top\). Then
\[B_{ij} = v_i^\top\cdot v_j^- = \langle v_i, v_j\rangle.\]
\end{proof}

Now we start the proof of Theorem \ref{thm:pf}.

\emph{Proof of Theorem \ref{thm:pf}}\((i)\)

Let \(\langle , \rangle\) denote the dot product on \(V = \mathbb{R}^n\). Set
\begin{align}
B & = \theta I - C\\
  & = \textrm{symmetric matrix with eigenvalues }\theta_0 - \theta_i \geq 0\\
  & = (\langle v_i, v_j\rangle)_{1\leq i,j\leq n}
\end{align}
with the same \(v_1, \ldots, v_n \in V\) by Lemma \ref{lem:pfl}.

Observe: \(\sum_{i = 1}^n \alpha_iv_i = 0\).

\emph{Pf.}
\begin{align}
\|\sum_{i = 1}^n \alpha_iv_i\|^2 & = \langle \sum_{i=1}^n\alpha_iv_i, \sum_{i=1}^n\alpha_iv_i\rangle\\
& = \begin{pmatrix} \alpha_1 &\ldots &\alpha_n\end{pmatrix}B\begin{pmatrix} \alpha_1\\\vdots\\\alpha_n\end{pmatrix}\\
& = v^\top Bv\\
& = 0,
\end{align}
since \(Bv = (\theta_0 I - C)v = 0\).

Now set
\[s = \textrm{the number of indices} i, \textrm{ where } \alpha_i >0.\]
Replacing \(v\) by \(-v\) if necessary, without loss of generality we may assume that \(s\geq 1\). We want to show \(s = n\).

Assume \(s < n\). Without loss of generality, we may assume that \(\alpha_i >0\) for \(1\leq s\leq s\) and \(\alpha_i = 0\) for \(s+1 \leq i \leq n\).
Set
\[ \rho = \alpha_1v_1 + \cdots + \alpha_sv_s = -\alpha_{s+1}v_{s+1} - \cdots - \alpha_nv_n.\]
Then, for \(i = 1,\ldots, s\),
\begin{align}
\langle v_i, \rho \rangle & = \sum_{j = s+1}^n -\alpha_j\langle v_i, v_j\rangle \quad (\langle v_i, v_j\rangle = B_{ij}, B = \theta_0I - C)\\
  & = \sum_{j = s+1}^n (-\alpha_{ij})(-C_{ij})\\
  & \leq 0.
\end{align}
Hence
\[0\leq \langle \rho, \rho\rangle = \sum_{i=1}^s \alpha_i \langle v_i, \rho\rangle \leq 0,\]
as \(\alpha > 0\) and \(\langle v_i, \rho\rangle \leq 0\). Thus, we have
\(\langle, \rho, \rho \rangle = 0\) and \(\rho = 0\).
For \(j = s+1, \ldots, n\),
\[0 = \langle \rho, v_j\rangle = \sum_{i=1}^s \alpha_i\langle v_i, v_j\rangle \leq 0,\]
as \(\langle v_i, v_j\rangle = -C_{ij}\).

Therefore,
\[0 = \langle v_i, v_j \rangle = -C_{ij} \textrm{ for } 1\leq i, \leq s, \: s+1 \leq j \leq n.\]
Since \(C\) is symmetric,
\[C = \begin{pmatrix} \ast & O \\ O & \ast\end{pmatrix}\]
Thus \(C\) is reducible, which is not the case. Hence \(s = n\).

\emph{Proof of Theorem \ref{thm:pf} \((ii)\).}

Suppose \(\dim V_0 \geq 2\). Then,
\[\dim\left(V_0 \cap \begin{pmatrix}1\\0\\\vdots\\0\end{pmatrix}^\bot\right) \geq 1.\]
So, there is a vector
\[0\neq v = \begin{pmatrix}\alpha_1\\\vdots\\\alpha_n\end{pmatrix} \in V_0\]
with \(\alpha_1 = 0\). This contradicts 1.

Now pick
\[0\neq w = \begin{pmatrix}\beta_1\\\vdots\\\beta_n\end{pmatrix} \in V_r.\]

\emph{Proof of Theorem \ref{thm:pf} \((iii)\).}

Suppose \(\theta_r < -\theta_0\). Since the eigenvalues of \(C^2\) are the squares of those of \(C\), \(\theta_r^2\) is the maximal eigenvalue of \(C^2\).

Also we have \(C^2w = \theta_r^2w\).

Observe that \(C^2\) is irreducible. (As otherwise, \(C\) is bipartite by Note 3, and we must have \(\theta_r = -\theta_0\).)
Therefore, \(\beta_i > 0\) for all \(i\) or \(\beta_i < 0\) for all \(i\). We have
\[\langle v, w\rangle = \sum_{i=1}^n\alpha_i\beta_j \neq 0.\]
This is a contradiction, as \(V_0 \bot V_r\).

\emph{Proof of Theorem \ref{thm:pf} \((iv)\)}

\(\Rightarrow\): Let \(\theta_r = -\theta_0\). Then \(\theta = \theta_1^2 = \theta_0^2\) is the maximal eigenvalue of \(C^2\), and \(v\) and \(w\) are linearly independent eigenvalues for \(\theta\) for \(C^2\). Hence, for \(C^2\), \(\mathrm{mult}(\theta) \geq 2\).

Thus by 2, \(C^2\) must be reducible. Therefore, \(C\) is bipartite by Note 3.

\(\Leftarrow\): This is Note 1.
\(\Box\)

Let \(\Gamma = (X, E)\) be any graph.

\begin{definition}
\(\Gamma\) is said to be \emph{bipartite}\index{bipartite} if the adjacency matrix \(A\) is bipartite. That is, \(X\) can be written as a disjoint union of \(X^+\) and \(X^-\) such that \(X^+, X^-\) contain no edges of \(\Gamma\).
\end{definition}

\begin{corollary}
\protect\hypertarget{cor:spec}{}\label{cor:spec}

For any (connected) graph \(\Gamma\) with
\[\mathrm{Spec}(\Gamma) = \begin{pmatrix}\theta_0 & \theta_1 &\cdots & \theta_r\\m_1 & m_1 & \cdots & m_r\end{pmatrix} \:\textrm{ with }\: \theta_0 > \theta_1 > \cdots > \theta_r.\]
Let \(V_i\) be the eigenspace of \(\theta_i\). Then the following holds.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Supppose \(0\neq v = \begin{pmatrix} \alpha_1\\\vdots \\\alpha_n \end{pmatrix} \in V_0\in \mathbb{R}^n\). Then \(\alpha_i > 0\) for all \(i\) or \(\alpha_i < 0\) for all \(i\).
\item
  \(m_0 = 1\).
\item
  \(\theta_r \geq -\theta_0\) if and only if \(\Gamma\) is bipartite. In this case,
  \[-\theta_i = \theta_{r-i} \; \textrm{and} \; m_i = m_{r-i} \quad (0\leq i\leq r)\]
\end{enumerate}

\end{corollary}

\begin{proof}
This is a direct consequences of Theorem \ref{thm:pf} and Note 3.
\end{proof}

\hypertarget{lec3}{%
\chapter{Cayley Graphs}\label{lec3}}

\textbf{Monday, January 25, 1993}

Given graphs \(\Gamma = (X, E)\) and \(\Gamma' = (X', E')\).

\begin{definition}

A map \(\sigma: X \to X'\) is an \emph{isomorphism}\textbackslash index\{isomorophism of graphs whenever;

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  \(\sigma\) is one-to-one and onto,
\item
  \(xy\in E\) if and only if \(\sigma x \sigma y \in E'\) for all \(x, y\in X\).
\end{enumerate}

\end{definition}

We do not distinguish between isomorphic graphs.

\begin{definition}
Suppose \(\Gamma = \Gamma'\). Above isomorphism \(\sigma\) is called an \emph{automorphism}\index{automorphism} of \(\Gamma\). Then set \(\mathrm{Aut}(\Gamma)\) of all automorphisms of \(\Gamma\) becomes a finite group under composition.
\end{definition}

\begin{definition}
If \(\mathrm{Aut}(\Gamma)\) acts transitive on \(X\), \(\Gamma\) is called \emph{vertex transitive}\index{vertex transitive}.
\end{definition}

\begin{example}
A Cayley graphs:
\end{example}

\begin{definition}[Cayley Graphs]
\protect\hypertarget{def:cayley}{}\label{def:cayley}Let \(G\) be any finite group, and \(\Delta\) any generating set for \(G\) such that \(1_G \not\in \Delta\) and \(g\in \Delta \to g^{-1}\in \Delta\).
Then Cayley graph \(\Gamma = \Gamma(G, \Delta)\) is defined on the vetex set \(X = G\) with the edge set \(E\) define by the following. \index{Cayley graph}

\[E = \{(h_1,h_2)\mid h_1, h_2\in G, h_1^{-1}h_2\in \Delta\} = \{(h, hg) \mid h\in G, g\in \Delta\}\]
\end{definition}

\begin{example}

\(G = \langle a \mid a^6 = 1\rangle\), \(\Delta = \{a, a^{-1}\}\).

\begin{center}\includegraphics{t-algebra_files/figure-latex/unnamed-chunk-2-1} \end{center}

\end{example}

\begin{example}

\(G = \langle a \mid a^6 = 1\rangle\), \(\Delta = \{a, a^{-1}, a^2, a^{-2}\}\).

\begin{center}\includegraphics{t-algebra_files/figure-latex/unnamed-chunk-3-1} \end{center}

\end{example}

\begin{example}

\(G = \langle a, b \mid a^6 = 1, b^2, ab = ba\rangle\), \(\Delta = \{a, a^{-1}, b\}\).

\begin{center}\includegraphics{t-algebra_files/figure-latex/unnamed-chunk-4-1} \end{center}

\end{example}

\begin{remark}
\(\mathrm{Aut}(\Gamma) \simeq D_6\times \mathbb{Z}_2\) contains two regular subgroups isomorphic to \(D_6\) and \(\mathbb{Z}_5 \times \mathbb{Z}_2\) and \(\Gamma\) is obtained as Cayley graphs in two ways.
\end{remark}

Cayley graphs are vertex transitive, indeed.

\begin{theorem}
\protect\hypertarget{thm:cayley-graph}{}\label{thm:cayley-graph}The following hold.

\((i)\) For any Cayley graph \(\Gamma = \Gamma(G, \Delta)\), the map

\[G \to \mathrm{Aut}(\Gamma) \; (g\mapsto \hat{g})\]
is an injective homomorphism of groups, where
\[\hat{g}(x) = gx \quad \textrm{for all }\: g\in G \textrm{ and for all } x\in X (= G).\]
Also, the image \(\hat{G}\) is regular on \(X\). i.e., the image \(\hat{G}\) acts transitively on \(X\) with trivial vertex stabilizers.

\((ii)\) For any graph \(\Gamma = (X, E)\), suppose there exists a subgroup \(G \subseteq \mathrm{Aut}(\Gamma)\) that is regular on \(X\). Pick \(x\in X\), and let

\[\Delta = \{g\in G\mid \langle x, g(x)\in E\}.\]
Then \(1\not\in \Delta\), \(g\in \Delta \to g^{-1}\in \Delta\), and \(\Delta\) generates \(G\). Moreover, \(\Gamma \simeq \Gamma(G, \Delta)\).
\end{theorem}

\begin{proof}
\((i)\) Let \(g\in G\). We want to show that \(\hat{g}\in \mathrm{Aut}(\Gamma)\). Let \(h_1, h_2\in X = G\). Then,
\begin{align}
(h_1, h_2)\in E & \to h_1^{-1}h_2\in \Delta\\
  & \to (gh_1)^{-1}(gh_2)\in \Delta \\
  & \to (gh_1, gh_2)\in E\\
  & \to (\hat{g}(h_1), \hat{g}(h_2)) \in E.
\end{align}
Hence, \(\hat{g}\in \mathrm{Aut}(\Gamma)\).

Observe: \(g \mapsto \hat{g}\) is a homomorphism of groups:
\[\hat{1}_G = 1, \; \widehat{g_1g_2} = \widehat{g_1}\widehat{g_2}.\]

Observe: \(g \mapsto \hat{g}\) is one-to-one:
\[\widehat{g_1} = \widehat{g_2} \to g_1 = \widehat{g_1}(1_G) = \widehat{g_2}(1_G) = g_2.\]

Observe: \(\hat{G}\) is regular on \(X\): Clear by construction.

\((ii)\) \(1_G\not\in \Delta\): Since \(\Gamma\) has not loops, \((x, 1_Gx) \not\in E\).

\(g\in \Delta \to g^{-1}\in \Delta\):
\[g\in \Delta \to (x, g(x))\in E \to E \ni (g^{-1}(x), g^{-1}(g(x))) = (g^{-1}(x), x).\]

\(\Delta\) generates \(G\): Suppose \(\langle \Delta \subsetneq G\). Let \(\hat{X} = \{g(x)\mid g\in \langle \Delta\rangle\} \subsetneq X\). (\(\hat{X} \subsetneq X\) as \(G\) acts regularly on \(X\).)

Since \(\Gamma\) is connected, there exists \(y\in \hat{X}\) and \(z\in X\setminus \hat{X}\) with \(yz\in E\).

Let \(y = g(x)\), \(g\in \langle \Delta\rangle\), \(z\in h(x)\), \(h\in G\setminus \langle \Delta\rangle\). Then
\[(y,z)=(g(x),h(x))\in E \to (x,g^{-1}h(x))\in E \to g^{-1}h\in \langle \Delta \rangle \to h\in \langle \Delta \rangle. \]
This is a contradition. Therefore, \(\Delta\) generates \(G\).

Let \(\Gamma' = (X', E')\) denote \(\Gamma(G, \Delta)\). We shall show that
\[\theta: X' \to X \; (g\mapsto g(x))\]
is an isomorphism of graphs.

\(\theta\) is one-to-one: For \(h_1, h_2\in X' = G\),
\[\theta(h_1)=\theta(h_2) \to h_1(x) = h_2(x) \to h_2^{-1}h_1(x)=x \to h_2^{-1}h_1\in \mathrm{Stab}_G(x) = \{1_G\} \to h_1 = h_2.\]
(\(\mathrm{Stab}_G = \{g\in G\mid g(x) = x\}\).)

\(\theta\) is onto: Since \(G\) is transitive,
\[X = \{g(x)\mid g\in G\} = \theta(X') = \theta(G).\]

\(\theta\) respects adjacency: For \(h_1, h_2\in X' = G\),
\[(h_1,h_2)\in E' \leftrightarrow h_1^{-1}h_2\in \Delta \leftrightarrow (x, h_1^{-1}h_2(x))\in E \leftrightarrow (h_1(x),h_2(x))\in E \leftrightarrow (\theta(h_1), \theta(h_2))\in E.\]
Therefore \(\theta\) is an isomorphism between graphs \(\Gamma(G, \Delta)\) and \(\Gamma(X, E)\).
\end{proof}

How to compute the eigenvalues of the Cayley graph of and abelian group.

Let \(G\) be any finite abelian group. Let \(\mathbb{C}^*\) be the multiplicative group on \(\mathbb{C}\setminus \{0\}\).

\begin{definition}
A (linear) \(G\)-character\index{character} is any group homomorphism \(\theta: G \to \mathbb{C}^*\).
\end{definition}

\begin{example}
\(G = \langle a\mid a^3 =1\rangle\) has three characters, \(\theta_0, \theta_1, \theta_2\).
\[
\begin{array}{c|ccc}
\theta_i(a^j) & 1 & a & a^2 \\
\hline
\theta_0 & 1 & 1 & 1\\
\theta_1 & 1 & \omega & \omega^2\\
\theta_2 & 1 & \omega^2 & \omega
\end{array}, \quad \textrm{with }\; \omega = \frac{-1+\sqrt{-3}}{2}.
\]
Here \(\omega\) is a primitive cube root pf \(q\) in \(\mathbb{C}^*\), i.e., \(1+\omega + \omega^2 = 0\).
\end{example}

For arbitraty group \(G\), let \(X(G)\) be the set of all characters of \(G\).

Observe: For \(\theta_1, \theta_2\in X(G)\), one can define product \$\theta\_1\theta\_2:
\[\theta_1\theta_2(g) = \theta_1(g)\theta_2(g) \quad \textrm{for all }\; g\in G.\]
Then \(\theta_1\theta_2\in X(G)\).

Observe: \(X(G)\) with this product is an (abelian) group.

\begin{lemma}
\protect\hypertarget{lem:charactergroup}{}\label{lem:charactergroup}The groups \(G\) and \(X(G)\) are isomorphic for all finite abelian groups \(G\).
\end{lemma}

\begin{proof}
\(G\) is a direct sum of cyclic groups;
\[G = G_1\oplus G_2 \oplus \cdots \oplus G_m, \quad \textrm{where } \; G_i = \langle a_i\mid a_i^{d_i} = 1\rangle \quad (1\leq i\leq m).\]
Pick any alement \(\omega_i\) of order \(d_i\) in \(\mathbb{C}^*\), i.e., a primitive \(d_i\)-the root of \(1\). Define
\[\theta_i: G \to \mathbb{C}^* \quad (a_1^{\varepsilon_1}\cdots a_m^{\varepsilon_m} \mapsto \omega_i^{\varepsilon_i} \quad \textrm{where }\; 0\leq \varepsilon_i < d_i, 1\leq i\leq m).\]
Then \(\theta_i\in X(G)\). (Exercise)

Claim: There exists an isomorphism of groups \(G \to X(G)\) that sends \(a_i\) to \(\theta_i\).

Observe: \(\theta_i^{d_i} = 1\). For every \(g = a_1^{\varepsilon_1}\cdots a_m^{\varepsilon_m} \in G\),
\[\theta_i^{d_i}(g) = (\theta_i(g))^{d_i} = (\omega_i^{\varepsilon_i})^{d_i} = (\omega_i^{d_i})^{\varepsilon_i} = 1.\]
Observe: If \(\theta_1^{\varepsilon_1}\theta_2^{\varepsilon_2}\cdots \theta_m^{\varepsilon_m} = 1\) for some \(0\leq \varepsilon_i < d_i, 1\leq i\leq m\). Then \(\varepsilon_1 = \varepsilon_2 = \cdots = \varepsilon_m = 0\).

\emph{Pf.} \(1 = \theta_1^{\varepsilon_1}\theta_2^{\varepsilon_2}\cdots \theta_m^{\varepsilon_m}(a_i) = \omega_i^{\varepsilon_i}\), Since \(\omega_i\) is a primitive \(d_i\)-th root of \(1\), \(\varepsilon_i = 0\) for \(1\leq i\leq m\).

Observe: \(\theta_1, \ldots, \theta_m\) generate \(X(G)\). Pick \(\theta\in X(G)\). Since
\(a_i^{d_i} = 1\), \(1 = \theta(a_i^{d_i}) = \theta(a_i)^{d_i}\).

Hence \(\theta(a_i) = \omega^{\varepsilon_i}\) for some \(\varepsilon_i\) with \(0\leq \varepsilon_i < d_i\).

Now \(\theta = \theta_1^{\varepsilon_1}\cdots \theta_m^{\varepsilon_m}\), since these are both equal to \(\omega_i^{\varepsilon_i}\) at \(a_i\) for \(1\leq i \leq m\).

Therefore,
\[G \to X(G) \quad (a_i \mapsto \theta_i)\]
is an isomorphism of groups.
\end{proof}

\textbf{Note.} The correspondence above is clearly a group homomorphism.

\hypertarget{lec4}{%
\chapter{Examples}\label{lec4}}

\textbf{Wednesday, January 27, 1993}

\begin{theorem}
\protect\hypertarget{thm:charcter-group}{}\label{thm:charcter-group}Given a Cayley graph \(\Gamma = \Gamma(G, \Delta)\). View the standard module \(V \equiv \mathbb{C}G\) (the group algebra), so
\[\left\langle \sum_{g\in G}\alpha_g g, \;\sum_{g\in G}\beta_g g\right\rangle = \sum_{g\in G}\alpha_g\overline{\beta_g}, \quad \textrm{with}\; \alpha_g, \beta_g\in \mathbb{C}.\]
For any \(\theta\in X(G)\), write
\[\hat{\theta} = \sum_{g\in G}\theta(g^{-1})g.\]
Then the following hold.

~~\((i)\) \(\langle \hat{\theta_1}, \hat{\theta_2}\rangle = |G|\) if \(\theta_1 = \theta_2\) and \(0\) othewise for \(\theta_1, \theta_2\in X(G)\). In particular, \(\{\hat{\theta}\mid \theta\in X(G)\}\) forms a basis for \(V\).

~~\((ii)\) \(A\hat{\theta} = \Delta_\theta \hat{\theta}\) for \(\theta \in X(G)\), where \(A\) is the adjacency matrix and

\[\Delta_\theta = \sum_{g\in \Delta}\theta(g).\]
In particular, the eigenvalues of \(\Gamma\) are precisely
\[\Delta_\theta \mid \theta\in X(G)\}.\]
\end{theorem}

\begin{proof}
\leavevmode

\((i)\) Claim: For every \(\theta \in X(G)\), let

\[s:= \sum_{g\in G}\theta(g^{-1}) = \begin{cases} |G| & \text{if }\;\theta = 1\\
0 & \text{if } \;\theta \neq 1. 
\end{cases}\]
\emph{Pf.} Clear if \(\theta =1\).

Let \(\theta \neq 1\). Then \(\theta(h)\neq 1\) for some \(h\in G\).
\[s\cdot \theta(h) = \left(\sum_{g\in G}\theta(g^{-1})\right)\theta(h) = \sum_{g\in G}\theta(g^{-1}h) = \sum_{g'\in G}\theta(g'^{-1}) = s.\]
Since \(\theta(h)\neq 1\), \(s = 0\).

Claim. \(\theta(g^{-1}) = \overline{\theta(g)}\) for every \(\theta\in X(G)\) and every \(g\in G\).

Since \(\theta(g)\in \mathbb{C}\) is a root of \(1\),
\[|\theta(g)|^2 = \theta(g)\overline{\theta(g)} = 1.\]
On the other hand, since \(\theta\) is a homomorphism,
\[\theta(g)\theta(g^{-1}) = \theta(1) = 1.\]
Hence \(\theta(g^{1}) = \overline{\theta(g)}\).

Now
\begin{align}
\langle \widehat{\theta_1}, \widehat{\theta_2}\rangle & = \sum_{g\in G}\theta_1(g^{-1})\overline{\theta_2(g^{-1})}\\
& = \sum_{g\in G}\theta_1(g^{-1})\theta_2(g)\\
& = \sum_{g\in G}\theta_1\theta_2^{-1}(g^{-1})\\
& = \begin{cases} |G| & \text{if}\quad \theta_1\theta_2^{-1} = 1\\
0 & \text{if} \quad \theta_1\theta_2^{-1}\neq 1.
\end{cases}
\end{align}
Since \(|G| = |X(G)|\) by Lemma \ref{lem:charactergroup}, and \(\widehat{\theta_i}\)'s are orthogonal nonzero elements in \(V\), thet form a basis of \(V\).

~\((ii)\) Let \(\Delta = \{g_1, \ldots, g_r\}\). Then

\begin{align}
A\hat{\theta} & = A\left(\sum_{g\in G}\theta(g^{-1}g)\right)\\
& = \sum_{g\in G}\theta(g^{-1})(gg_1 + \cdots + gg_r) \quad (\Gamma(g) = \{gg_1, \ldots, gg_r\})\\
& = \sum_{i = 1}^r \left(\sum_{g\in G}\theta(g^{-1})(gg_i)\right)\\
& = \sum_{i=1}^r\left(\sum_{g\in G}\theta(g_ig_i^{-1}g^{-1})(gg_i)\right)\\
& = \sum_{i = 1}^r\left(\sum_{g\in G}\theta(g_i)\theta((gg_i)^{-1})gg_i\right)\\
& = \sum_{i = 1}^r\theta(g_i)\sum_{h\in G}\theta(h^{-1})h \\
& = \Delta_\theta\cdot \hat{\theta}.
\end{align}
Since \(\{\hat{\theta}\mid \theta\in X(G)\}\) forms a basis, the eigenvalues of \(\Gamma\) are precisely,
\[\{\Delta_\theta\mid \theta\in X(G)\}.\]

This completes the proof.

\end{proof}

\begin{example}
Let \(G = \langle a\mid a^6 = 1\rangle\), and \(\Delta = \{a, a^{-1}\}\). Pick a primitive 6-th root of 1, \(\omega\). Then
\[X(G) = \{\theta^i\mid 0\leq i\leq 5\} \quad \text{such that }\quad \theta(a) = \omega, \; \omega + \omega^{-1} = 1.\]

\begin{center}\includegraphics{t-algebra_files/figure-latex/unnamed-chunk-5-1} \end{center}

\[\begin{array}{c | c | c}
\varphi\in X(G) & \varphi(a) & \Delta_\varphi = \theta(a) + \theta(a)^{-1}\\
\hline
1 & 1 & 2\\
\theta & \omega & \omega+\omega^{-1} = 1\\
\theta^2 & \omega^2 & -1\\
\theta^3 & \omega^3 = -1 & -2\\
\theta^4 & \omega^4 & -1\\
\theta^5 & \omega^5 & 1
\end{array}\]
\[\text{Spec}(\Gamma) = \begin{pmatrix} 2 & 1 & -1 & -2\\ 1 & 2 & 2 & 1\end{pmatrix}.\]
\end{example}

\begin{example}
\(D\)-cube, \(H(D,2)\). Let
\[X = \{(a_1, \ldots, a_D)\mid a_i\in \{1,-1\}, \; 1\leq i\leq D\},\]
\[E = \{xy\mid x, y\in X, \; x, y \text{: different in exactly one coordinate}\}.\]
Also \(H(D,2)\) is a Cayley graph \(\Gamma(G, \Delta)\), where
\[G = G_1\oplus G_2 \oplus \cdots \oplus G_D, \]
\[G_i = \langle a_i\mid a_i^2  = 1\rangle,\quad \Delta = \{a_1, \ldots, a_D\}.\]
\end{example}

\textbf{Homework}: The spectrum of \(H(D,2)\) is
\[\begin{pmatrix} \theta_0 & \theta_1 & \cdots & \theta_D\\
m_0 & m_1 & \cdots & m_D\end{pmatrix},\]
where
\[\theta_i = D-2i \quad (0\leq i\leq D), \quad m_i = \binom{D}{i}.\]

\begin{remark}
Let \(\theta \in X(G)\). Then \(\theta: X \to \{\pm 1\}\). If
\[\nu(\theta) = |\{i\mid \theta(a_i) = -1\}|, \]
then \(\Delta_\theta = D-2i\). Since there are \(\binom{D}{i}\) such \(\theta\), we have te assertion.
\end{remark}

We want to compute the subconstituent algebra for \(H(D,2)\). First, we make a few observations about arbitrary graphs.

Let \(\Gamma = (X,E)\) be any graph, \(A\), the adjacemcy matrix of \(\Gamma\), and \(V\), the standard module over \(K = \mathbb{C}\).

Fix a base \(x\in X\). Write \(E_i^* = E_i^*(x)\), and
\[T \equiv T(x) = \text{the algebra generated by}\; A, E_0^*, E_1^*, \ldots .\]

\begin{definition}
Let \(W\) be any orreducible \(T\)-module (\(\subseteq V\)). Then the endpoint\index{endpoint} \(r \equiv r(W)\) satisfied
\[r = \min\{i\mid E_i^*W \neq 0\}.\]
The diameter\index{diameter} \(d = d(W)\) satisfied
\[d = |\{i\mid E_i^*W \neq 0\}| - 1.\]
\end{definition}

\begin{lemma}
\protect\hypertarget{lem:irreducible}{}\label{lem:irreducible}

With the above notation, let \(W\) be an irreducible \(T\)-module. Then

\((i)\) \(E_i^*AE_j^* = 0 \; \text{ if }\; |i-j|=1, \; E_i^*AE_j^*\neq 0 \; \text{ if }\; |i-j| = 1, \quad 0\leq i,j\leq d(x)\).\\
\((ii)\) \(AE_j^*W \subseteq E_{j-1}^*W + E_j^*W + E^*_{j+1}W\), \(0\leq j \leq d(x)\). \((E_i^*W = 0 \; \text{ if } i<j\) or \(i > d(x)\).)\\
\((iii)\) \(E^*_jW \neq 0\) if \(r\leq j \leq r+d\), \(=0\) if \(0\leq j\leq r\) or \(r+d < j \leq d(x)\).\\
\((iv)\) \(E_i^*AE^*_jW \neq 0\), if \(|i-j| = 1\) \((r \leq i,j \leq r+d)\).

\end{lemma}

\begin{proof}
\leavevmode

\((i)\) Pick \(y\in X\) with \(\partial(x,y) = j\). We want to find \(E_i^*AE^*_j \hat{y}\). Note,

\[E_j^*\hat{y} = \begin{cases} 0 & \text{if }\; \partial(x.y)\neq j\\
\hat{y} & \text{if }\; \partial(x,y) = j.\end{cases}.\]
\begin{align}
E_i^*AE_j^*\hat{y} &= E_i^*A\hat{y} \\
& = E_i^*\sum_{z\in X, yz\in E}\hat{z}\\
& = \sum_{z\in X, yz\in E, \partial(x, z) = i}\hat{z}  \label{eq:eiaeejy}\\
& = 0 \; \text{ if }\; |i-j|>1 && \text{by triangle inequality.}
\end{align}
If \(|i-j| = 1\), there exist \(y, y'\in X\) such that \(\partial(x,y) = j\), \(\partial(x,y') = i\), \(yy'\in E\) by connectivity of \(\Gamma\). Hence
\eqref{eq:eiaeejy} contains \(\widehat{y'}\) and \eqref{eq:eiaeejy} is not equal to zero.

\((ii)\) We have

\begin{align}
AE_j^*W & = \left(\sum_{i=0}^{d(x)}E_i^*\right)AE_j^*W\\
& = E_{j-1}^*AE^*_jW + E^*_jAE_j^*W + E^*_{j+1}AE_j^*W\\
& \subseteq E^*_{j-1}W + E^*_jW + E^*_{j+1}W.
\end{align}

\((iii)\) Suppose \(E_j^*W = 0\) for some \(j\) \((r\leq j \leq r+d)\). Then \(r < j\) by the definition of \(r\). Set

\[\widetilde{W} = E^*_rW + E^*_{r+1}W + \cdots + E^*_{j-1}W.\]
Observe \(0\subsetneq \widetilde{W} \subsetneq W\).
Also \(A\widetilde{W} \subseteq \widetilde{W}\) by \((ii)\), and \(E_i^*\widetilde{W} \subseteq \widetilde{W}\) for every \(i\) by construction.

Thus, \(T\widetilde{W} \subseteq \widetilde{W}\), contradicting \(W\) beging irreducible.

\end{proof}

\hypertarget{lec5}{%
\chapter{\texorpdfstring{\(T\)-Modules of \(H(D,2)\), I}{T-Modules of H(D,2), I}}\label{lec5}}

\textbf{Friday, January 29, 1993}

Let \(\Gamma = (X, E)\) be a graph, \(A\) the adjacency matrix, and \(V\) the standard module over \(K = \mathbb{C}\).

Fix a base \(x\in X\) and write \(E_i^* \equiv E_i^*(x)\), and \(T \equiv T(x)\).

Let \(W\) be an irreducible \(T\)-module with endpoint \(r:= \min\{i\mid E_i^*W \neq 0\}\) and diameter \(d:=|\{i\mid E_i^*W\neq 0\}|-1\).

We have
\begin{align}
E_i^*W & \neq 0 & r\leq i \leq r+d\\
& = 0 & 0 \leq i < r \;\text{ or }\; r+d < i \leq d(x).
\end{align}

Claim: \(E_i^*AE_j^*W \neq 0\) if \(|i-j| = 1\) for \(r\leq i,j\leq r+d\). (See Lemma \ref{lem:irreducible}.)

Suppose \(E_{j+1}^*AE_j^*W = 0\) for some \(j\) with \(r \leq j < r+d\).
Observe that
\[\tilde{W} = E^*_rW + \cdots + E^*_jW\]
is \(T\)-invariant with
\[0 \subsetneq \tilde{W} \subsetneq W.\]
Becase \(A\tilde{W} \subseteq \tilde{W}\) since \(AE_j^*W \subseteq E^*_{j-1}W + E^*_jW\),
\[E_k^*\tilde{W} \subseteq \tilde{W} \quad\text{for all }\; k,\]
we have \(T\tilde{W} \subseteq{W}\).

Suppose \(E_{i-1}^*AE_i^*W = 0\) for some \(i\) with \(r \leq i < r+d\).

Similarly,
\[\tilde{W} = E^*_iW + \cdots + E^*_{r+d}.W\]
is a \(T\)-module with \(0\subsetneq \tilde{W} \subsetneq W\).

\begin{definition}
\protect\hypertarget{def:isomorphic-modules}{}\label{def:isomorphic-modules}Let \(\Gamma\), \(E^*_i\), and \(T\) be as above. Irreducible \(T\)-modules \(W\) and \(W'\) are isomorphic\index{isomorphic} whenever there is an isomorphism \(\sigma: W \to W'\) of vector spaces such that \(a\sigma = \sigma a\) for all \(a\in T\).
\end{definition}

Recall that the standard module \(V\) is an orthogonal direct sum of irreducible \(T\)-modules \(W_1 \oplus W_2 \oplus \cdots\). Given \(W\) in this list, the multiplicity\index{multiplicity} of \(W\) in \(V\) is
\[|\{j \mid W_j \simeq W\}|.\]

\begin{remark}
It is known that the multiplicity does not depend on the decomposition.
\end{remark}

Now assume that \(\Gamma\) is the \(D\)-cube, \(H(D,2)\) with \(D\geq 1\). Vew
\begin{align}
X & = \{a_1\cdots a_D\mid a_i\in \{1, -1\}, 1\leq i\leq D\},\\
E & = \{xy\mid x, y\in X, \; x, y \;\text{ differ in exactly 1 coordinate.}\}.
\end{align}
Find \(T\)-modules.

Claim: \(H(D,2)\) is bipartite with a partition \(X = X^+ \cup X^-\), where
\begin{align}
X^+ & = \{a_1\cdots a_D\in X\mid \prod a_i > 0\}\\
X^- & = \{a_1\cdots a_D \in X \mid \prod a_i < 0\}
\end{align}

Observe: for all \(y, z\in X\),
\[\partial(y,z) = i \Leftrightarrow y, z \; \text{ differ in exactly in }\; i\; \text{ coorinates with }\; 0\leq i\leq D.\]
Here, the diameter of \(H(D, 2) = D = d\) for all \(x\in X\).

\begin{theorem}
\protect\hypertarget{thm:hd2-modules}{}\label{thm:hd2-modules}Let \(\Gamma = H(D,2)\) be as above. Fix \(x\in X\), and write \(E_i^* = E^*_i(x)\), and \(T = T(x)\).

Let \(W\) be an irreducible \(T\)-module with endpoint \(r\), and diameter \(d\) with \(0\leq r \leq r+d\leq D\).

\((i)\) \(W\) has a basis \(w_0, w_1, \ldots, w_d\) with \(w_i\in E^*_{i+r}W\) for \(0\leq i\leq d\). With respect to which the matrix representing \(A\) is

\[
\begin{pmatrix}
0 & d & 0 & \cdots & 0 & 0 & 0\\
1 & 0 & d-1 & \cdots & 0 & 0 & 0\\
0 & 2 & 0 & \cdots & 0 & 0 & 0\\
0 & 0 & 3 & \cdots & 0 & 0 & 0\\
\cdots & \cdots & \cdots & \ddots & \ddots & \cdots & \cdots \\
0 & 0 & 0 & \ddots & 0 & 2 & 0\\
0 & 0 & 0 & \cdots & d-1 & 0 & 1\\
0 & 0 & 0 & \cdots & 0 & d & 0
\end{pmatrix}
\]

\((ii)\) \(d= D-2r\). In particular, \(0\leq r\leq D/2\).

\((iii)\) Let \(W'\) denote an irreducible \(T\)-module with endpoint \(r'\). Then \(W\) and \(W'\) are isormorphic as \(T\)-modules if and only if \(r = r'\).

\((iv)\) The multiplicity of the irreducible \(T\)-module with endpoint \(r\) is

\[\binom{D}{r} - \binom{D}{r-1} \quad \text{if } 1\leq r \leq R/2,\]
and \(1\) if \(r = 0\).
\end{theorem}

\begin{proof}
Recall that \(\Gamma\) is vertex transitive. It is a Cayley graph.

Hence without loss of generality, we may assume that
\(x = \overbrace{11\cdots 1}^{D}\).

Notation: Set \(\Omega = \{1, 2, \ldots, D\}\). For every subset \(S \subseteq \Omega\), let
\[\hat{S} = a_1\cdots a_d \in X \quad a_i = \begin{cases} -1 & \text{if }\; i\in S\\ 1 & \text{if } i\not\in S.\end{cases}\]
In particular, \(\hat{\emptyset} = x\) and
\[|S| = i \Leftrightarrow \partial(x, \hat{S}) = i \Leftrightarrow \hat{S}\in E^*_iV.\]
For all \(S, T\subseteq \Omega\), we say \(S\) covers \(T\) if and only if \(S\supseteq T\) and \(|S| = |T| +1\).

Observe that \(\hat{S}, \hat{T}\) are adjacent in \(\Gamma\) if and only if either \(T\) coverse \(S\) or \(S\) coverr \(T\).

Define the `raising matrix'
\[R = \sum_{i=0}^D E^*_{i+1}AE^*_i.\]

Observe that
\[RE_i^*V \subseteq E^*_{i+1} V \; \text{ for }\; 0\leq i \leq D, \; \text{ and }E^*_{D+1}V = 0.\]
Indeed for any \(S\subseteq \Omega\) with \(|S| = i\),
\begin{align}
R\hat{S} & = RE^*_i\hat{S} \\
& = E^*_{i+1}A\hat{S} \\
& = \sum_{T_1 \subseteq \Omega, S \text{ covers }T_1} E^*_{i+1}\widehat{T_1} + \sum_{T \subseteq \Omega, T \text{ covers }S} E^*_{i+1}\hat{T}\\
& = \sum_{T \subseteq \Omega, T \text{ covers }S} E^*_{i+1}\hat{T}.
\end{align}

Define the `lowering matrix'
\[L = \sum_{i=0}^D E^*_{i-1}AE^*_i.\]

Observe that
\[LE_i^*V \subseteq E^*_{i-1}V \; \text{ for }\; 0\leq i \leq D, \; \text{ and }E^*_{-1}V = 0.\]
Indeed for any \(S\subseteq \Omega\),
\[L\hat{S} = \sum_{T\subseteq \Omega, S \text{ covers }T} \hat{T}.\]

Observe that \(A = L + R\).

For convenience, set
\[A^* = \sum_{i=0}^D (D-2i)E_i^*.\]

Claim: The following hold.

\((a)\) \(LR - RL = A^*\).\\
\((b)\) \(A^*L - LA^* = 2L\).\\
\((c)\) \(A^*R - RA^* = -2R\).

In particular \(\mathrm{Span}(R,L, A^*)\) is a 'representation of Lie algebra \(\mathrm{sl}_2(\mathbb{C})\).

\begin{remark}[Lie Algebra sl2(C)]
\[\mathrm{sl}_2(\mathbb{C}) = \{X\mid \mathrm{Mat}(\mathbb{C} \mid \mathrm{tr}(X) = 0\}.\]
For \(X, Y\in \mathrm{sl}_2(\mathbb{C})\), define a binary operation \([X, Y] = XY - YX\).
\[A^*\sim \begin{pmatrix} 1 & 0 \\ 0 & -1\end{pmatrix}, \quad L\sim \begin{pmatrix} 0 & 1 \\ 0 & 0\end{pmatrix}, \quad R\sim \begin{pmatrix} 0 & 0 \\ 1 & 0\end{pmatrix}.\]
Then these satisfy the relations \((a)\) - \((c)\) above.
\end{remark}

\emph{Proof of Claim.}
Apply both sides to \(\hat{S}\) \quad \((S\subseteq \Omega)\). Say \(|S| = i\).

\emph{Proof of \((a)\):}
\begin{align} 
(LR - RL)\hat{S} & = L\left(\sum_{\substack{T \subseteq \Omega, T \text{ covers }S\\(D-i \text{ of them})}}\hat{T}\right) - R \left(\sum_{\substack{U \subseteq \Omega, S \text{ covers }U\\(i \text{ of them})}}\hat{T}\right)\\
& = (D-i)\hat{S} + \sum_{V \subseteq \Omega, |V| = i, |S\cap V| = i-1}\hat{V} - \left(i\hat{S} + \sum_{V \subseteq \Omega, |V| = i, |S\cap V| = i-1}\hat{V}\right)\\
& = (D-2i)\hat{S}\\
& = A^*\hat{S}.
\end{align}

\emph{Proof of \((b)\):}
\begin{align} 
(A^*L - LA^*)\hat{S} & = (D-2(i-1))L\hat{S} - (D-2i)L\hat{S} \quad (\text{since} \; L\hat{S}\in E^*_{i-1}V)\\
& = 2L\hat{S}.
\end{align}

\emph{Proof of \((c)\):}
\begin{align} 
(A^*R - RA^*)\hat{S} & = (D-2(i+1))R\hat{S} - (D-2i)R\hat{S} \quad (\text{since} \; R\hat{S}\in E^*_{i+1}V)\\
& = 2R\hat{S}.
\end{align}

Let \(W\) be an irreducible \(T\)-module with endpoint \(r\) and diameter \(d\) \((0\leq r \leq r+d \leq D)\).

\emph{Proof of \((i)\) and \((ii)\):}

Pick \(0\neq w \in E^*_rW\).

Claim: \(LRw = (D-2r)w\).

\emph{Pf.}
\begin{align} 
LRw & = (A^*+RL)w \quad (\text{by Claim }(a))\\
& = A^*w \quad (Lw \in E^*_{r-1}W = 0)\\
& (D-2r)w.
\end{align}
Define
\[w_i = \frac{1}{i!}R^iw \in E^*_{r+i}W \quad (0\leq i \leq d).\]
Then,
\begin{align}
Rw_i & = (i+1)w_{i+1}\quad (0\leq i \leq d)\\
Rw_d & = 0 \quad (\text{by definition of }d)
\end{align}

Claim: \(Lw_0 = 0\) and
\[Lw_i = (D-2r-i+1)w_{i-1} \quad (1\leq i\leq d).\]

\emph{Pf.} We prove by induction on \(i\).
The case \(i=0\) is trivial, and the case \(i=1\) follows from above claim.
Let \(i\geq 2\),
\begin{align}
Lw_i & = \frac{1}{i}LRw_{i-1} = \frac{1}{i}(A^*+RL)w_{i-1} \quad (\text{by Claim (a)})\\
& \quad \text{(by induction hypothesis)}\\
& = \frac{1}{i}((D-2(r+i-1))w_{i-1} + (D-2r-(i-1)+1)Rw_{i-2} \quad (Rw_{i-2} = (i-1)w_{i-1})\\
& = \frac{1}{i}i(D-2r-i+1)w_{i-1}\\
& = (D-2r-i+1)w_{i-1}.
\end{align}

Claim: \(w_0, \ldots, w_d\) is a basis for \(W\).

\emph{Pf.}
Let \(W' = \mathrm{Span}\{w_0, \ldots, w_d\}\). Then \(W'\) is \(R\) and \(L\) invariant. So it is \(A = R+L\) invariant.

Also it is \(E^*_i\)-invariant for every \(i\).

Hence \(W'\) is a \(T\)-module.

Since \(W\) is irreducible, \(W' = W\).

As \(w_i\)'s are orthogonal, theyy are linearly independent. Note that \(w_i\neq 0\) by the definition of \(d\) and Lemma \ref{lem:irreducible} \((iv)\).

Claim: \(d = D-2r\).

\emph{Pf.}
By \((a)\),
\begin{align}
0 & = (LR - RL - A^*)w_d \\
& = 0 - (D-2r-d+1)Rw_{d-1} - (D-2(r+d))w_d\\
& = -d(D-2r-d+1)w_d - (D-2(r+d))w_d\\
& = (-dD + 2rd + d^2 - d - D + 2r + 2d)w_d\\
& = (d^2 + (2r-D+1)d + 2r - D)w_d\\
& = (d+2r-D)(d+1)w_d.
\end{align}
Hence \(d = D-2r\).

Therefore, with respect to a bais \(w_0, w_1, \ldots, w_d\),
\(A = L+R\), \(w_{-1} = w_{d+1} = 0\),
\[Lw_i = (d-i+1)w_{i-1}, \quad Rw_i = (i+1)w_{i+1}.\]
\[L = \begin{pmatrix} 0 & d & 0 & \cdots & 0 & 0\\
0 & 0 & d-1 & \cdots & 0 & 0\\
\vdots & \vdots & \ddots & \ddots & \vdots & \vdots \\
\vdots & \vdots & \cdots & \cdots & 0 & 1\\
0 & 0 & 0 & \cdots & 0 & 0
\end{pmatrix}, \qquad 
R = \begin{pmatrix} 0 & 0 & 0 & \cdots & 0 & 0\\
1 & 0 & 0 & \cdots & 0 & 0\\
0 & 2 & 0 & \cdots & \vdots & \vdots\\
\vdots & \vdots & \ddots  & \ddots & 0 & 1\\
0 & 0 & 0 & \cdots & d & 0
\end{pmatrix}.\]
This completes the proof of \((i)\) and \((ii)\).
\end{proof}

\hypertarget{lec6}{%
\chapter{\texorpdfstring{\(T\)-Modules of \(H(D,2)\), II}{T-Modules of H(D,2), II}}\label{lec6}}

\textbf{Monday, February 1, 1993}

\begin{proof}[Proof of Theorem \ref{thm:hd2-modules} Continued]
\leavevmode

\((iii)\) Let \(r = r'\),

\(w_0,\ldots, w_d\): a basis for \(W\) with \(w_i\in E^*_iW\), and

\(w_0', \ldots, w_d'\): a basis for \(W'\) with \(w_i'\in E^*_iW'\).

Then \(d = D-2r = D-2r' = d'\), and
\[\sigma: W \to W' \quad (w_i\mapsto w_i')\]
is an isomorsphism of \(T\)-modules by \((i)\).

If \(r\neq r'\), then
\[d = D-2r \neq D-2r' = d',\]
hence, \(\dim W \neq \dim W'\).

\((iv)\) Let \(W_i\) be the irreducible \(T\)-module with endpoint \(i\). Then

\[\dim E_r^*V = \binom{D}{r} = \sum_{i=0}^r \mathrm{mult}(W_i).\]
Hence, we have that
\[\mathrm{mult}(W_r) = \binom{D}{r} - \binom{D}{r-1}\]
by induction on \(r\).

\end{proof}

\begin{theorem}
\protect\hypertarget{thm:hd2-modules2}{}\label{thm:hd2-modules2}Let \(\Gamma = H(D,2)\) with \(D\geq 1\). Fix a vertex \(x\in X\) and write
\[E^*_i \equiv E^*_i(x), \quad T = T(x), and A^* \equiv \sum_{i=0}^D(D-2i)E^*_i.\]
Let \(W\) be an irreducible \(T\)-module with endpoint \(r\) with \(0\leq r\leq D/2\). Then,

\((i)\) \(W\) has a basis

\[w_0^*, w_1^*, \ldots, w_d^* \quad(d = D-2r), \; \text{ such that }\; w_i^*\in E_{i+r}W\; (0\leq i \leq d)\]
with respect to which the matrix corresponding to \(A^*\) is
\[\begin{pmatrix} 
0 & d & 0   &  & & &  \\
1 & 0 & d-1 &  & & & \\
0 & 2 & 0   &  & & & \\
  &   & \ddots    & \ddots & \ddots &  & \\
  &   &                   & & 0 & 2 & 0 \\
  &   &      & & d-1 & 0 & 1\\
  &   &      & & 0 & d & 0
\end{pmatrix}.\]
In particular,
\textbar{} \((ii)\) \(E_iA^*E_j = 0\) if \(|i-j|\neq 1\) for \(0 \leq i, j\leq D\).
\end{theorem}

\begin{proof}
We use the notation,
\[[\alpha, \beta] = \alpha\beta - \beta\alpha \; (=-[\beta, \alpha]).\]

Recall that

\((a)\) \([L, R] = A^*\),\\
\((b)\) \([A^*, L] = wL\),\\
\((c)\) \([A^*, R] = -2R\),

and \(A = L + R\).

Write \((a) - (c)\) in terms of \(A\) and \(A^*\), we have,
\[[A, A^*] = [L, A^*]+ [R, A^*] = 2(R-L).\]
\[\begin{cases} R + L & = A\\
R-L & = [A,A^*]/2. \end{cases}.\]
Hence,
\begin{align}
R & = \frac{1}{4}(2A + [A, A^*]) \quad \text{ and }\\
L & = \frac{1}{4}(2A - [A, A^*]).
\end{align}

Now \((a)\), \((b)\) become
\begin{align}
A^2A^* - 2AA^*A + A^*A^2 - 4A^*  & = 0  \label{eq:6-1}\\
{A^*}^2A - 2A^*AA^* + A{A^*}^2 - 4A  & = 0 \label{eq:6-2}
\end{align}
\emph{Pf.} \quad By \((b)\),
\begin{align}
2A - AA^* + A^*A & = 4L\\
& = 2[A^*, L]\\
& = A^* \frac{2A - [A,A^*]}{2} - \frac{2A - [A, A^*]}{2}A^*
\end{align}
So we have (\eqref{eq:6-2})
\[
{A^*}^2A - 2A^*AA^* + A{A^*}^2 - 4A   = 0.
\]

By \((a)\),
\begin{align}
-16A^* & = [2A + [A, A^*], 2A - [A, A^*]]\\
& (2A + [A, A^*])(2A - [A, A^*]) - (2A - [A,A^*])(2A + [A, A^*])\\
& = [4A^2 - 2A[A, A^*] + [A, A^*](2A) - [A,A^*]^2\\
& \quad - 4A^2 - 2A[A, A^*] + [A, A^*](2A) + [A, A^*]^2\\
& = -4A^2A^* + 4AA^*A + 4AA^*A - 4A^*A^2.
\end{align}
So,
\[A^2A^* - 2AA^*A + A^*A^2 - 4A^* = 0.\]

Claim: \(E_i^*A^*E_j = 0\) if \(|i-j| \neq 1\) for \(0\leq i, j\leq D\).

\emph{Pf.} We have,
\begin{align}
0 & = E_i(A^2A^* - 2AA^*A + A^*A^2 - 4A^*)E_j\\
& = E_iA^*E_j(\theta_i^2 - 2\theta_i\theta_j + \theta_j^2 - 4)\\
& \quad (AE_j = \theta_jE_j, \; E_iA = (AE_j)^\top = (\theta_iE_i)^\top = \theta_iE_i)\\
& = E_iA^*E_j(\theta_i - \theta_j -2)(\theta_i - \theta_j + 2)\\
& = E_iA^*E_j(D-2i - (D-2j)-2)(D-2i - (D-2j) + 2)\\
& \quad (\theta_k = D-2k)\\
& = E_iA^*E_j \cdot 4(i-j+1)(i-j-1)
\end{align}
and \(i-j+1 \neq 0\), \(i-j-1\neq 0\).
Hence, \(E_i^*A^*E_j = 0\).

Now define ``dual raising matrix'',
\[R^* = \sum_{i=0}^D E_{i+1}A^*E_i.\]
So,
\[R^*E_iV \subseteq E_{i+1}V, \quad (0\leq i\leq D, \; E_{D+1}V = 0).\]
Define ``dual lowering matrix''
\[L^* = \sum_{i=0}^D E_{i-1}A^*E_i.\]
Then
\[L^*E_iV \subseteq E_{i-1}V \quad (0\leq i\leq D, \; E_{-1}V = 0).\]
Observe that
\[A^* = \left(\sum_{i=0}^DE_i\right)A^*\left(\sum_{j=0}^DE_j\right) = L^* + R^*\]
by Claim 1.

Claim 2. We have
\textbar{} \((a)\) \([L^*, R^*] = A\),
\textbar{} \((b)\) \([A, L^*] = 2L^*\),
\textbar{} \((c)\) \([A, R^*] = -2R^*\).

\emph{Pf.}
\((b)\)
\begin{align}
AL^* - L^*A & = \sum_{i=0}^D(AE_{i-1}A^*E_i - E_{i-1}A^*E_iA)\\
& = \sum_{i=0}^D E_{i-1}A^*E_i (\theta_{i-1} - \theta_i)\\
& \quad (\theta_k = D-2k, \quad \theta_{i-1}- \theta_i = 2I - 2(i-1) = 2\\
& = 2L^*.
\end{align}

\((c)\) Similar.

\begin{remark}
\begin{align}
AR^* - R^*A & = \sum_{i=0}^D (AE_{i+1}A^*E_i - E_{i+1}A^*E_iA)\\
& = \sum_{i=0}^D E_{i+1}A^*E_i (\theta_{i+1} - \theta_i)\\
& = 2R^*.
\end{align}
\end{remark}

\((a)\) We have, by \((b)\), \((c)\)
\begin{equation}
[A, A^*] = [A, L^*] + [A, R^*] = 2(L^* - R^*).
\end{equation}
Since \(A^* = L^* + R^*\),
\[R^* = \frac{2A^* + [A^*, A]}{4}, \quad L^* = \frac{2A^* - [A^* - A]}{4}.\]
Now \((a)\) is seen to be equivalent to (\eqref{eq:6-2}) upon evaluation.
This proves Claim 2.

\begin{remark}
\begin{align}
[L^*,R^*] & = \frac{1}{16}((2A^*-[A^*,A])(2A^*+[A^*,A]) - (2A^*+[A^*,A])(2A^*- [A,A^*]))\\
& = \frac{1}{16}(4{A^*}^2 + 2A^*[A^*,A] - [A^*,A]2A^* - [A^*,A]^2 - 4{A^*}^2 + 2A^*[A^*,A] - [A^*,A]2A^* + [A^*,A]^2)\\
& = \frac{1}{4}({A^*}^2A - 2A^*AA^* + A{A^*}^2)\\
& = A,
\end{align}
by (\eqref{eq:6-2}).
\end{remark}

Now apply same argument as for (\eqref{eq:6-1}), (\eqref{eq:6-2}) of Theorem \ref{thm:hd2-modules} and observe \(A^*\) has \(D+1\) distinct eigenvalues. So,
\[A^* = \sum_{i=0}^D(D-2i)E^*_i\]
generates
\[M^* = \mathrm{Span}(E^*_0, \ldots, E^*_D).\]
Hence, \(E_0, \ldots, E_D, \; A^*\) generates \(T\).

Take an irreducible \(T\)-module \(W\) with endpoint \(r\) with \(0\leq r \leq D/2\). Set
\(t = \min\{i\mid E_iW\}\).

Pick \(0\neq w_0^*\in E_tW\). Set
\[w_i^* = \frac{1}{i!}{R^*}^i w_0^* \in E_{t+i}W \quad \text{for all }i.\]
Then,
\[R^*w_i^* = (i+1)w_{i+1}^* \quad \text{for all }i.\]
By \((a)\), we get by induction, \(L^*w_i^* = (D-2t-i+1)w^*_{i-1}\),
\begin{align}
L^*w_i^* & = \frac{1}{i}L^*R^*w_{i-1}^* \\
& = \frac{1}{i}(A + R^*L^*)w_{i-1}^* \\
& = \frac{1}{i}((D-2(t+i-1))w^*_{i-1} + (i-1)(D-2t-i+2)w_{i-1}^*)\\
& = (D-2t - i + 1)w_{i-1}^*.
\end{align}

So \(\mathrm{Span}(w_0^*, w_1^*, \ldots )\) is \(L^*\), \(R^*\), \(A^*\)-invariant. Hence,
\(W = \mathrm(Span)(w_0^*, w_1^*, \ldots, w_d^*)\), \(w_0^*, w_1^*, \ldots, w_d^* \neq 0\), \(w^*_i = 0\) for every \(i>d\) by dimension.

Thus \(d = D-2t\).

\emph{Pf.}
\begin{align}
(D -2(t+d))w^*_d & = Aw_d^* \\
& = (L^*R^* - R^*L^*)w_d^*\\
& = -(D-2t - d + 1)R^*w_{d-1}^*\\
& = -(D-2t - d +1)dw^*_d.
\end{align}
Hence,
\[0 = d^2 + (2t - D - 1 + 2)d - (D-2t) = (d-D+2t)(d+1)\]
So \(d = D-2t\).
\end{proof}

\begin{definition}
\protect\hypertarget{def:thin-dualthin}{}\label{def:thin-dualthin}

For any graph \(\Gamma = (X, E)\), pick a vertex \(x\in X\) and set \(E^*_i \equiv E^*_i(x)\) and \(T \equiv T(x)\).

\((i)\) an irreducible \(T\)-module \(W\) is thin \index{thin} if \(\dim E_i^*W \leq 1\) for every \(i\),

\((ii)\) \(\Gamma\) is thin with respet to \(x\), if every irreducible \(T(x)\)-module is thin,

\((iii)\) an irreducible \(T\)-module \(W\) is dual thin \index{dual thin} if \(\dim E_iW \leq 1\) for every \(i\),

\((iv)\) \(\Gamma\) is dual thin with respect to \(x\), if every irreducible \(T(x)\)-module is dual thin.

\end{definition}

Observe: \(H(D,2)\) is thin, dual thin with respect to each \(x\in X\).

\begin{definition}
\protect\hypertarget{def:q-polynomial-graph}{}\label{def:q-polynomial-graph}

With above notation, write \(D \equiv D(x)\).

\((i)\) an ordering \(E_0, E_1, \ldots, E_R\) of primitive idempotents of \(\Gamma\) is restricted \index{restricted} if \(E_0\) corresponds to the maximal eigenvalue.

Fix a restricted ordering,

\((ii)\) \(\Gamma\) is \(Q\)-polynomial with respect to \(x\), above ordering if there exists \(A^* \equiv A^*(x)\) such that

\(\quad (a)\) \(E_0^*V, \ldots, E_D^*V\) are the maximal eigenspaces for \(A^*\).

\(\quad (b)\) \(E_iA^*E_j = 0\) if \(|i-j| > 1\) for \(0\leq i,j\leq R\).

\end{definition}

Observe \(H(D,2)\) is \(Q\)-polynomial with respect to the natural ordering of the idempotents and every vetex.

\textbf{Program.} Study graphs that are thin and \(Q\)-polynomial with respect to each vertex.

(In fact, thin with respect to \(x\) implies dual thin with respect to \(x\).)

Get a situation like \(H(D,2)\), where \(T\) is generated by \(A\), \(A^*\). Except \(\mathrm{sl}_s(\mathbb{C})\) is repalaced by a quantum Lie algebra.

\hypertarget{lec7}{%
\chapter{\texorpdfstring{The Johnson Graph \(J(D,N)\)}{The Johnson Graph J(D,N)}}\label{lec7}}

\textbf{Wednesday, February 3, 1993}

\begin{definition}
The Johnson graph, \(\Gamma = J(D,N)\) \((1\leq D\leq N-1)\) satisfies
\begin{align}
X & = \{S\mid S\subset \Omega, \; |S| = D\} \quad\text{where }\; \Omega = \{1, 2, \ldots, N\}\\
E & = \{ST\mid S, T\in X, \quad |S\cap T| = D-1\}.
\end{align}
\end{definition}

\begin{example}

\(J(2,4)\)

\begin{center}\includegraphics{t-algebra_files/figure-latex/unnamed-chunk-6-1} \end{center}

\end{example}

\textbf{Note 1.}
The symmetric group \(S_N\) acts on \(\Omega\). \(S_N \subseteq \mathrm{Aut}(\Gamma)\) acts vertex transitively on \(\Gamma\).

\textbf{Note 2.}
\(\Gamma = J(D,N)\) is isomorphic to \(\Gamma' = J(N-D,N)\).
\begin{align}
\Gamma = (X, E) & & \Gamma' = (X', E')\\
X\ni S & \qquad \longrightarrow & \bar{S} = \Omega\setminus S \in X'
\end{align}
This correspondence induces an isomorphism of graphs.

\emph{Pf.}
\begin{align}
ST\in E & \Leftrightarrow  |S\cap T| = D-1\\
  & \Leftrightarrow  |\Omega - (S\cup T)| = N-D-1\\
  & \Leftrightarrow  |\bar{S} \cap \bar{T}| = N-D-1\\
  & \Leftrightarrow  \bar{S}\bar{T} \in E'
\end{align}

Hence, without loss of generality, assume
\[D\leq N/2 \quad \text{for } J(D,N).\]

We sill need the eigenvalues of \(J(D,N)\) for certain problem later in the course. We can get these eigenvalues from our study of \(H(D,2)\).

\begin{lemma}
The eigenvalues for \(J(D,N)\) with \(1\leq D \leq N/2\) are give by
\begin{align}
\theta_i & = (N-D-i)(D-i) - i \quad (0\leq i\leq D)\\
m_i & = \binom{D}{i} - \binom{N}{i-1}.
\end{align}
\end{lemma}

\begin{proof}
Let
\begin{align}
\Gamma_J & \equiv J(D,N) = (X_J, E_J)\\
\Gamma_H & \equiv H(N,2) = (X_H, E_H).
\end{align}
Set \(x \equiv 11\cdots 1 \in X_H\).

Define \(\tilde{\Gamma} \equiv (\tilde{X}, \tilde{E})\), where
\begin{align}
\tilde{X} & = \{y\in X_H \mid \partial_H(x,y) = D\} \quad \partial_H:\text{distance in }\Gamma_H\\
\tilde{E} & = \{yz\in X_H \mid \partial_H(y,z) = 2\}.
\end{align}
Observe
\begin{align}
X_J & \to & \tilde{X}\\
S & \mapsto & \hat{S},
\end{align}
where
\[\hat{S} = a_1\cdots a_N, \quad a_i = \begin{cases} -1 & \text{if }i\in S\\ 1 & \text{if }i\not\in S \end{cases}\]
induces an isomorphism of graphs \(\Gamma_J \to \tilde{\Gamma}\).

\emph{Pf.}
\begin{align}
ST \in E_J &\Leftrightarrow |S\cap T| = D-1\\
& \Leftrightarrow \partial_H(\hat{S}, \hat{T}) = 2\\
& \Leftrightarrow (\hat{S}, \hat{T})\in \tilde{E}.
\end{align}

Identify, \(\Gamma_J\) with \(\tilde{\Gamma}\). Then the standard module \(V_J\) of \(\Gamma_J\) becomes \(\tilde{V} = E^*_DV_H\), where \(V_H\) is the standard module of \(\Gamma_H\), and \(E^*_D \equiv E^*_D(x)\).

Let \(R\) be the raising matrix with respect to \(x\) in \(\Gamma_H\), and

let \(L\) be the lowering matrix with respect to \(x\) in \(\Gamma_H\).

Recall
\[(RL - DE^*_D) |_{\tilde{V}}\]
is the adjacency map in \(\tilde{\Gamma}\).

To find eigenvalues of \(\tilde{A}\), pick any irreducible \(T(x)\)-module \(W\) with the endpoint \(r\leq D\). Then by Theorem \ref{thm:hd2-modules}
\[\text{diam}(W) = N-2r+1.\]
Let \(w_0, w_1, \ldots, w_{N-2r}\) denote a basis for \(W\) as in Theorem \ref{thm:hd2-modules}. Then,
\[w_{D-r} \in E^*_DW \subseteq \tilde{V}.\]

Observe:
\begin{align}
\tilde{A}w_{D-r} & = RLw_{D-r} - DE_D^*w_{D-r}\\
& = R(N-2r-D+r+1)w_{D-r-1} - Dw_{D-r}\\
& = ((N-D-r+1)(D-r) - D)w_{D-r}.
\end{align}
Note that this is valid for \(D = r\) as well.

Hence,
\[\tilde{A}w_{D-r}  = ((N-D-r)(D-r)-r)w_{D-r}.\]
Let
\[V_H = \sum W \quad (\text{direct sum of irreducible }T(x)-\text{modules}.)\]
Then,
\begin{align}
V_J & = E_D^*V_H\\
& = \sum_{W:r(W)\leq D} E_D^*W\\
& = \text{a direct sum of 1 dimensional eigenspaces for }\tilde{A}.
\end{align}
The eigenspace for eigenvalue
\[(N-D-r)(D-r)-r \quad (\text{monotonously decreasing with respec to }r)\]
appears with multiplicity
\[\binom{N}{r} - \binom{N}{r-1}\]
in this sum by Theorem \ref{thm:hd2-modules} \((iv)\).
\end{proof}

\begin{theorem}
\protect\hypertarget{thm:thin-condition}{}\label{thm:thin-condition}Let \(\Gamma = (X, E)\) be any graph. For a fixed vertex \(x\in X\), let
\[E_i^*\equiv E_i^*(x), \quad T\equiv T(x), \quad D \equiv D(x), \text{ and } K = \mathbb{C}.\]
Then we have the following implications of conditions:
\[\text{TH} \Leftrightarrow \text{C} \Leftarrow \text{S} \Leftarrow \text{G}.\]
where

(TH) \(\Gamma\) is thinn with respect to \(x\).

(C) \(E^*_iTE^*_i\) is commutative for every \(i\), \((0\leq i \leq D)\).

(S) \(E^*_iTE^*_i\) is symmetric for every \(i\), \((0\leq i \leq D)\).

(G) For every \(y, z\in X\) with \(\partial(x,y) = \partial(x,z)\), there exists \(g\in \mathrm{Aut}(\Gamma)\) such that

\[gx = x, \; gy = z, \; gz = y.\]
\end{theorem}

\begin{proof}
\leavevmode

(TH) \(\Rightarrow\) (C)

Fix \(i\) with \(0\leq i\leq D\). Let
\[V = \sum W. \; \text{The standard module written as a direct sum of irreducible $T$-modules}.\]
The,
\[E_i^*V = \sum E_i^*W. \; \text{The direct sum of 1-dimensional $E_i^*TE_i^*$-modules}.\]
Since \(\dim E_i^*W = 1\), for \(a, b\in E_i^*TE_i^*\), \({ab - ba}_{| E_i^*W} = 0\). Hence \(ab - ba = 0\).

(C) \(\Rightarrow\) (TH)

Suppose \(\dim E_i^*W \geq 2\) for some irreducible \(T\)-module \(W\) with some \(i\) with \(1\leq i\leq D\).

Claim: \(E_i^*W\) is an irreducible \(E_i^*TE_i^*\)-module.

\emph{Pf.}
Suppose
\[0 \subsetneq U \subsetneq E_i^*W,\]
where \(U\) is a \(E_i^*TE_i^*\)-module. Then by the irreducibility,
\[TU = W.\]
So
\[U \supseteq E_i^*TE_i^*U = E_i^*TU = E^*_iW.\]
This is a contradiction.

Claim 2: Each irreducible \(S = E_i^*TE_i^*\)-module \(U\) has dimension \(1\). In particular, \(\Gamma\) is thin with respect to \(x\).

\emph{Pf.}
Pick
\[0\neq a \in E_i^*TE_i^*.\]
Since \(\mathbb{C}\) is algebraicallt closed, \(a\) has an eigenvector \(w\in U\) with eigenvalue \(\theta\). Then,
\begin{align}
(a- \theta I)U & = (a-\theta I)Sw\\
& = S(a-\theta I)w\\
& = 0.
\end{align}
Hence,
\[a_{|U} = \theta I_{|U} \quad \text{for all }\; a\in S.\]
Thus each \(1\) dimensional subspace of \(U\) is an \(S\)-module. We have
\[\dim U = 1.\]
By Claim 1 and Claim 2, we hat (TH).

\end{proof}

\hypertarget{lec8}{%
\chapter{Thin Graphs}\label{lec8}}

\textbf{Friday, February 5, 1993}

\begin{proof}[Proof of Theorem \ref{thm:thin-condition} continued]
\leavevmode

(S) \(\Rightarrow\) (C)

Fix \(i\) and pick \(a, b\in E_i^*TE_i^*\).

Since \(a\), \(b\) and \(ab\) are symmetric,
\[ab = (ab)^\top = b^\top a^\top = ba.\]
Hence \(E_i^*TE_i^*\) is commutative.

(G) \(\Rightarrow\) (S)

Fix \(i\) and pick \(a \in E_i^*TE_i^*\). Pick vertices \(y, z\in X\).

We want to show that
\[a_{yz} = a_{zy}.\]
We may assume that
\[\partial(x, y) = \partial(x,z) = i,\]
othewise
\[a_{yz} = a_{zy} = 0.\]
By our assumption, there exists \(g\in G\) such that
\[g(y) = z, \quad g(z) = y, \quad g(x) = x.\]
Let \(\hat{g}\) denote the permutation matrix representing \(g\), i.e.,
\[\hat{g}\hat{y} =\widehat{g(y)} \quad \text{for all }\ y\in X, \quad \hat{y} = \begin{pmatrix}0\\\vdots \\ 1 \\\vdots \\0\end{pmatrix}\begin{matrix} \\ \\ \leftarrow y \\ \\ \text{ } \end{matrix}.\]
If \(g\in \mathrm{Aut}(\Gamma)\), then
\[\hat{g}A = A\hat{g} \quad \text{Exercise}.\]
Also we have
\[\hat{g}E_j^* = E^*_j\hat{g} \quad (0\leq j\leq D),\]
since
\[\partial(x,y) = \partial(g(x), g(y)) = \partial(x, g(y)).\]
Hence \(\hat{g}\) commutes with each element of \(T\). We have
\begin{align}
a_{yz} & = (\hat{g}^{-1}a\hat{g})_{yz}, \quad (\hat{g})_{yz} = \begin{cases} 1 & g(z) = y\\ 0 & \text{else.}\end{cases}\\
& = \sum_{y', z'}(\hat{g}^{-1})_{yy'}a_{y'z'}\hat{g}_{z'z}\\
& \quad (\text{zero except for $g^{-1}(y') = y, \; g(z) = z'$}.)\\
& = a_{g(y)g(z)}\\
& a_{zy}.
\end{align}
This proves Theorem \ref{thm:thin-condition}.

\end{proof}

\textbf{Open Problem:}
Find all the graphs that satisfy the condition (G) for every vertex \(x\).

\(H(N, 2)\) is one example, because
\[\mathrm{Aut}\Gamma_{1\cdots 1} \simeq S_\Omega, \quad x = (1\cdots 1), \Gamma_i(x)
 = \{\hat{S} \mid |S| = i\}.\]

Property (G) is clearly related to the distance-transitive property.

\begin{definition}
Let \(\Gamma = (X, E)\) be any graph. \(\Gamma\) with \(G\subseteq \mathrm{Aut}(\Gamma)\) is said to be distance-transitive \index{distance-transitive} (or two-point homogeneous), whenever
\[\text{for all } x, x', y, y'\in X \; \text{ with } \partial(x,y) = \partial(x',y'),\]
there exists \(g\in G\) such that
\[g(x) = y,\quad g(x') = y'.\]
(This means \(G\) is as close to being doubly transitive as possible.)
\end{definition}

\begin{lemma}
\protect\hypertarget{lem:property-g}{}\label{lem:property-g}

Suppose a graph \(\Gamma = (X, E)\) satisfies the property \textrm{(G) = (G($x$))} for every \(x\in X\). Then,

\((i)\) either\\
\(\quad (ia)\) \(\Gamma\) is vertex transitive; or\\
\(\quad (iia)\) \(\Gamma\) is bipartite \((X = X^+ \cup X^-)\) with \(X^+\), \(X^-\) each an orbit of \(\mathrm{Aut}(\Gamma)\).

\((ii)\) if \((ia)\) holds, then \(\Gamma\) is distance-transitive.

\end{lemma}

\begin{proof}
\((i)\)
Claim. Suppose \(y, z\in X\) are conneced by a path of even length. Then \(y, z\) are in the same orbit of \(\mathrm{Aut}(\Gamma)\).

\emph{Pf.}
It suffices to assume that the path has lenght \(2\), \(y \sim w\sim z\).

Now \(\partial(y,w) = \partial(w,z) = 1\). So there exits \(g\in \mathrm{Aut}(\Gamma)\) such that
\$\(gw = w, \quad gy = z, \quad gz = y.\)
This proves Claim.

Fix \(x\in X\). Now suppose that \(\Gamma\) is not vertex transitive, and we shall show \((ib)\).

Observe that \(X = X^+ \cup X^-\), where
\begin{align}
X^+ & = \{y\in X\mid \text{there exists a path of even length connecting $x$ and $y$}\}\\
X^- & = \{y\in X\mid \text{there exists a path of odd length connecting $x$ and $y$}\}
\end{align}
Asi \(X^+\) is contained in an orbit \(O^+\) of \(\mathrm{Aut}(\Gamma)\), and \(X^-\) is contained in an orbit \(O^-\) of \(\mathrm{Aut}(\Gamma)\).

Now \(O^+\cap O^- = \emptyset\) (else \(O^+ = O^- = X\) and vertex transitive).
So,
\(X = O^+\), and \(X^- = O^-\).

Also \(X^+ \cup X^- = X\) is a bipartition by construction.

\((ii)\) Fix \(x, y, x', y'\) with \(\partial(x,y) = \partial(x',y')\).

By vertex transitivity, there exists an element
\[g_1\in G \text{ such that } g_1x = x'.\]
Observe that
\[\partial(x', y') = \partial(x,y) = \partial(g_1x, g_1y) = \partial(x', g_1y).\]
Hence, there exisits an element
\[g_2\in G \text{ such that } g_1x' = x', g_2y' = g_1y', g_2g_1y = y'\]
by (G(\(x'\))) property.

Set \(g = g_2g_1\). Then
\[gx = x', gy = y'\]
by construction.
\end{proof}

The following graphs \(\Gamma = (X, E)\) are vertex transitive, and satisfy the property (G(\(x\))) for all \(x\in X\).
\[J(D, N), \quad H(D, r), \quad J_q(D,N),\]
where

\(H(D,r)\):

\begin{align}
X & = \{a_1\cdots a_D\mid a_i\in F, 1\leq i\leq D\}\\
& \quad F: \text{ any set of cardinality $r$}\\
E & = \{xy\mid y, x\in X, \; \text{$x$ and $y$ differ in exactly one coordiate}\}.
\end{align}

\(J_q(D, N)\):

\begin{align}
X & = \text{the set of all $D$-dimensional subspaces of $N$-dimensional vector space over $GF(q)$.}\\
& \quad F: \text{ any set of cardinality $r$}\\
E & = \{xy\mid y, x\in X, \; \dim (x\cap y) = D-1\}.
\end{align}

The following graph is distance-transitive but does not satisify (G(\(x\))) for any \(x\in G\).

\(H_q(D,N)\):

\begin{align}
X & = \text{the set of all $D\times N$ matrices with entries in $GF(q)$}.\\
E & = \{xy\mid y, x\in X, \; \mathrm{rank}(x-y) = 1.\}.
\end{align}

\begin{remark}
\leavevmode

\(H(D,r)\): \(G = S_r \mathrm{wr} S_D\), \(G_x = S_{r-1} \mathrm{wr} S_D\),

For \(x, y\in X\) with \(\partial(x, y) = \partial(x,z) = i\),
\begin{align}
Y = \{j\in \Omega \mid x_j\neq y_j\} & \leftrightarrow Z = \{j\in \Omega \mid x_j\neq z_j\}\\
(y_{j_1}, \ldots, y_{j_i}) & \leftrightarrow (z_{\ell_1}, \ldots, z_{\ell_i})
\end{align}

\(J(D, N)\): \(G = S_N\), \(G_x = S_D \times S_{N-D}\).

\begin{align}
X\cap Y & \leftrightarrow X \cap Z\\
(\Omega \setminus X)\cap Y & \leftrightarrow (\Omega \setminus X)\cap Z.
\end{align}

The following graph is distance-transitive but does not satisify (G(\(x\))) for any \(x\in G\).

\(J_q(D,N)\):

\[X\cap Y  \leftrightarrow X \cap Z.\]

\end{remark}

The theory of single thin irreducible \(T\)-module.

Let \(\Gamma = (X, E)\) be any graph.
\begin{align}
M & = \text{Bose-Mesner algebra over $K/\mathbb{C}$ generated by the adjacency matrix $A$.}\\
& = \mathrm{Span}(E_0, \ldots, E_R).
\end{align}

\(M\) acts on the standard module \(V = \mathbb{C}^{|X|}\).

Fix \(x\in X\), let
\(D \equiv D(x)\) be the \(x\)-diameter, and \(k = k(x)\) be the valency of \(x\).

\hypertarget{lec9}{%
\chapter{\texorpdfstring{Thin \(T\)-Module, I}{Thin T-Module, I}}\label{lec9}}

\textbf{Monday, February 8, 1993}

Let \(\Gamma = (X, E)\) be any graph.

\(M\): Bose-Mesner algebra over \(K/\mathbb{C}\) generated by the adjacency matrix \(A\).

\(\quad M = \mathrm{Span}(E_0, \ldots, E_R)\).

\(M\) acts on the standard module \(V = \mathbb{C}^{|X|}\).

Fix \(x\in X\), let
\(D \equiv D(x)\) be the \(x\)-diameter, and \(k = k(x)\) be the valency of \(x\).

\begin{definition}
Pick \(x\in X\) and write \(E_i^* \equiv E_i^*(x)\) and \(T \equiv T(x)\).

Let \(W\) be an irreducible thin \(T\)-module with endpoint \(r\), diameter \(d\).

Let \(a_i = a_i(W)\in \mathbb{C}\) satisfying
\[E_{r+i}^*A{E^*_{r+i}|}_{E_{r+i}^*W} = a_i1|_{E_{r+i}^*} \quad (0\leq i\leq d).\]
Let \(x_i = x_i(W)\in \mathbb{C}\) satisfying
\[E_{r+i-1}^*A{E^*_{r+i}AE^*_{r+i-1}|}_{E_{r+i}^*W} = x_i1|_{E_{r+i}^*} \quad (0\leq i\leq d).\]
\end{definition}

\begin{lemma}
\protect\hypertarget{lem:thin-module-structure}{}\label{lem:thin-module-structure}With above notation, the following hold.

\((i)\) \(a_i\in \mathbb{R} \quad (0\leq i\leq d)\).

\((ii)\) \(x_i\in \mathbb{R}^{>0} \quad (0\leq i\leq d)\).

\((iii)\) Pick \(0\neq w_0\in E^*_rW\). Set \(w_i = E^*_{r+i}A^iw_0\) for all \(i\). Then

\(\quad (iiia)\) \(w_0, w_1, \ldots, w_d\) is a basis for \(W\), \(w_{-1} = w_{d+1} = 0\).\\
\(\quad (iiib)\) \(Aw_i = w_{i+1} + a_iw_{i} + x_iw_{i-1} \quad (0\leq i\leq d)\).

\((iv)\) Define \(p_0, p_1, \ldots, p_{d+1}\in \mathbb{R}[\lambda]\) by

\[p_0 = 1, \quad \lambda p_i = p_{i+1} + a_i p_i + x_i p_{i-1} \quad (0\leq i\leq d),\quad p_{-1} = 0.\]

\(\quad (iva)\) \(p_i(A)w_0 = w_i, \quad (0\leq i\leq d+1)\).\\
\(\quad (ivb)\) \(p_{d+1}\) is the minimal polynomial of \(A|_W\).
\end{lemma}

\begin{proof}
\((i)\) \(a_i\) is an eigenvalue of a real symmetric matrix \(E_{r+i}^*AE^*_{r+i}\).

\((ii)\) \(x_i\) is an eigenvalue of a real symmetrix matrix \(B^\top B\), where
\[B = E^*_{r+i}AE^*_{r+i-1}.\]
Hence, \(x_i\in \mathbb{R}\).

Since \(B^\top B\) is positive semidefinite,
\[x_i \geq 0.\]
\emph{Pf.} If \(B^\top Bv = \sigma v\) for some \(\sigma \in \mathbb{R}\), \(v\in \mathbb{R}^m \setminus \{0\}\), then
\[0\leq \|Bv\|^2 = v^\top B^\top Bv = \sigma v^\top v = \sigma \|v\|^2, \quad \|v\|^2 >0.\]
Hence, \(\sigma \geq 0\).

Moreover, \(x_i\neq 0\) by Lemma \ref{lem:irreducible} \((iv)\).

\((iiia)\) Observe
\[w_i = E^*_{r+i}AE^*_{r+i-1}w_{i-1} \quad (1\leq i \leq d).\]
So \(w_i \neq 0 \quad (1\leq i \leq d)\) by Lemma \ref{lem:irreducible} \((iv)\).

Hence,
\[W = \mathrm{Span}(w_0, \ldots, w_d)\]
by Lemma \ref{lem:irreducible}. \((iii)\).

\((iiib)\) We have that
\begin{align}
Aw_i & = E^*_{r+i+1}Aw_i + E_{r+i}^*Aw_i + E^*_{r+i-1}Aw_i\\
& = w_{i+1} + E^*_{r+i}AE^*_{r+i}w_i + E^*_{r+i-1}AE^*_{r+i}AE^*_{r+i-1}w_{i-1}\\
& = w_{i+1} + a_iw_{i} + x_iw_{i-1}
\end{align}

\((iva)\) Clear for \(i=0\). Assume it is valid for \(0, \ldots, i\).
\[p_{i+1}(A)w_0 = (A-a_iI)w_i - x_iw_{i-1} = w_{i+1}.\]

\((ivb)\) By definition,
\[p_{d+1}(A)w_0 = 0.\]
Moreover, \(p_{d+1}(A)W = 0\). For every \(w\in W\), write
\begin{align}
w & = \sum_{i=0}^d \alpha_i w_i \\
& = \sum_{i=0}^d \alpha_i p_i(A)w_0 && \text{for some }\alpha_i\in\mathbb{C}\\
& = p(A)w_0 && \text{for some }p\in \mathbb{C}[\lambda]
\end{align}
Hence,
\begin{align}
p_{d+1}(A)w & = p_{d+1}(A)p(A)w_0\\
& = p(A)p_{d+1}(A)w_0\\
& = 0.
\end{align}

Note that \(p_{d+1}\) is the minimal polynomial.

\emph{Pf.}
Suppose \(q(A)W = 0\) for some \(0\neq q\in \mathbb{C}[\lambda]\) with \(\deg q < \deg p_{d+1} = d+1\).
Then,
\[q = \sum_{i=0}^d\beta_ip_i \quad \text{for some }\beta_i\in \mathbb{C}.\]
We have,
\[0  = q(A)w_0  = \sum_{i=0}^d \beta_iw_i.\]
Hence \(\beta_0 = \cdots = \beta_d = 0\) by \((iiia)\). Thus \(q = 0\) and a contradiction.
\end{proof}

\begin{corollary}
\protect\hypertarget{cor:thin-is-dualthin}{}\label{cor:thin-is-dualthin}

Let \(\Gamma\), \(W\), \(r\), \(d\) be as above. Then

\((i)\) \(W\) is dual thin, that is,

\[\dim E_iW \leq 1 \quad (1\leq i \leq d).\]

\((ii)\) \(d = |\{i \mid E_iW \neq 0\}| - 1.\)

\end{corollary}

\begin{proof}
\((i)\) Set as in Lemma \ref{lem:thin-module-structure},
\[w_i = p_i(A)w_0\in E^*_{r+i}W.\]
Then \(w_0, w_1, \ldots, w_d\) is a basis for \(W\). We have
\[W = Mw_0.\]
So,
\[E_iW = E_iMw_0 = \mathrm{Span}(E_iw_0).\]
Thus,
\[\dim E_iW = \begin{cases}1 & \text{if } E_iw_0\neq 0\\ 0 & \text{if }E_iw_0 = 0.\end{cases}\]
In particular,
\[\dim E^*_iW \leq 1.\]

\((ii)\) Immediate as
\[\dim W = d+1.\]
This proves the lemma.
\end{proof}

\begin{lemma}
\protect\hypertarget{lem:measure-wi}{}\label{lem:measure-wi}Given an irreducible \(T(x)\)-module \(W\) with endpoint \(r = r(W)\), diameter \(d = d(W)\). Write
\[x_i = x_i(W) \; (0\leq i\leq d), \quad w_i = p_i(A)w_0\in E^*_{r+i}W \; (0\leq i\leq d), \quad 0\neq w_0 \in E^*_rW.\]
Then,
\[\frac{\|w_i\|^2}{\|w_0\|^2} = x_1x_2\cdots x_i \quad (1\leq i\leq d).\]
\end{lemma}

\begin{proof}
It suffices to show that
\[\|w_i\|^2 = x_i\|w_i\|^2 \quad (1\leq i\leq d).\]
Recall by Lemma \ref{lem:thin-module-structure} \((iiib)\) that
\[Aw_j = w_{j+1} + a_jw_j + x_jw_{j-1} \quad (0\leq j\leq d), \quad w_{-1} = w_{d+1} = 0.\]
Now observe,
\begin{align}
\langle w_{i-1}, Aw_i\rangle & = \langle w_{i-1}, w_{i+1}+ a_iw_i + x_iw_{i-1}\rangle\\
& = \overline{x_i}\|w_{i-1}\|^2\\
& = x_i\|w_{i-1}\|^2.
\end{align}
by Lemma \ref{lem:thin-module-structure} \((ii)\).
Also,
\begin{align}
\langle w_{i-1}, Aw_i\rangle & = \langle Aw_{i-1}, w_i\rangle \quad (text{since}\; \bar{A}^\top = A)\\
& = \langle x_i + a_{i-1}w_{i-1} + x_{i-1}x_{i-2}, w_i\rangle\\
& = \|w_i\|^w.
\end{align}
This proves the lemma.
\end{proof}

\begin{definition}
\protect\hypertarget{def:measure}{}\label{def:measure}Let \(W\) be an irreducible thin \(T(x)\) module with endpoint \(r\), \(E^*_i \equiv E_i^*(x)\).

The measure \index{measure} \(m = m_W\) is the function
\[m: \mathbb{R} \to \mathbb{R}\]
such that
\[
m(\theta) = \begin{cases}\frac{\|E_iw\|^2}{\|w\|^2} & \text{where } 0\neq w \in E^*_rW\\
& \text{ if $\theta = \theta_i$ is an eigenvalue for $\Gamma$}\\
0 & \text{if $\theta$ is not an eigenvalue for $\Gamma$.}
\end{cases}\]
\end{definition}

\hypertarget{lec10}{%
\chapter{\texorpdfstring{Thin \(T\)-Module, II}{Thin T-Module, II}}\label{lec10}}

\textbf{Wednesday, February 10, 1993}

Let \(\Gamma = (X, E)\) be any graph.

Fix a vertex \(x\in X\). Let \(E^*_i \equiv E^*_i(x)\), \(T\equiv T(x)\), the subconstituent algebra over \(\mathbb{C}\), and \(V = \mathbb{C}^{|X|}\) the standard module.

\begin{lemma}
\protect\hypertarget{lem:orthogonality}{}\label{lem:orthogonality}

With above notation, let \(W\) denote a thin irreducible \(T(x)\)-module with endpoint \(r\) and diameter \(d\). Let
\begin{align}
a_i & = a_i(W) \quad (0\leq i \leq d)\\
x_i & = x_i(W) \quad (1\leq i \leq d)\\
p_i & = p_i(W) \quad (0\leq i \leq d+1)
\end{align}
be from Lemma \ref{lem:thin-module-structure}, and measure \(m = m_W\).
Then,

\((i)\) \(p_0, \ldots, p_{d+1}\) are orthogonal with respect to \(m\), i.e.,

\[\sum_{\theta\in \mathbb{R}}p_i(\theta)p_j(\theta)m(\theta) = \delta_{ij}x_1x_2\cdots x_i \quad (0\leq i,j\leq d+1) \text{ with }\; x_{d+1}=0.\]

\(\quad (ia)\) \({\displaystyle \sum_{\theta\in \mathbb{R}}p_i(\theta)^2m(\theta) = x_1\cdots x_i \quad (0\leq i\leq d)}\).

\(\quad (iia)\) \({\displaystyle \sum_{\theta\in \mathbb{R}}m(\theta) = 1}\).

\(\quad (iiia)\) \({\displaystyle \sum_{\theta\in \mathbb{R}}p_i(\theta)^2\theta m(\theta) = x_1\cdots x_ia_i \quad (0\leq i\leq d)}\).

\end{lemma}

\begin{proof}
Pick \(0\neq w_0\in E^*_rW\). Set
\[w_i = p_i(A)w_0 \in E^*_{r+i}W.\]
Since \(E^*_iW\) and \(E^*_jW\) are orthogonal if \(i\neq j\),

\begin{align}
\delta_{ij}\|w_i\|^2 & = \langle w_i, w_j\rangle\\
& = \langle p_i(A)w_0, p_j(A)w_0\rangle\\
& = \left\langle p_i(A)\left(\sum_{\ell=0}^R E_\ell\right)w_0, p_j(A)\left(\sum_{\ell=0}^R E_\ell\right)w_0\right\rangle\\
& = \left\langle \sum_{\ell=0}^R p_i(\theta_\ell)E_\ell w_0, \sum_{\ell=0}^R p_j(\theta_\ell)E_\ell w_0\right\rangle && (\text{as } AE_j = \theta_jE_j)\\
& = \sum_{\ell=0}^R p_i(\theta_\ell)\overline{p_j(\theta_\ell)}\|E_\ell w_0\|^2\\
& \qquad\qquad (\text{as } \; p_j\in \mathbb{R}[\lambda], \quad
\theta_\ell\in \mathbb{R}, \quad m(\theta_i)\|w_0\|^2 = \|E_iw_0\|^2)\\
& = \sum_{\theta\in \mathbb{R}} p_i(\theta)p_j(\theta)m(\theta)\|w_0\|^2.
\end{align}
Now we are done by Lemma \ref{lem:measure-wi} as
\[\|w_i\|^2 = \|w_0\|^2 x_1x_2\ldots x_i.\]
For \((ia)\), set \(i = j\), and for \((ib)\), set \(i=j=0\).

\((ii)\) We have
\begin{align}
\langle w_i,Aw_i\rangle & = \langle w_i, w_{i+1} + a_iw_i + x_i w_{i-1}\rangle\\
& = \overline{a_i}\|w_i\|^2\\
& = a_i x_1\cdots x_i\|w_0\|^2, 
\end{align}
as \(a_i\in \mathbb{R}\) by Lemma \ref{lem:thin-module-structure}.

Also,
\begin{align}
\langle w_i, Aw_i\rangle & = \langle p_i(A)w_0, Ap_i(A)w_0\rangle \\
& = \left\langle p_i(A)\left(\sum_{\ell=0}^R E_\ell\right)w_0, A p_i(A)\left(\sum_{\ell=0}^R E_\ell\right)w_0\right\rangle && (\text{ as in $(i)$})\\
& = \sum_{\ell = 0}^D p_i(\theta_\ell)^2 \theta_\ell \|E_\ell w_0\|^2\\
& = \sum_{\theta\in \mathbb{R}}p_i(\theta)^2\theta m(\theta)\|w_0\|^2.
\end{align}
Thus, we have \((ii)\).
\end{proof}

\begin{lemma}
\protect\hypertarget{lem:determined-by-m}{}\label{lem:determined-by-m}With above notation, let
\(W\) be a thin irreducible \(T(x)\)-module with measure \(m\). Then \(m\) determines diameter \(d(W)\),
\begin{align}
a_i & = a_i(W) \quad (0\leq i\leq d)\\
x_i & = x_i(W) \quad (1\leq i\leq d)\\
p_i & = p_i(W) \quad (0\leq i\leq d+1).
\end{align}
\end{lemma}

\begin{proof}
Note that \(d+1\) is the number of \(\theta\in \mathbb{R}\) such that \(m(\theta)\neq 0\).
Hence \(m\) determines \(d\).

Apply \((ia)\), \((ii)\) of Lemma \ref{lem:orthogonality}.
\begin{align}
& \sum_{\theta\in\mathbb{R}}m(\theta) = 1 && p_0 =1.\\
& \sum_{\theta\in\mathbb{R}}\theta m(\theta) = a_0 && p_1 = \lambda - a_0\\
& \sum_{\theta\in\mathbb{R}}p_1(\theta)^2 m(\theta) = x_1 \\
& \sum_{\theta\in\mathbb{R}}p_1(\theta)^2\theta m(\theta) = x_1a && \to a_1\\
& \qquad p_2 = (\lambda - a_1)p_1 - x_1p_0\\
& \sum_{\theta\in\mathbb{R}}p_2(\theta)^2 m(\theta) = x_1x_2 && \to x_2\\
& \sum_{\theta\in\mathbb{R}}p_2(\theta)^2\theta m(\theta) = x_1x_2a_2 && \to a_2\\
& \qquad p_3 = (\lambda-a_2)p_2 - x_2p_1\\
& \qquad\qquad \vdots\\
& \sum_{\theta\in\mathbb{R}}p_d(\theta)^2 m(\theta) = x_1x_2\cdots x_d && \to x_d\\
& \sum_{\theta\in\mathbb{R}}p_d(\theta)^2\theta m(\theta) = x_1x_2\cdots x_da_d && \to a_d\\
& \qquad p_{d+1} = (\lambda-a_d)p_d - x_dp_{d-1}.\\
\end{align}
This proves the assertions.
\end{proof}

\begin{corollary}
\protect\hypertarget{cor:isomorphic}{}\label{cor:isomorphic}

With above notation, let
\(W\), \(W'\) denote thin irreducible \(T(x)\)-modules. The following are equivalent.

\((i)\) \(W\), \(W\) are isomorpphic as \(T\)-modoles.

\((ii)\) \(r(W) = r(W')\) and \(m_W = m_{W'}\).

\((iii)\) \(r(W) = r(W')\), \(d(W) = d(W')\), \(a_i(W) = a_i(W')\) amd \(x_i(W) = x_i(W')\) \(\quad (0\leq i\leq d)\).

\end{corollary}

\begin{proof}
\((i)\Rightarrow (iii)\)
Write \(r\equiv r(W)\), \(r' \equiv r(W')\), \(d = d(W)\), \(d' = d(W')\), \(a_i = a_i(W)\), \(a_i' = a_i(W')\), \(x_i = x_i(W)\) and \(x_i' = x_i(W')\).

Let \(\sigma: W\to W'\) denote an isomorphism of \(T\)-modules. (See Definition \ref{def:isomorphic-modules}.)

For every \(i\),
\[\sigma E^*_iW = E^*_i\sigma W = E^*_iW'.\]
So, \(r = r'\) and \(d = d'\).

To show \(a_i = a_i'\), pick \(w\in E^*_{r+i}W \setminus \{0\}\). Then,
\[E^*_{r+i}AE^*_{r+i}\sigma (W) = \sigma(E^*_{r+i}AE^*_{r+i}w) = \sigma(a_iw) = a_i\sigma(w), \]
and \(\sigma w\neq 0\). So,
\begin{align}
a_i & = \text{eigenvalue of $E^*_{r+i}AE^*_{r+i}$ on $E^*_{r+i}W$}\\
& = a_i'
\end{align}
It is similar to show \(x = x'\).

\begin{remark}
Pick \(w\in E^*_{r+i-1}W \setminus \{0\}\)
\[E^*_{r+i-1}AE^*_{r+i}AE^*_{r+i-1}\sigma(W) = \sigma(E^*_{r+i-1}AE^*_{r+i}AE^*_{r+i-1}w) = x_i\sigma(w).\]
Hence, \(x_i\) is the eigenvalue of \(E^*_{r+i-1}AE^*_{r+i}AE^*_{r+i-1}\) on \(E^*_{r+i-1}W = x_i'\).
\end{remark}

\((iii)\Rightarrow (i)\)

Pick \(0\neq w_0\in E^*_rW\), \(0\neq w_0'\in E^*_rW'\). Let \(p_i\) be in Lemma \ref{lem:thin-module-structure}, and set
\begin{align}
w_i & = p_i(A)w_0\in E^*_{r+i}W \quad (0\leq i\leq d) \\
w_i' & = p_i'(A)w_0' \in E^*_{r+i}W \quad (0\leq i\leq d)
\end{align}

Define a linear transformation,
\[\sigma: W \to W' \quad (w_i \mapsto w_i').\]
Since \(\{w_i\}\) and \(\{w_i'\}\) are bases with \(d = d'\), \(\sigma\) is an isomorphism of vector spaces.

We need to show
\[a\sigma = \sigma a \quad (\text{for all }\; a\in T).\]
Take \(a = E^*_j\) for some \(j\) \((0\leq j\leq d(x))\). Then for all \(i\), we have
\[E^*_j \sigma w_i = E^*_jw_i' = \delta_{ij}w_i',\]
\[\sigma E^*_jw_i = \delta_{ij}\sigma(w_i) = \delta_{ij}w_i'.\]
\[E^*_j \sigma w_i = \sigma E^*_jw_i?\]
Take an adjacency matrix \(A\) of \(a\). Then,
\[A\sigma w_i = Aw_i' = w_{i+1}' + a_i'w_i' + x_i'w_{i-1}' = \sigma(w_{i+1} + a_iw_i + x_iw_{i-1}) = \sigma Aw_i.\]

\((ii)\Rightarrow (iii)\) Lemma \ref{lem:determined-by-m}.

\((iii)\Rightarrow (ii)\)
Given \(d\), \(a_i\), \(x_i\), we can compute the polynomial sequence
\[p_0, p_1, \ldots, p_{d+1}\]
for \(W\).

Show \(p_0, p_1, \ldots, p_{d+1}\) determines \(m = m_W\). Set
\[\Delta = \{\theta\in \mathbb{R}\mid p_{d+1}(\theta) = 0\}.\]

Observe: \(|\Delta| = d+1\). See `An Introcuction to Interlacing'.

\(m(\theta) = 0\) if \(\theta\not\in\Delta\quad (\theta\in \mathbb{R})\). So it suffices to find \(m(\theta)\), \(\theta\in \Delta\).

By Lemma \ref{lem:orthogonality} \((i)\),
\[
\begin{cases}
\sum_{\theta\in\Delta} m(\theta)p_0(\theta) & = 1\\
\sum_{\theta\in\Delta} m(\theta)p_1(\theta) & = 0\\
\qquad \vdots & \\
\sum_{\theta\in\Delta} m(\theta)p_d(\theta) & = 0
\end{cases}
\]
\(d+1\) linear equation with \(d+1\) unknowns \(m(\theta)\) (\(\theta\in \Delta\)).

But the coefficient matrix is essentially Vander Monde (since \(\deg p_i = i\)).
Hence the system is nonsingular and there are unique values for \(m(\theta)\) \((\theta\in \Delta)\).
\end{proof}

\begin{remark}
\[\begin{pmatrix}
\theta-a_0 & -1 & \cdots & 0 & 0 \\
-x_1 & \theta - a_1  & \cdots & 0 & 0 \\
\vdots & \ddots  & \ddots & \ddots & \vdots \\
0 & 0 & \cdots & \theta-a_{d-1}  & -1\\
0 & 0 & \cdots & -x_d & \theta - a_d
\end{pmatrix}
\begin{pmatrix}
p_0(\theta)\\
\vdots\\
\vdots\\
\vdots\\
p_d(\theta)
\end{pmatrix} = 0,\]
where \(\theta\) is an eigenvalue of a diagonalizable matrix
\[
L = 
\begin{pmatrix}
a_0 & 1 & \cdots & 0 & 0 \\
x_1 & a_1  & \cdots & 0 & 0 \\
\vdots & \ddots  & \ddots & \ddots & \vdots \\
0 & 0 & \cdots & a_{d-1}  & 1\\
0 & 0 & \cdots & x_d & \theta a_d
\end{pmatrix}
\]
with multiplicity \(\dim (\mathrm{Ker}(\theta I - L) = 1)\).
\end{remark}

\hypertarget{lec11}{%
\chapter{\texorpdfstring{Examples of \(T\)-Module}{Examples of T-Module}}\label{lec11}}

\textbf{Friday, February 12, 1993}

Let \(\Gamma = (X, E)\) be a connected graph.

Let \(\theta_0\) be the maximal eigenvalue of \(\Gamma\), and \(\delta\) its corresponding eigenvector.
\[\delta = \sum_{y\in X}\delta_y \hat{y}.\]
Without loss of generality, we may assume that \(\delta_y\in \mathbb{R}^*\) for all \(y\in X\).

\begin{lemma}
\protect\hypertarget{lem:principal-module}{}\label{lem:principal-module}

Fix a vertex \(x\in X\). Write \(T \equiv T(x)\), \(E^*_i\equiv E^*_i(x)\).

\((i)\) \(T\delta = T\hat{x}\) is an irreducible \(T\)-module.

\((ii)\) Given any irreducible \(T\)-module \(W\), the following are equivalent:\\
\(\quad (iia)\) \(W = T\delta\).\\
\(\quad (iib)\) The diameter \(d(W) = d(x)\).\\
\(\quad (iic)\) The endpoint \(r(W) = 0\).

\end{lemma}

\begin{proof}
\((i)\) Observe: there exists an irreducible \(T\)-module \(W\) that contains \(\delta\).

Let \(V = \sum_{i}W_i\) be a direct sum decomposition of the standard module. Then
\[\mathrm{Span}(\delta) = E_0V = \sum_{i}E_0W_i.\]
So, \(E_0W_i \neq 0\) for some \(i\). Then,
\[\delta \in E_0W_i \subseteq W_i.\]
Observe: \(T\delta\) is an irreducible \(T\)-module.

Since \(\delta\in W\), where \(W\) is a \(T\)-module. As \(T\delta \subseteq W\) and \(W\) is irreducible, \(T\delta = W\).

Observe: \(T\delta = T\hat{x}\).

Since \(\hat{x} = \delta_x^{-1}E^*_0\delta \in T\delta\), \(T\hat{x} \subseteq T\delta\). Since \(T\delta\) is irreducible, \(T\hat{x} = T\delta\).

\((ii)\) \((a)\to (b)\):
\[E^*_i\delta = \sum_{y\in X, \partial(x,y) = i}\delta_y\hat{y} \neq 0, \quad (0\leq i\leq d(x)), \]
because \(\delta_y >0\) for every \(y\in X\).

Hence,
\[E^*_iT\delta \neq 0, \quad (0\leq i\leq d(x)).\]
Thus, \(d(x) = d(W)\).

\((b)\to (c)\): Immediate.

\((c)\to (a)\):
Since \(r(W) = 0\), \(E^*_0W \neq 0\).
Hence, \(\hat{x}\in W\) and \(T\hat{x} \subseteq W\).

By the irreduciblity, we have \(T\hat{x} = W\).
\end{proof}

\begin{lemma}
\protect\hypertarget{lem:baiparite-principal}{}\label{lem:baiparite-principal}Assume \(\Gamma\) is bipartite \((X = X^+ \cup X^-)\) (\(X^+\) and \(X^-\) are nonempty). Then the following are equivalent.

\((i)\) There exist \(\alpha^+\) and \(\alpha^-\in \mathbb{R}\) such that

\[\delta_x = \begin{cases} \alpha^+ & \text{if }\: x\in X^+\\
\alpha^- & \text{if } x\in X^-.
\end{cases}\]
\textbar{} \((ii)\) There exist \(k+\) and \(k^-\in \mathbb{Z}^{>0}\) such that
\[k(x) = \begin{cases} k^+ & \text{if }\: x\in X^+\\
k^- & \text{if } x\in X^-.
\end{cases}\]
In this xase, \(k^+k^- = \theta_0^2\), and \(\Gamma\) is called bi-regular.
\end{lemma}

\begin{proof}
\((i)\to(ii)\)

\begin{center}\includegraphics{t-algebra_files/figure-latex/unnamed-chunk-7-1} \end{center}

\begin{align}
A\delta & = A\left(\alpha^+\sum_{x\in X^+}\hat{x} + \alpha^-\sum_{y\in X^-}\hat{y}\right)\\
& = \alpha^+\sum_{y\in X^-}k(y)\hat{y} + \alpha^-\sum_{x\in X^+}k(x)\hat{x}\\
& = \theta_0\delta.
\end{align}
So,
\[k(x)\alpha^- = \theta_0\alpha^+, \quad k(y)\alpha^+ = \theta_0\alpha^-.\]
As \(\alpha^+\neq =\) and \(\alpha^- \neq 0\),
\begin{align}
k^+ & := k(x) \; \text{ is independent of the choice of $x\in X^+$, and}\\
k^- & := k(y) \; \text{ is independent of the choice of $y\in X^-$.}
\end{align}
Moreover, \(k^+k^- = \theta_0^2\).

\((ii)\to(i)\)
Set
\[\delta' = \sum_{y\in X}\alpha_y\hat{y} \quad \text{where}\; \alpha = \begin{cases} 1/\sqrt{k^-} & \text{if }\; y\in X^+\\ 1/\sqrt{k^+} & \text{if }\; y\in X^-.\end{cases}\]
Then one checks
\begin{align}
A\delta' & = A\left(\frac{1}{\sqrt{k^-}}\sum_{y\in X^+}\hat{y} + \frac{1}{\sqrt{k^+}}\sum_{y\in X^-}\hat{y}\right)\\
& =  \frac{k^-}{\sqrt{k^-}}\sum_{y\in X^-}\hat{y} + \frac{k^+}{\sqrt{k^+}}\sum_{y\in X^+}\hat{y}\\
& = \sqrt{k^+k^-}\delta'
\end{align}
Since \(\delta' >0\), \(\delta'\in \mathrm{Span}(\delta)\), and \(\theta_0 = \sqrt{k^+k^-}\).
\end{proof}

\begin{definition}
\protect\hypertarget{def:distance-regular}{}\label{def:distance-regular}For any graph \(\Gamma = (X, E)\), fix a vertex \(x\in X\). Set \(d = d(x)\).

\(\Gamma\) is distance-regular \index{distance-regular} with respect to \(x\), if for all \(i\) : (0\leq i\leq d), and all \(y\in X\) such that \(\partial(x,y) = i\):
\begin{align}
c_i(x) & := |\{z\in X \mid \partial(x,z) = i-1, \; \partial(y,z) = 1\}|\\
a_i(x) & := |\{z\in X \mid \partial(x,z) = i, \; \partial(y,z) = 1\}|\\
b_i(x) & := |\{z\in X \mid \partial(x,z) = i+1, \; \partial(y,z) = 1\}|
\end{align}
depends only on \(i\), \(x\), and not on \(y\).

(In this case, \(c_0(x) = a_0(x) = b_d(x) = 0\), \(c_1(x) = 1\), \(b_0(x) = k(x)\) is the valency of \(x\).)

We call \(c_i(x)\), \(a_i(x)\) and \(b_i(x)\) the intersection numbers with respect to \(x\).
\end{definition}

\begin{example}
\leavevmode

\begin{center}\includegraphics{t-algebra_files/figure-latex/unnamed-chunk-8-1} \end{center}

\begin{align}
c_0 &= 1 & c_1 &= 1 & c_2 &= 1\\
a_0 &= 0 & a_1 &= 1 & a_2 &= 1\\
b_0 &= 2 & b_1 &= 1 & b_2 &= 0
\end{align}

\end{example}

\hypertarget{lec12}{%
\chapter{Distance-Regular}\label{lec12}}

\textbf{Monday, February 15, 1993}

\begin{lemma}
\protect\hypertarget{lem:distance-reguarity}{}\label{lem:distance-reguarity}

For any connected graph \(\Gamma = (X, E)\), the following are equivalent.

\((i)\) The trivial \(T(x)\)-module is thin for all \(x\in X\).

\((ii)\) \(\displaystyle{\left\{\sum_{y\in X, \partial(x,y) = i}\hat{y} \left| 0\leq i\leq d(x)\right.\right\}}\) is a basis for the trivial \(T(x)\)-module for every \(x\in X\).

\((iii)\) \(\Gamma\) is distance-regular with respect to \(x\) for all \(x\in X\).

\end{lemma}

\textbf{Note.}
Let \(\Gamma = (X, E)\) be a graph, with \(X = \{x, y_1, y_2, y_3, z_1, z_2, z_3\}\), \(E = \{xy_1, xy_2, xy_3, y_1z_1, y_1z_2, y_2z_3, y_3z_3\}\).

\begin{center}\includegraphics{t-algebra_files/figure-latex/unnamed-chunk-9-1} \end{center}

Then \((i)\), \((ii)\) are not equivalent for a single vertex \(x\).
\begin{align}
E^*_0T\hat{x} & = \langle \hat{x}\rangle, \\
E^*_1T\hat{x} & = \langle y_1 + y_2 + y_3\rangle, \\
E^*_2T\hat{x} & = \langle z_1 + z_2 + 2z_3\rangle.
\end{align}

\begin{proof}[Proof of Lemma \ref{lem:distance-reguarity}]
\((i)\to (ii)\)
Let \(\delta = \sum_{y\in X}\delta_y\hat{y}\) be an eigenvector for the maximal eigenvalue \(\theta_0\). Then,
\begin{align}
\sum_{y\in X, \partial(x,y) = 1}\hat{y} & = A\hat{x} \in T(x)\hat{x} = T(x)\delta \ni E^*_1\delta\\
& = \sum_{y\in X, \partial(x,y)=1}\delta_h\hat{y}
\end{align}
If the trivial \(T(x)\)-module is thin,
\[\delta_y = \delta_z \; \text{ for }\; y, z\in X, \; \partial(x,y) = \partial(x,z) = 1.\]
Hence, \(\delta_y = \delta_z\) if \(y\) and \(z\) in \(X\) are connected by a path of even length.

So, \(\Gamma\) is regular or bipartite biregular by Lemma \ref{lem:baiparite-principal}.

In particular, \(\delta_y = \delta_z\) if \(\partial(x,y) = \partial(x,z)\), as there is a path of length \(2\cdot \partial(x,y)\);
\[y\sim \cdots \sim x \sim \cdots \sim z.\]
Hence,
\[E^*_i\delta \in \mathrm{Span}\left(\sum_{y\in X, \partial(x,y) = i}\hat{y}\right).\]
Since \(E^*_0\delta, E^*_1\delta, \ldots, E^*_d\delta\) forms a basis for \(T(x)\delta\), we have \((ii)\).

\((ii)\to (iii)\)
Fix \(x\in X\), and let \(T \equiv T(x)\), \(E^*_i\equiv E^*_i(x)\), and \(d \equiv d(x)\).

\begin{align}
A\sum_{y\in X, \partial(x.y)=i}\hat{y} & = \sum_{z\in X} |\{y\in X \mid \partial(y,z) = 1, \; \partial(x,y) = i\}|\hat{z}\\
& = \sum_{z\in X, \partial(x,y) = i-1}b_{i-1}(x,z)\hat{z} \\
& \qquad + \sum_{z\in X, \partial(x,y) = i} a_{i}(x,z)\hat{z} \\
& \qquad + \sum_{z\in X, \partial(x,y) = i+1} c_{i+1}(x,z)\hat{z}\\
& \in \mathrm{Span}\left\{\left.\sum_{z\in X, \partial(x,z) = j}\hat{z} \; \right| \; j = 0, 1, \ldots, d \right\}.
\end{align}

Hence, \(b_{i-1}(x,z)\), \(a_i(x,z)\) and \(c_{i+1}(x,z)\) depend only on \(i\) and \(x\), and not on \(z\).
Therefore, \(\Gamma\) is distance-regular with respect to \(x\).

\((iii)\to (i)\)
Fix \(x\in X\), and let \(T \equiv T(x)\), \(E^*_i\equiv E^*_i(x)\), and \(d \equiv d(x)\).
By defintion of distance-regularity, for every \(i\) \((0\leq i\leq d)\),
\begin{align}
A\left(\sum_{y\in X, \partial(x,y)=i}\hat{y}\right) & = 
b_{i-1}(x)\sum_{y\in X, \partial(x,y) = i-1}\hat{y} \\
& \qquad + a_{i}(x)\sum_{y\in X, \partial(x,y) = i}\hat{y}\\
& \qquad + c_{i+1}(x)\sum_{y\in X, \partial(x,y) = i+1}\hat{y}.
\end{align}
Hence,
\[W = \left\{\left.\sum_{y\in X, \partial(x,y)=i}\hat{y} \; \right| \; 0\leq i\leq d\; \right\}\]
is \(A\)-invariant and so \(T\)-invariant. Since \(\hat{x}\in W\), \(T\hat{x} = W\) is the trivial module and \(T\hat{x}\) is thin.
\end{proof}

Next, we show more is true if \((i)-(iii)\) hold in Lemma \ref{lem:distance-reguarity}.

In fact, \(d(x)\), \(a_i(x)\), \(c_i(x)\), and \(b_i(x)\) are
\[\begin{cases} 
\text{independent of $X$} & \text{if $\Gamma$ is regular; or}\\
\text{constant over $X^+$ and $X^-$} & \text{if $\Gamma$ is biregular.}
\end{cases}\]

Let \(\Gamma = (X, E)\) be any (connected) graph. Pick vertices \(x, y\in X\).

Let \(W\) be a thin, irreducible \(T(x)\)-module, and
measure \(m: \mathbb{R} \to \mathbb{R}\) determined by \(W\).

Let \(W'\) be a thin, irreducible \(T(y)\)-module, and
measure \(m: \mathbb{R} \to \mathbb{R}\) determined by \(W'\).

Recall \(W\), \(W'\) are orthogonal if
\[\langle w, w'\rangle = 0 \quad \text{for all }w\in W, w'\in W'.\]

We shall show if \(W\) and \(W'\) are note orthogonal, then \(m\) and \(m'\) are related:
\[m\cdot \mathrm{poly}_1 = m'\cdot \mathrm{poly}_2\]
for some polynomials with
\[\deg \mathrm{poly}_1 + \deg \mathrm{poly}_2 \leq 2\cdot \partial(x,y).\]

\textbf{Notation.}
\(V\): standard module of \(\Gamma\).

\(H\): any subspace of \(V\).

\[V = H + H^\bot \quad \text{orthogonal direct sum},\]
and for \(v = v_1 + v_2\) \(\mathrm{proj}_H: V\to H \; (v\mapsto v_1)\): linear transformation.

Observe:
For every \(v\in V\),
\[v - \mathrm{proj}_H v \in H^\bot.\]
So,
\[\langle v - \mathrm{proj}_H v, h\rangle = 0 \quad \text{for all }\;h\in H \text{ or},\]
\[\langle v, h\rangle = \langle \mathrm{proj}_H v, h\rangle \quad \text{for all }\;v\in V, \;\text{ and for all }\: h\in H.\]

\begin{theorem}
\protect\hypertarget{thm:two-thin-modules}{}\label{thm:two-thin-modules}Let \(\Gamma = (X,E)\) be any graph. Pick vertices \(x,y\in X\) and set \(\Delta = \partial(x,y)\). Assume

\(W\): thin irreducible \(T(x)\)-module with endpoint \(r\), diameter \(d\), and measure \(m\).

\(W'\): thin irreducible \(T(y)\)-module with endpoint \(r'\), diameter \(d'\), and measure \(m'\).

\(W\) and \(W'\) are not orghotonal.

Now pick
\[0\neq w\in E^*_r(x)W, \quad 0\neq w\in E^*_{r'}(x)W'.\]
Then,

\((i)\) \({\displaystyle \mathrm{proj}_{W'}w = p(A)\frac{\|w\|}{\|w'\|}w'}\)

for some \(0\neq p\in \mathbb{C}[\lambda]\) with \(\deg p \leq \Delta - r' + r, d'\),

\({\displaystyle \mathrm{proj}_{W}w' = p'(A)\frac{\|w'\|}{\|w\|}w}\)

for some \(0\neq p'\in \mathbb{C}[\lambda]\) with \(\deg p \leq \Delta - r + r', d\).

\((ii)\) For all eigenvalues \(\theta_i\) of \(\Gamma\),

\[\frac{\langle E_iw, E_iw'\rangle}{\|w\|\|w'\|} = m(\theta_i)\overline{p'(\theta_i)}.\]

\((iii)\) For all eigenvalues \(\theta_i\) of \(\Gamma\),

\[p(\theta_i)p'(\theta_i)\]
is in a real number in interval \([0,1]\).
\end{theorem}

\begin{proof}
\((i)\) Since \(W\), \(W'\) are not orthogonal, there exist
\[v\in W, \; v'\in W' \; \text{ sich that }\; \langle v, v'\rangle \neq 0.\]
Then there exists \(a\in M\) such that
\[v' = aw'.\]
(This is becase \(w'_i = p'_i(A)w_0'\) and hence for every \(v'\in W'\), there is a polynomial \(q\in \mathbb{C}[\lambda]\), \(q(A)w_0' = v\).)

We have
\[0\neq \langle v', v\rangle = \langle aw', v\rangle = \langle w', a^*v\rangle\]
and \(a^*v\in W\).

Hence, \(\mathrm{proj}_{W} w' \neq 0\).

Let \(p_0, \ldots, p_d\in \mathbb{C}[\lambda]\) be from Lemma \ref{lem:thin-module-structure}.

Then, \(w_i = p_i(A)w\) is a basis for \(E^*_{r+i}(x)W \quad (0\leq i\leq d)\).

Hence,
\[\mathrm{proj}_{W}w' = \alpha_0w_0 + \cdots + \alpha_dw_d \quad \text{for some }\; \alpha_j\in \mathbb{C}.\]
Set
\[p' := \frac{\|w\|}{\|w'\|}\sum_{i=0}^d \alpha_ip_i.\]
Then \(0\neq p'\in \mathbb{C}[\lambda]\) and \(\deg p' \leq d\).

Claim: \(\alpha_i = 0\) \((\Delta - r + r' < i\leq d)\).

In particular, \(\deg p' \leq \Delta - r + r'\).

\emph{Pf.} Obseve:
\[w'\in E^*_{r'}(y)V, \quad w \in E^*_r(x)V,\]
for \(\partial(x,y) = \Delta\).
\[E^*_{r'}(y)V \cap E^*_{r+i}(x)V = 0\]
by triangle inequality.

(\(\Delta = \partial(x,y) < r+i - r'\) or \(\Delta + r' < r + i\) by our choice of \(i\).)

\begin{center}\includegraphics{t-algebra_files/figure-latex/unnamed-chunk-10-1} \end{center}

Hence,
\[E^*_{r'}(y)V \bot E^*_{r+i}(x)V,\]
or
\begin{align}
0 & = \langle w', w_i\rangle \\
& = \langle \mathrm{proj}_{W}w', w_i\rangle\\
& = \sum_{j=0}^d\alpha_j\langle w_j, w_i\rangle\\
& = \alpha_i\|w_i\|^2.
\end{align}
Hence, \(\alpha_i = 0\).
Thus,
\begin{align}
\mathrm{proj}_{W}w' & = \sum_{i=0}^{\Delta + r' - r}\alpha_iw_i\\
& = \sum_{i=0}^{\Delta + r' - r}\alpha_ip_i(A)w_0\\
& = p'(A)\frac{\|w'\|}{\|w\|}w.
\end{align}

\((ii)\) We have
\begin{align}
\frac{\langle E_iw, E_iw'\rangle}{\|w\|\|w'\|}
& = \frac{\langle E_iw, w'\rangle}{\|w\|\|w'\|}\\
& = \frac{\langle E_iw, \mathrm{proj}_W w'\rangle}{\|w\|\|w'\|} && \text{as }\; \mathrm{proj}_Ww' = p'(A)\frac{\|w\|}{\|w'\|}w\\
& = \frac{\langle E_iw, p'(A) w\rangle}{\|w\|^2}\\
& = \frac{\langle E_iw, E_ip'(A) w\rangle}{\|w\|^2}\\
& = \overline{p'(\theta_i)}\frac{\|E_iW\|^2}{\|w\|^2}\\
& = \overline{p'(\theta_i)}m(\theta_i).
\end{align}
Moreover, as \(m(\theta_i)\), \(m'(\theta_i)\in \mathbb{R}\),
\[\frac{\langle E_iw, E_iw'\rangle}{\|w\|\|w'\|}  = \frac{\overline{\langle E_iw, E_iw'\rangle}}{\|w'\|\|w\|}
 = \overline{\overline{p(\theta_i)}m'(\theta_i)} 
 = p(\theta_i)m'(\theta_i).\]

\((iii)\) Sicne,
\[\frac{|\langle E_iw, E_iw'\rangle\|^2}{\|w\|^2\|w'\|^2} = p(\theta_i)p'(\theta_i)m(\theta_i)m'(\theta_i),\]
\begin{align}
p(\theta_i)p'(\theta_i) & = \frac{|\langle E_iw, E_iw'\rangle\|^2}{m(\theta_i)m'(\theta_i)\|w\|^2\|w'\|^2} \in \mathbb{R}\\
& = \frac{|\langle E_iw, E_iw'\rangle\|^2}{\frac{\|E_iw\|^2}{\|w\|^2}\frac{\|E_iw'\|^2}{\|w'\|^2}\|w\|^2\|w'\|^2}.
\end{align}

By Cauchy-Schwartz inequality,
\[(|\langle a, b\rangle | \leq \|a\|\|b\|,)\]
\[\frac{|\langle E_iw, E_iw'\rangle\|^2}{\|E_iw\|^2\|E_iw'\|^2} \leq 1.\]
Hence, we have the assertion.
\end{proof}

\hypertarget{lec13}{%
\chapter{Modules of a DRG}\label{lec13}}

\textbf{Wednesday, February 17, 1993}

\begin{lemma}
\protect\hypertarget{lem:two-principal-modules}{}\label{lem:two-principal-modules}Let \(\Gamma = (X, E)\) be any graph. Pick an edge \(xy\in E\).

Assume the trivial \(T(x)\)-module \(T(x)\delta\) is thin with measure \(m_x\),

and the trivial \(T(y)\)-module \(T(y)\delta\) is thin with measure \(m_y\).

Then,

\((ia)\) \({\displaystyle \frac{m_x(\theta)}{k_x} = \frac{m_y(\theta)}{k_y} \text{ for all } \theta\in \mathbb{R}\setminus \{0\}}\).

\((ib)\) \({\displaystyle \frac{m_x(0) -1}{k_x} = \frac{m_y(0)-1}{k_y} \text{ for all } \theta\in \mathbb{R}\setminus \{0\}}\).

\[(\delta = \sum_{y\in X}\delta_y \hat{y} \quad \text{eigenvector corresponding to the maximal eigenvalue})\]
\end{lemma}

\begin{proof}
Apply Theorem \ref{thm:two-thin-modules},
\begin{align}
W & = T(x)\delta \quad r = 0, \quad d = d(x)\\
W' & = T(y)\delta \quad r' = 0, \quad d' = d(y).
\end{align}
Take \(w = \hat{x}\), \(w' = \hat{y}\).

Claim. \(\mathrm{proj}_{T(y)\delta}\hat{x} = k^{-1}_yA\hat{y}\).

\emph{Pf.}
Since
\[\hat{y}\in T(y)\delta, \quad A\hat{y}\in T(y)\delta.\]
Show
\[(\hat{x} - {k_y}^{-1} A\hat{y}) \bot (T(y)\delta).\]
Recall
\[A\hat{y} = \sum_{z\in X, yz\in E}\hat{z}.\]
\[\hat{x} - {k_y}^{-1}Ay \in E^*_1(y)V.\]
So,
\[\hat{x} - \frac{1}{k_y}A\hat{y} \; \bot \; E^*_j(y)T(y)\delta \quad \text{if $j\neq 1$}\; (0\leq j\leq k(y)).\]
And we have,
\begin{align}
\left\langle \hat{x} - \frac{1}{k_y}A\hat{y}, A\hat{y}\right\rangle
& = \left\langle \hat{x}, \sum_{z\in X, yz\in E}\hat{z}\right\rangle - \frac{1}{k_y}\left\|\sum_{z\in X, yz\in E}\hat{z}\right\|^2\\
& =  1 - 1\\
& = 0
\end{align}
This proves Claim.

Similarly,
\[\mathrm{prof}_{T(x)\delta}\hat{y} = {k_x}^{-1}A\hat{x}.\]
Hence, the polynomials \(p, p'\in \mathbb{C}[\lambda]\) from Theorem \ref{thm:two-thin-modules} equal
\[\frac{\lambda}{k_y} \quad \text{ and }\quad \frac{\lambda}{k_x}\]
respectively.

By Theorem \ref{thm:two-thin-modules},
\[\frac{m_x(\theta)\theta}{k_x} = m_x(\theta)\overline{p'(\theta)} = m_y(\theta)\overline{p(\theta)} = \frac{m_y(\theta)\theta}{k_y}.\]
If \(\theta\neq 0\), we have \((ia)\).

Also,
\begin{align}
\frac{1-m_x(0)}{k_x} & = \left(\sum_{\theta\in \mathbb{R}\setminus \{0\}}m_x(0)\right)\frac{1}{k_x} && \text{by $(ia)$}\\
& = \left(\sum_{\theta\in \mathbb{R}\setminus \{0\}}m_y(0)\right)\frac{1}{k_y}\\
& = \frac{1 - m_y(0)}{k_y}
\end{align}
Hence, we have \((ib)\).
\end{proof}

\begin{theorem}
\protect\hypertarget{thm:distance-regular-x}{}\label{thm:distance-regular-x}Suppose any graph \(\Gamma = (X, E)\) is distance-regular with respect to every vertex \(x\in X\).
(So \(\Gamma\) is regular or biregular by Lemma \ref{lem:distance-reguarity}.)

Then,

Case \(\Gamma\) is regular: the diameter \(d(x)\) and the intersection numbers \(a_i(x)\), \(b_i(x)\), \(c_i(x)\) \((0\leq i\leq d(x))\) are independent of \(x\in X\).

(And \(\Gamma\) is called distance-regular.)

Case \(\Gamma\) is biregular: \((X = X^+\cup X^-)\)

\(d(x)\) and \(a_i(x)\), \(b_i(x)\), \(c_i(x)\) \((0\leq i\leq d(x))\) are constant over \(X^+\) and \(X^-\).
(And \(\Gamma\) is called distance-biregular.)
\end{theorem}

\begin{proof}
We apply Lemma \ref{lem:two-principal-modules}.

Case \(\Gamma\): regular.

Then \(m_x = m_y\) for all \(xy\in E\). Hence, the measure of the trivial \(T(x)\)-module is independent of \(x\in X\).

Case \(\Gamma\) is biregular.

Then \(m_x = m_{x'}\) for all \(x, x'\in X\) with \(\partial(x,x') = 2\).

Hence, the measure of the trivial \(T(x)\)-module is constant over \(x\in X^+\), \(X^-\).

Fix \(x\in X\). Write \(T\equiv T(x)\), \(E^*_i \equiv E^*_i(x)\), \(W = T\delta\) with measure \(m\), diameter \(d = d(x)\).

We know by Corollary \ref{cor:isomorphic} that \(m\) determines
\[d, \quad a_i(W) \; (0\leq i\leq d), \quad x_i(W) \; (1\leq i\leq d)\]
(as \(d = D(x) = d(W)\) by Lemma \ref{lem:principal-module}.)

We shall show that \(m\) determines
\[a_i(x), \;  c_i(x), \; b_i(x) \quad (0\leq i\leq d).\]
Observe:
\begin{align}
a_i(W) & = a_i(x) \quad (0\leq i\leq d)\\
x_i(W) & = b_{i-1}c_i(x) \quad (1\leq i\leq d)
\end{align}

\begin{remark}
\(a_i = a_i(W)\) is an eigenvalue of
\[E^*_iAE^*_i \text{ on } E^*_iW = \langle \sum_{y\in \Gamma_i(x)}\hat{y}\rangle. \]
(See Lemma \ref{lem:distance-reguarity}.)

\(x_i = x_i(W)\) is an eigenvalue of
\[E^*_{i-1}AE^*_iAE^*_{i-1} \text{ on } E^*_{i-1}W,\]
and
\begin{align}
A\sum_{y\in X,\partial(x,y)}\hat{y} & = b_{i-1}(x)\sum_{y\in X, \partial(x,y)=i-1}\hat{y} \\
& \quad + a_i(x)\sum_{y\in X, \partial(x,y)=i}\hat{y} \\
& \quad + c_{i+1}\sum_{y\in X, \partial(x,y)=i+1}\hat{y}
\end{align}
So \(x_i = b_{i-1}(x)c_i(x)\).
\end{remark}

Set \(k^+ = k_x\). Define
\[k^- = \frac{{\theta_0}^2}{k^+},\]
where \(\theta_0\) is the maximal eigenvalue. (See Lemma \ref{lem:principal-module}.)

(So, \(k^+ = k^-\) is the valency, if \(\Gamma\) is regular.)

For every \(i \; (0\leq i\leq d)\) and for every \(z\in X\) with \(\partial(x,z) = i\),
\begin{align}
k_z & = c_i(x) + a_i(x) + b_i(x)\\
& = \begin{cases} k^+ & \text{if $i$ is even,}\\
k^- & \text{ if $i$ is odd.}
\end{cases}
\end{align}

Now \(m\) determines
\[c_0(x) = a_0(x) = 0,\quad c_1(x) = 1,\]
\[b_0(x) = b_0(x)c_1(x) = x_1(W).\]
\begin{align}
k^+ & = b_0(x)\\
k^- & = {\theta_0}^2/k^+\\
c_i(x) & = x_i(W)/b_{i-1}(x) \quad (1\leq i\leq d)\\
b_i(x) & = \begin{cases} k^+ - a_i(x) - c_i(c) \quad \text{$i$; even,}\\
k^--a_i(x)-c_i(x) \quad \text{$i$: odd}.
\end{cases}
\end{align}
This proves the assertions.
\end{proof}

\begin{proposition}
\protect\hypertarget{prp:dim-diameter}{}\label{prp:dim-diameter}

Under the assumption of Theorem \ref{thm:distance-regular-x}, the following hold.

Case \(\Gamma\): regular.

\((i)\) \(\dim E_iV = |X|m(\theta_i)\).\\
\((ii)\) \(\Gamma\) has exactly \(d+1\) distinct eigenvalues

(\(d = \mathrm{diam}\Gamma = d(x), \; \text{ for all }\; x\in X\)).

Case \(\Gamma\): biregular.

\((i)\) \(\dim E_V = |X^+| m^+(\theta_i) + |X^-|m^-(\theta_i)\).\\
\((ii)\) \(\Gamma\) has exactly \(d^++1\) distinct eigenvalues \((d^+\geq d^-\)).\\
\((iii)\) If \(d^+\) is odd, the \(\Gamma\) is regular.\\
\((iv)\) \(d^+ = d^-\), or \(d^+ = d^-+1\) is even.\\
\((v)\) \(a_i(x) = 0\) for all \(i\) and for all \(x\).

\end{proposition}

\begin{proof}

\((i)\) Suppose \(\Gamma\) is regular.

Let \(m_x\) be the measure of the trivial \(T(x)\)-module,
\[m_x(\theta_i) = \|E_i\hat{x}\|^2, \quad \text{as}\; \|\hat{x}\| = 1.\]
Now,
\begin{align}
|X|m_x(\theta_i) & = \sum_{x\in X}m_x(\theta_i)\\
& = \sum_{x\in X}\|E_i\hat{x}\|^2\\
& = \sum_{y,z\in X}|(E_i)_{yz}|^2\\
& = \mathrm{trace} E_i\overline{E_i}^\top.
\end{align}
Since \(A\) is real symmetric and
\[E_i\overline{E_i}^\top = E_i^2 = E_i\]
with \(E_i\) symmetric
\[E_i \sim \begin{pmatrix} I & O \\ O & I\end{pmatrix}.\]
\[\mathrm{trace} E_i = \rank E_i = \dim E_iV.\]
Thus, we have the assertion in this case.

Suppose \(\Gamma\) is biregular.

Then, same except,
\[\sum_{x\in X} m_x(\theta_i) = |X^+|m^+(\theta_i) + |X^-||m^-(\theta_i).\]

\((ii)\)
\(\Gamma\): regular. Immediately, if \(\theta\) is an eigenvalue of \(\Gamma\), then \(m(\theta) \neq 0\).

\(\Gamma\): biregular. For each \(\theta = \theta_i \in \mathbb{R}\setminus\{0\}\),
\begin{align}
m^-(\theta) \neq 0 &\Leftrightarrow m^+(\theta) \neq 0\\
& \Leftrightarrow \theta \; \text{ is an eigenvalue of $\Gamma$}\\
& \quad\quad \left(\frac{m^+(\theta)}{k^+} = \frac{m^-(\theta)}{k^-} \right)
\end{align}

\((iv)\) and \((v)\) are clear.

\begin{remark}
\((iii)\) If \(d^+\) is odd, \(d^+ = d^-\) and \(\Gamma\) has even number of eigenvalues, i.e., \(0\) is not an eigenvalue.
So \(A\) is nonsingular, and \(\Gamma\) is regular.
\end{remark}

\end{proof}

\hypertarget{lec14}{%
\chapter{Parameters of Thin Modules, I}\label{lec14}}

\textbf{Friday, February 19, 1993}

Summary.

\begin{definition}
Assume \(\Gamma = (X, E)\) is distance-regular with respect to every vertex \(x\in X\).

Notation: Let \(x\in X\). The data of the trivial \(T(x)\)-module.

\[\begin{array}{|c|c|c|}
\hline
& \text{Case DR}  & \text{Case DBR} \\
\hline
\text{valency} k_x & k & \begin{cases} k^+ & \text{ if } x\in X^+\\
k^- & \text{ if } x\in X^-\end{cases}\\
x\text{-diameter } D_x & D & \begin{cases} D^+ & \text{ if } x\in X^+\\
D^- & \text{ if } x\in X^-\end{cases}\\
\text{measure $m_x$} & m & \begin{cases} m^+ & \text{ if } x\in X^+\\
m^- & \text{ if } x\in X^-\end{cases}\\
\text{int. number }c_i(x) & c_i & \begin{cases} c_i^+ & \text{ if } x\in X^+\\
c_i^- & \text{ if } x\in X^-\end{cases}\\ 
\text{int. number }b_i(x) & b_i & \begin{cases} b_i^+ & \text{ if } x\in X^+\\
b_i^- & \text{ if } x\in X^-\end{cases}\\ 
\text{int. number }a_i(x) & a_i & \begin{cases} a_i^+ & \text{ if } x\in X^+\\
a_i^- & \text{ if } x\in X^-\end{cases}\\ 
\hline
\end{array}\]

Call \(m\), \(m^{\pm 1}\) the measure of \(\Gamma\).
\end{definition}

Assume \(\Gamma = (X, E)\) is distance-regular.

To what extent do \(a_i\)'s, \(b_i\)'s and \(c_i\)'s determine the structure of irreducible \(T(x)\)-modules? In general the following hold.

\begin{lemma}
\protect\hypertarget{lem:basic-data}{}\label{lem:basic-data}Assume \(\Gamma = (X, E)\) is distance-regular. Pick \(x\in X\). Let \(X\) be a thin irreducible \(T(x)\)-module with endpoint \(r\), diameter \(d\) and measure \(m_W\).

\((i)\) There is a unique polynomial \(f_W \in \mathbb{C}[\lambda]\) with the following properties.

\(\quad (ia)\) \(\deg f_W \leq D\) (diameter of \(\Gamma\)).

\(\quad (ib)\) \(m_W(\theta) = m(\theta)f_W(\theta)\) for every \(\theta\in \mathbb{R}\), where \(m\) is the measure of \(\Gamma\).

Moreover, \(f_W\in \mathbb{R}[\lambda]\), and

\((ii)\) \(\deg f_W \leq 2r\).\\
\((iii)\) For all eigenvalues \(\theta_i\) of \(\Gamma\), \(\lambda - \theta_i\) is a factor of \(f_W\) whenever, \(E_iW = 0\).

In particular, \(2r-D+d\geq 0\).
\end{lemma}

\begin{proof}
Let \(\theta_0, \ldots, \theta_D\) denote distinct eigenvalues of \(\Gamma\). Then \(m(\theta_i) \neq 0\) \((0\leq i\leq D)\) by Proposition \ref{prp:dim-diameter}.

There exists a unique \(f_W\in \mathbb{C}[\lambda]\) with \(\deg f_W\leq D\) such that
\[f_W(\theta_i) = \frac{m_W(\theta_i)}{m(\theta_i)} \quad (0\leq i\leq D)\]
by polynomial interpolation.

\(f_W\in \mathbb{R}[\lambda]\) since
\[\theta_0, \ldots, \theta_D\in \mathbb{R} \quad \text{and}\quad f_W(\theta_0), \ldots, f_W(\theta_D) \in \mathbb{R}.\]

\((ii)\) Without loss of generality, we may assume \(r < D/2\), else trivial.

Pick \(0\neq w \in E^*_r(x)W\).
\[w = \sum_{y\in W, \partial(x,y) = r}\alpha_y\hat{y} \quad \text{ some } \; \alpha_y\in \mathbb{C}.\]
Pick \(y\in X\) such that \(\alpha_y\neq 0\).

Set \(W'\) be the trivial \(T(y)\)-module. (\(\langle w, \hat{y}\rangle \neq 0, \text{ as } W\not\bot W'\).)
\[r' = 0, \quad m' = m, \quad \Delta = r.\]

Apply Theorem \ref{thm:two-thin-modules}, we have
\begin{align}
\deg p & \leq \Delta - r' + r = 2r, \quad p\neq 0\\
\deg p' & \leq \Delta - r + r' = 0 , \quad p'\neq 0.
\end{align}
\[m_W(\theta)\overline{p'(\theta)} = m(\theta)p(\theta) \quad (\text{ for all } \theta \in \mathbb{R}).\]
So,
\[\deg p/\bar{p}' \leq 2r,\]
and \(p/\bar{p}'\) satisfies the conditions of \(f_W\).
\[\left(\frac{p(\theta)}{\bar{p}'(\theta)} = \frac{m_W(\theta)}{m(\theta)}\right)\]

\((iii)\)
\[E_iW = 0 \rightarrow m_W(\theta_i) = 0 \rightarrow f_W(\theta_i) = 0.\]
that is, \(E_iW = 0\). Hence \(\theta_i\) is a root of \(f_W(\lambda) = 0\).
So,
\[2r \geq \deg f_W \geq |\{\theta_i\mid E_iW = 0\}| = D-d.\]
Hence,
\[2r-D + d \geq 0.\]
This proves the assertions.
\end{proof}

\begin{lemma}
\protect\hypertarget{lem:thin-endpoint1}{}\label{lem:thin-endpoint1}Let \(\Gamma = (X, E)\) be any distance-regular graph with valency \(k\), diameter \(D\) \((d\geq 2)\), measure \(m\), and eigenvalues
\[k = \theta_0 > \theta_1 > \cdots > \theta_D.\]
Pick \(x\in X\). Let
\(W\) be a thin irreducible \(T(x)\)-module with endpoint \(r = 1\), diameter \(D\) and measure \(m_W = mf_W\).
Then one fo the following cases \((i)-(iv)\) occurs.

\[\begin{array}{|c|c|c|c|}
\hline
\text{Case} & d  & f_W(\lambda) & a_0(W) \\
\hline
(i) & D-2 & \frac{(\lambda - k)(\lambda - \theta_1)}{k(\theta_1 + 1)} & -\frac{b_1}{\theta_1 + 1} -1\\
(ii) & D-2 & \frac{(\lambda - k)(\lambda - \theta_D)}{k(\theta_D + 1)} & -\frac{b_1}{\theta_1 + 1} -1\\
(iii) & D-1 & \frac{k - \lambda}{k} & -1\\
(iv) & D-1 & \frac{(\lambda - k)(\lambda - \beta)}{k(\beta + 1)} & -\frac{b_1}{\beta + 1} -1\\ 
\hline
\end{array}\]
for some \(\beta\in \mathbb{R}\) with \(\beta\in (-\infty, \theta_D) \cup (\theta_1, \infty)\).
Moreover, the isomorphism class of \(W\) is determined by \(a_0(W)\).
\end{lemma}

\textbf{Note.} By \((iii)\), the possible ``shapes'' of a thin irreducible \(T(x)\)-modules are:
\begin{align}
r = 0 & \quad d = D\\
r = 1 & \quad d = D-1\\
r = 1 & \quad d = D-2
\end{align}

\hypertarget{lec15}{%
\chapter{Parameters of Thin Modules, II}\label{lec15}}

\textbf{Monday, February 22, 1993}

\begin{proof}[Proof of Lemma \ref{lem:thin-endpoint1} Continued]
\leavevmode

We have \(\deg f_W\leq 2\) by Lemma \ref{lem:basic-data} \((ii)\).

Also bt Lemma \ref{lem:principal-module}, \(E_0W = 0\).

(As otherwise \(\langle \delta \rangle = E_0V \subseteq W\) and \(r(W) = 0\).)

Hence, \(\lambda - \theta_0 = \lambda - k\) is a factor of \(f_W\) by Lemma \ref{lem:basic-data} \((iii)\).

Let \(p_0, p_1, \ldots, p_D\) denote the polynomials for the trivial \(T(x)\)-module from Lemma \ref{lem:thin-module-structure}.

Recall,
\begin{align}
\sum_{\theta\in \mathbb{R}}m(\theta)p_i(\theta)p_j(\theta) & = \delta_{ij}x_1x_2\cdots x_i \quad (0\leq i,j\leq D)\\
& = \delta_{ij}b_0b_1\cdots b_{i-1}c_1c_2\cdots c_i.
\end{align}
Note that \(x_i = b_{i-1}c_i\) is in the proof of Theorem \ref{thm:thin-condition}.

By construction,
\begin{align}
p_0(\lambda) & = 1.
p_1(\lambda) & = \lambda.
p_2(\lambda) & \lambda^2 - a_1\lambda - k.
\end{align}

Apparently,
\[f_W = \sigma_0 p_0 + \sigma p_1 + \sigma_2 p_2\]
for some \(\sigma_0, \sigma_1, \sigma_2\in \mathbb{C}\).

Claim:
\begin{align}
\sigma_0 & = 1,\\
\sigma_1 & = \frac{a_0(W)}{k},\\
\sigma_2 & -\frac{1+a_0(W)}{kb_1}.
\end{align}

\emph{Pf of Claim.}
\begin{align}
1 & = \sum_{\theta\in \mathbb{R}}m_W(\theta)\\
& = \sum_{\theta\in \mathbb{R}}m(\theta)f_W(\theta)\\
& = \sum_{j=0}^2 \sigma_j\left(\sum_{\theta\in \mathbb{R}}m(\theta)p_j(\theta)\right)\\
& = \sigma_0.
\end{align}
We applied Lemma \ref{lem:orthogonality} \((ib)\), Lemma \ref{lem:basic-data} \((ib)\), and Lemma \ref{lem:orthogonality} \((i)\) in this order.

Next by Lemma \ref{lem:orthogonality} \((ii)\), and \(p_1(\theta) = \theta\),
\begin{align}
a_0(W) & = \sum_{\theta\in \mathbb{R}}m_W(\theta)\theta\\
& = \sum_{\theta\in \mathbb{R}}f_W(\theta)\theta\\
& = \sum_{j = 0}^2\sigma_j\sum_{\theta\in \mathbb{R}}m(\theta)p_j(\theta)p_1(\theta)\\
& = \sigma_1 x_1(T\delta)\\
& = \sigma_1b_0c_1\\
& = \sigma_1 k.
\end{align}
So for,
\[f_W(\lambda) = 1 + \frac{a_0(W)}{k}\lambda + \sigma_2(\lambda^2 - a_1\lambda-k).\]
But,
\begin{align}
0 & = f_W(k)\\
& = 1 + a_0(W) + \sigma_2k(k-a_1-1)\\
& 1 + a_0(W) + \sigma_2kb_1.
\end{align}
Thus,
\[\sigma_2 = -\frac{1+a_0(W)}{kb_1}.\]
This proves Claim.

Case: \(a_0(W) = -1\).

Here, \(\sigma_2 = 0\) and
\[f_W(\lambda) = 1 + \frac{a_0(W)\lambda}{k} = 1-\frac{\lambda}{k}.\]
Also,
\[d+1 = |\{\theta \mid \theta \;\text{ is an eigenvalue of $\Gamma$}, \; f_W(\theta)\neq 0\} = D.\]

Case: \(a_0(W) \neq -1\).

Here, \(\sigma_2\neq 0\), and \(\deg f_W = 2\). So,
\[f_W(\lambda) = (\lambda - k)(\lambda - \beta)\alpha\]
for some \(\alpha, \beta\in \mathbb{C}, \; \alpha \neq 0\).

Comparing the coefficients in
\[(\lambda-k)(\lambda - \beta)\alpha = 1 + \frac{a_0(W)}{k}\lambda - \frac{a_0(W)+1}{kb_1}(\lambda^2 - a_1\lambda -k),\]
we find
\begin{align}
\alpha & = -\frac{a_0(W) + 1}{kb_1},\\
-(k+\beta)\alpha & = \frac{a_0(W)}{k} + \frac{a_0(W)+1}{kb_1}a_1,\\
k\beta\alpha & = 1 + \frac{a_0(W)+1}{b_1}.
\end{align}
Hence,
\[-\beta(a_0(W) + 1) = b_1 + (a_0(W) + 1).\]
Thus, we have
\begin{equation}
(1+a_0(W))(1+\beta) = -b_1. \label{eq:a0W}
\end{equation}
In particular, \(\beta \neq -1\), and
\[\alpha = -\frac{1+a_0(W)}{kb_1} = \frac{1}{k(\beta+1)}.\]
Also, by Definition \ref{def:measure},
\begin{align}
0 &\leq m_W(\theta) \\
& = m(\theta)f_W(\theta) \quad (\text{for all} \; \theta\in \mathbb{R}).
\end{align}
But if \(\theta\) is an eigenvalue of \(\Gamma\),
\[0 < m(\theta).\]
So,
\begin{align}
0 & \leq f_W(\theta)\\
& = \frac{(\theta-k)(\theta-\beta)}{k(\beta+1)}.
\end{align}
Either
\[\beta+1 >0 \to \theta-\beta \leq 0 \;\text{ or }\; \beta \geq \theta_1,\]
or
\[\beta+1 < 0 \to \theta-\beta \geq 0 \;\text{ or }\; \beta \leq \theta_D.\]
If \(\beta = \theta_1\),
\begin{align}
a_0(W) & = - \frac{b_1}{\beta+1}-1 = -\frac{b_1}{\theta_1+1}-1\\
f_W(\lambda) & = \frac{(\lambda - k)(\lambda - \theta_1)}{k(\theta_1+1)},
\end{align}
and we have \((i)\).

If \(\beta = \theta_D\),
\begin{align}
a_0(W) & = -\frac{b_1}{\theta_D+1}-1\\
f_W(\lambda) & = \frac{(\lambda - k)(\lambda - \theta_D)}{k(\theta_D+1)},
\end{align}
and we have \((ii)\).

If \(\beta \not\in \{\theta_1, \theta_2\}\),
\[\theta \in (-\infty, \theta_D) \cup (\theta_1, \infty),\]
we have \((iv)\).

Note using \eqref{eq:a0W}, we have \((iv)\).

\end{proof}

\textbf{Note.}
Using \eqref{eq:a0W},
\[a_0(W) \to \beta \to f_W \to m_W \to \text{isomorphism class of $W$}.\]

\textbf{Note on Lemma \ref{lem:thin-endpoint1}.}
In fact, \(\theta_1 > -1\), \(\theta_D < -1\) if \(D\geq 2\).

\begin{definition}
\protect\hypertarget{def:complete-graph}{}\label{def:complete-graph}The complete graph \index{complete graph} \(K_n\) has \(n\) vertices and diameter \(D = 1\), i.e., \(xy\in E\) for all vertices \(x,t\).
\end{definition}

\(K_n\) is distance-regular with valency \(k = n-1\) and \(a_1 = n-2\), \(D = 1\).
Moreover, it has two distince eigenvalues \(\theta_0\), \(\theta_1\).

Recall, \(\theta_0, \ldots, \theta_D\) are roots of \(p_{D+1}\), i.e., \(D+1\) st polynomial for the trivial module/
\begin{align}
p_0 & = 1\\
p_1 & = \lambda\\
p_2 & = \lambda^2 - a_1\lambda - k\\
& = \lambda^2 - (n-2)\lambda - (n-1)\\
& = (\lambda - (n-1))(\lambda +1).
\end{align}
The roots are \(\theta_0 = n-1 = k\) and \(\theta_1 = -1\).

\begin{lemma}
\protect\hypertarget{lem:sedond-and-largest-ev}{}\label{lem:sedond-and-largest-ev}

Let \(\Gamma = (X, E)\) be distance-regular of diameter \(D\geq 1\) with distinct eigenvalues
\[k = \theta_0 > \theta_1 > \cdots > \theta_D.\]

\((i)\) \(\theta_D\leq -1\) with equality if and only if \(D = 1\).

\((ii)\) \(\theta_1 \geq -1\) with equality if and only if \(D=1\).

\end{lemma}

\begin{proof}
\((i)\) Suppose \(\theta_D \geq -1\).

Then \(I + A\) is positive semi-definite.

By Lemma \ref{lem:pfl}, there exists vectors \(\{v_x\mid x\in X\}\) in a Euclidean space such that
\begin{align}
\langle v_x,v_y\rangle & = (I+A)_{xy}\\
& = \begin{cases} 1 & \text{if $x = y$ or $xy\in E$,}\\0 & \text{othewise}.
\end{cases}
\end{align}
For every \(xy\in E\),
\[\langle v_x, v_y\rangle = \|v_x\|\|v_y\| = 1.\]
Hence, \(v_x = v_y\), and \(v_x\) is independent of \(x\in X\).

Shus \(\langle v_x,v_y\rangle = 1\) for all \(x,y\in X\).

We have \(I + A = J\), (all 1's matrix), and \(D = 1\).

\((ii)\) Let \(m\) be the trivial measure. Then,
\begin{align}
1 & = \sum_{\theta\in \mathbb{R}}m(\theta) + \sum_{\theta\in \mathbb{R}}m(\theta)\theta \\
& = \sum_{\theta\in \mathbb{R}}m(\theta)(\theta+1)\\
& = m(k)(k+1) + \sum_{\theta\neq k}m(\theta)(\theta+1)\\
& \leq (k+1)|X|^{-1}.
\end{align}
Note that \(m(k) = |X|^{-1}\dim d_0V = |X|^{-1}\).

So \(k+1 \geq |X|\) or \(k = |X|-1\). Thus,
\(xy\in E\) for every \(x,y\in X\), and \(D = 1\).
\end{proof}

\textbf{Note.}
Lemma \ref{lem:sedond-and-largest-ev} does not require distance-regular assumption.

\hypertarget{lec16}{%
\chapter{Thin Modoles of a DRG}\label{lec16}}

\textbf{Wednesday, February 24, 1993}

Let \(\Gamma = (X, E)\) denote any graph of diameter \(D\).

\begin{definition}
\protect\hypertarget{def:ith-incidence}{}\label{def:ith-incidence}For all integer \(i\), the \(i\)-th incidence matrix \(A_i\in \mathrm{Mat}_X(\mathbb{C})\) satisfies
\[(A_i)_{xy} = \begin{cases} 1 & \text{if $\partial(x,y) = i$,}\\
0 & \text{if $\partial(x,y)\neq i$,}
\end{cases} \quad (x,y\in X).\]
Observe,
\begin{align}
A_0 & = I && (\text{identity})\\
A_1 & = A && (\text{adjacency matrix})\\
A_0 + A_1 + \cdots + A_D & = J && (\text{all 1's matrix}).
\end{align}
In general, \(A_i\) may not belong to Bose-Mesner algebra.
\end{definition}

\begin{lemma}
\protect\hypertarget{lem:incidence-matrices}{}\label{lem:incidence-matrices}

Assume \(\Gamma = (X, E)\) is distance-regular with diameter \(D\geq 1\) and intersection numbers \(c_i, a_i, b_i\).

\((i)\) \[AA_i = c_{i+1}A_{i+1} + a_iA_i + b_{i-1}A_{i-1}, \quad (0\leq i\leq D, \; A_{-1} = A_{D+1} = O).\]

\((ii)\) \({\displaystyle A_i = \frac{p_i(A)}{c_1c_2\cdots c_i}, \quad (0\leq i\leq D)}\), where \(p_0, p_1, \ldots, p_D\) are polynomials for the trivial module from Lemma \ref{lem:thin-module-structure}.

\((iii)\) \(A_0, A_1, \ldots, A_D\) form a bais for Bose-Mesner algebra \(M\).

\((iv)\) For all distances \(h, i, j \quad (0\leq, i, j, h\leq D)\), and for all vertices \(x, y\in X\) with \(\partial(x,y) = h\), the constant

\[p^h_{i,j} = |\{z\in X\mid \partial(x,z) = i, \; \partial(y,z) = j\}|\]
depends only on \(h, i, j\) and not on \(x, y\).

\((v)\) \({\displaystyle E_0 = \frac{1}{|X|}J.}\)

\end{lemma}

\begin{proof}
\leavevmode

\((i)\) Pick \(x\in X\). Apply each side to \(\hat{x}\), we want to show that

\[AA_i \hat{x} = c_{i+1}A_{i+1}\hat{x} + a_iA_i\hat{x} + b_{i-1}A_{i-1}\hat{x}.\]
\begin{align}
\mathrm{LHS} & = A\left(\sum_{y\in X, \partial(x,y) = i}\hat{y}\right)\\
& = c_{i+1}\left(\sum_{z\in X, \partial(x,z) = i+1}\hat{z}\right) + a_i\left(\sum_{z\in X, \partial(x,z) = i}\hat{z}\right) + b_{i-1}\left(\sum_{z\in X, \partial(x,z) = i-1}\hat{z}\right)\\
& = \mathrm{RHS}.
\end{align}

\((ii)\) Recall (Lemma \ref{lem:thin-module-structure})

\[Ap_i(A) = p_{i+1}(A) + a_ip_i(A) + b_{i-1}c_ip_{i-1}(A) \quad (0\leq i\leq D).\]
Dividing by \(c_1c_2\cdots c_i\), we have
\[A\frac{p_i(A)}{c_1c_2\cdots c_i} = c_{i+1}\frac{p_{i+1}(A)}{c_1c_2\cdots c_{i+1}} + a_i\frac{p_i(A)}{c_1c_2\cdots c_i} + b_{i-1}\frac{p_{i-1}(A)}{c_1c_2\cdots c_i}.\]
So, \(A_i\), \(p_i(A)/(c_1c_2\cdots c_i)\) satisfy the same recurrence.

Also boundary condition,
\[A_0 = p_0(A) = I.\]
Hence,
\[A_i = \frac{p_i(A)}{c_1c_2\cdots c_i}\quad (0\leq i\leq D).\]

\((iii)\) Since \(E_0, E_1, \ldots, E_D\) form a basis for \(M\), \(\dim M = D+1\).

Observe \(A_0, A_1, \ldots, A_D\in M\) by \((ii)\),
\(A_0, A_1, \ldots, A_D\) are linearly independent, since \(p_0, p_1, \ldots, p_D\) are linearly independent.

Thus, \(A_0, A_1, \ldots, A_D\) form a basis for \(M\).

\((iv)\) \(A_0, A_1, \ldots, A_D\) form a basis for an algebra \(M\),

\begin{equation}
A_iA_j = \sum_{\ell = 0}^Dp^{\ell_{ij}}A_\ell \quad \text{for some }\; p^\ell_{ij}\in\mathbb{C}. \label{eq:plij}
\end{equation}
Fix \(h \quad (0\leq h\leq D)\). Pick \(x, y\in X\) with \(\partial(x,y) = h\).

Compute \(x, y\) entry in \eqref{eq:plij},
\begin{align}
(A_iA_j)_{xy} & = \sum_{z\in X}(A_i)_{xz}(A_j)_{zy}\\
& = \sum_{z\in X, \partial(x,z)=i, \partial(y,z)=j} 1\cdot 1\\
& = |\{z\in X\mid \partial(x,z)=i, \partial(y,z)=j\}|.
\end{align}
On the other hand,
\[\left(\sum_{\ell=0}^D p^\ell_{ij} A_\ell\right)_{xy} = p^h_{ij}(A_h)_{xy} = p^h_{ij}.\]

\((v)\) \(\frac{1}{|X|}J\) is the orthogonal projection onto \(\mathrm{Span}(\delta) = E_0V\). Hence,

\[\frac{1}{|X|} = E_0.\]
This proves the assertions.

\end{proof}

\begin{theorem}
\protect\hypertarget{thm:endpoint1}{}\label{thm:endpoint1}Let \(\Gamma = (X, E)\) be distance-regular with diameter \(D\geq 2\) and intersection numbers \(c_i, a_i, b_i\). Pick a vertex \(x\in X\). Let \(W\) be a thin irreducible \(T(x)\)-module with endpoint \(r = 1\) and diameter \(d\) (\(d = D-2\) or \(D-1\)). Set \(\gamma_0 = a_0(W) + 1\).

\((i)\) The scalars

\begin{equation}
\gamma_i := \frac{c_2c_3\cdots c_{i+1}b_2b_3\cdots b_{i+1}\gamma_0}{x_1(W)x_2(W)\cdots x_i(W)}   \quad (0\leq i\leq d)
\label{eq:gamma}
\end{equation}
\(a_i(W), x_i(W)\) are algebraic integers in \(\mathbb{Q}[\gamma_0]\). In particular, if \(\gamma_0\in \mathbb{Q}\), then \(\gamma_i\), \(a_i(W)\) and \(x_i(W)\) are integers for all \(i\).

\((ii)\) The numbers, \(\gamma_i, a_i(W), x_i(W)\) can all be determined from \(\gamma_0\) and the intersection numbers of \(\Gamma\) in order

\[x_1(W), \gamma_1, a_1(W), x_2(W), \gamma_2, a_2(W), \ldots \]
using \((i)\),
\begin{equation}
x_i(W) = c_ib_i + \gamma_{i-1}(a_i + c_i - c_{i+1} - a_{i-1}(W)) \quad (1\leq i\leq D-1), \label{eq:xi}
\end{equation}
and
\begin{equation}
a_i(W) = \gamma_i - \gamma_{i-1} + a_i + c_i - c_{i+1} \quad (1\leq i\leq D). \label{eq:ai}
\end{equation}
\end{theorem}

\textbf{Note.}
\[p_i = p_1^W + \gamma_{i-1}p^W_{i-1} - c_i(p_{i-1}^W + \gamma_{i-2}^W), \; (\gamma_{-1} = -\gamma_{-2} = 0, \; 0\leq i\leq d+1).\]

\begin{proof}
Set
\[\tilde{A}_i = A_0 + A_1 + \cdots + A_i \quad (0\leq i\leq D).\]

Claim 1. \(A\tilde{A}_i = c_{i+1}\tilde{A}_{i+1}+(a_i-c_{i+1}+c_i)\tilde{A}_i + b_i\tilde{A}_{i-1} \quad (0\leq i\leq D-1).\)

\emph{Proof of Claim 1.}
\begin{align}
\text{LHS} & = \sum_{j=0}^i AA_j\\
& = \sum_{j=0}^i (c_{j+1}A_{j+1} + a_jA_j + b_{j-1}A_{j-1})\\
& = \sum_{j=0}^{i-1}A_j(c_j+a_j+b_j) + A_i(c_i+a_i) + A_{i+1}c_{i+1}\\
& = k(A_0 + \cdots + A_{i-1}) + (a_i+c_i)A_i + c_{i+1}A_{i+1}.
\end{align}
\begin{align}
\text{RHS} & = c_{i+1}(A_0 + A_1 + \cdots + A_{i-1} + A_i + A_{i+1})\\
& \qquad + (a_i - c_{i+1}+c_i)(A_0 + A_1 + \cdots + A_{i-1} + A_i)\\
& \qquad + b_i(A_0 + A_1 + \cdots + A_{i-1})\\
& = k(A_0 + \cdots + A_{i-1}) + A_i(a_i + c_i) + A_{i+1}c_{i+1}.
\end{align}
This proves Claim 1.

Now pick \(0\neq w \in E^*_1(x)W\) and let
\[w = \sum_{z\in X, \partial(x,z)=1}\alpha_z\hat{z}.\]
Pick \(y\), where \(\alpha_y\neq 0\).

For \(i\) \((0\leq i\leq D)\), define
\begin{align}
B_i & = \tilde{A}_i(\hat{x}- \hat{y})\\
& = \sum_{z\in X, \partial(x,z)\leq i}\hat{z} - \sum_{z\in X, \partial(y,z)\leq i} \hat{z}\\
& = \sum_{z\in X, \partial(x,z)=i, \partial(y,z)=i+1}\hat{z} - \sum_{z\in X, \partial(y,z)=i+1, \partial(y,z)=i} \hat{z}.
\end{align}
Note that \(B_D = O\), \(B_0 = \hat{x}-\hat{y}\), and
\[\langle B_0, w_0\rangle = -\alpha_y \neq 0.\]

From Claim 1,
\[AB_i = c_{i+1}B_{i+1}+(a_i-c_{i+1}+c_i)B_i + b_iB_{i-1} \; (0\;eq i\leq D), \; B_{-1} = O.\]
Let \(p_0^W, \ldots, p^W_d\) denote polynomials for \(W\) from Lemma \ref{lem:thin-module-structure}. So,
\[w_i = p_i^W(A)w \in E^*_{1+i}(x)W, \quad (0\leq i\leq d).\]

Claim 2. \(\langle w_i, B_j \rangle = 0\) if \(j\not\in \{i, i+1\}\), \((0\leq i\leq d, 0\leq j\leq D)\).

\emph{Proof of Claim 2.}
\[w_i\in E^*_{1+i}W, \quad B_j\in E^*_j(x)W + E^*_{j+1}(x)W.\]

\begin{center}\includegraphics{t-algebra_files/figure-latex/unnamed-chunk-11-1} \end{center}

Vertical lines indicate possible non-orthogonality.

Compute
\begin{equation}
\langle Aw_i, B_j\rangle = \langle w_i, AB_j\rangle, quad (0\leq i\leq D, \; 0\leq j\leq D-1).\label{eq:wibj}
\end{equation}
\begin{align}
\text{LHS} & = \langle w_{i+1},B_j\rangle + a_i(W)\langle w_i,B_j\rangle + x_i(W)\langle w_{i-1},B_j\rangle\\
\text{RHD}& = b_j\langle w_i, B_{j-1}\rangle + (a_j-c_{j+1}+c_j)\langle w_i, B_j\rangle + c_{j+1}\langle w_i, B_{j+1}\rangle.
\end{align}
Evaluate for \(i = j-2, j-1, j, j+1\).

Set \(i = j-2\).

\begin{center}\includegraphics{t-algebra_files/figure-latex/unnamed-chunk-12-1} \end{center}

Then \eqref{eq:wibj} becomes
\[\langle w_{j-1}, B_j\rangle = b_j\langle w_{j-2},B_{j-1}\rangle \quad (2\leq j\leq D-1).\]
By induction,
\[\langle w_{j-1}, B_j\rangle = b_2b_3\cdots b_j\langle w_{0},B_{1}\rangle \quad (1\leq j\leq D-1).\]
Define
\[\gamma_0 = \frac{\langle w_0, B_1\rangle}{\langle w_0, B_0\rangle}.\]
(We will show \(\gamma_0 = 1+a_0(W)\).)

Then,
\begin{equation}
\langle w_{j-1},B_j\rangle = b_2b_3\cdots b_j\gamma_0\langle w_0, B_0\rangle. \label{eq:wj-1bj}
\end{equation}

Set \(i = j+1\).
Then \eqref{eq:wibj} becomes
\[x_{j+1}(W)\langle w_j, B_j\rangle = c_{j+1}\langle w_0, B_{j+1}\rangle \quad (0\leq j\leq d).\]
Hence,
\begin{equation}
\langle w_j, B_j\rangle = \frac{x_1(W)\cdots w_j(W)}{c_1c_2\cdots c_j}\langle w_0, B_0\rangle \quad (0\leq j\leq d). \label{eq:wjbj}
\end{equation}

Set \(i = j-1\).
Then \eqref{eq:wibj} becomes
\[
\langle w_j, B_j\rangle + a_{j-1}(W)\langle w_{j-1}, B_j\rangle
 = (a_j-c_{j+1}+c_j)\langle w_{j-1},B_j\rangle + b_j\langle w_{j-1},B_{j-1}\rangle.
\]
Evaluate this using \eqref{eq:wj-1bj} and \eqref{eq:wjbj}. \((\langle w_0, B_0\rangle \neq 0)\). Then we have
\[\frac{w_1(W)\cdots x_j(W)}{c_1\cdots c_j}+(a_{j-1}(W)-a_j+c_{j+1}-c_j)b_2\cdots b_j\gamma_0 = b_j\frac{x_1(W)\cdots x_{j-1}(W)}{c_1\cdots c_{j-1}},\]
\[\left(\gamma_i:=\frac{c_2c_3\cdots c_{i+1}b_2b_3\cdots b_{i+1}\gamma_0}{x_0(W)x_2(W)\cdots x_i(W)} \right).\]
\[\frac{x_j(W)}{c_j}  = b_j + \frac{c_1c_3\cdots c_{j-1}b_2b_3\cdots b_{j}\gamma_0}{x_0(W)x_2(W)\cdots x_{j-1}(W)}(a_j+c_j-c_{j+1}-a_{j-1}).\]
So,
\[x_j(W)  = c_jb_j + \gamma_{j-1}(a_j+c_j-c_{j+1}-a_{j-1}(W)).\]
This proves \eqref{eq:xi}.

Set \(i = j\).
Then \eqref{eq:wibj} becomes
\[
a_j(W)\langle w_j, B_j\rangle + x_{j}(W)\langle w_{j-1}, B_j\rangle
 = (a_j-c_{j+1}+c_j)\langle w_{j},B_j\rangle + c_{j+1}\langle w_{j},B_{j+1}\rangle.
\]
\[
(a_j(W) - (a_j-c_{j+1}+c_j))\frac{x_1(W)\cdots x_j(W)}{c_1\cdots c_j}
x_j(W)b_2\cdots b_j\gamma_0 - c_{j+1}b_2\cdots b_{j+1}\gamma_0 = 0.
\]
Thus,
\[a_j(W)-(a_j-c_{j+1}+c_{j}) + \frac{c_1\cdots c_jb_2\cdots b_j\gamma_0}{x_1(W)\cdots x_{j-1}(W)} - \frac{c_1\cdots c_jc_{j+1}b_2\cdots b_{j+1}\gamma_0}{x_1(W)\cdots x_j(W)} = 0,\]
or
\[a_j(W) = a_j + c_j - c_{j+1} - \gamma_{j-1} + \gamma_j.\]
This proves \eqref{eq:ai}.

Also by setting \(i = j = 0\), we have
\begin{align}
a_0(W)\langle w_0, B_0\rangle & = (a_0-c_1+c_0)\langle w_0, B_0\rangle + c_1\langle w_0, B_1\rangle\\
& = -\langle w_0, B_0\rangle + \gamma_0\langle w_0, B_0\rangle.
\end{align}
Hence,
\[\gamma_0 = 1 + a_0(W).\]
Both \(a_i(W)\) and \(x_i(W)\) are algebraic integers, since they are eigenvalues of matrices with integer entries, namely,
\[E^*_{i+1}(x)AE^*_{i+1}(x) \; \text{ and }\; E^*_i(x)AE^*_{i+1}(x)AE^*_i(x).\]

Also \(\gamma_0 = 1+a_0(W)\) is an algebraic integer, and \(\gamma_i - \gamma_{i-1}\) is an algebraic integer by \eqref{eq:xi}.

Hence, \(\gamma_i\) is an algebraic integer by induction.

This completes the proof of Theorem \ref{thm:endpoint1}.
\end{proof}

\begin{example}[D=2]
\[D = 2 \Leftrightarrow \text{strongly regular}.\]
Free parameters are \(k, a_1, c_2\).
Let \(W\) be an irreducible module of endpoint \(1\).
The matrix representation of \(A|_W\) is
\[\begin{pmatrix}
a_0(W) & x_1(W)\\ 1 & a_1(W)
\end{pmatrix}.\]
\(a_0(W)\): free.
\begin{align}
x_1(W) & = c_1b_1 + (a_0(W) + 1)(a_1 + c_1 - c_2 - a_0(W))\\
& = k - a_1 - 1 + a_1a_0(W) + a_0(W) - c_2a_0(W) - a_0(W)^2 + a_1 + a - c_2 - a_0(W)\\
& = a_1a_0(W) - c_2a_0(W) + k - c_2 - a_0(W)^2,\\
\gamma_1 & = 0,\\
a_1(W) & = -(a_0(W)+1) + a_1 + c_1 - c_2\\
& = -a_0(W) + a_1 - c_2.
\end{align}

Then the matrix has eigenvalues \(\theta, \theta_1\).
There is one feasible condition: \(a_0(W)\) is an algebraic integer.
\end{example}

\begin{example}[D=3]
Free parameters \(c_2, c_3, k, a_1, a_2\). The matrix representation becomes
\[
A|_{W} = \begin{pmatrix} a_0(W) & x_1(W) & 0 \\ 1 & a_1(W) & x_2(W)\\ 0 & 1 & a_2(W) \end{pmatrix}.\]
Here, \(a_0(W)\) is free \((=\gamma - 1)\)
\begin{align}
x_1(W) & = k - 1 - a_1 + \gamma_0(a_1 + 1 - c_2 - a_0(W))\\
& = \gamma_0(a_1 - c_2 - a_0(W)) + k - a_1 + a_0(W).
\end{align}
Set
\[\gamma_1(W) = \frac{c_2b_2\gamma_0}{x_1(W)}.\]
\begin{align}
a_1(W) & = \gamma_1 - \gamma_0 + a_1 + 1 - c_2\\
x_2(W) & = \gamma_1(a_2 - c_3 - a_1(W)) + c_2(\gamma_0 + b_1 - a_2 + a_1(W))\\
a_2(W) & = -\gamma_1 + a_2 + c_2 - c_2.
\end{align}
The matrix has eigenvalues, \(\theta, \theta_2, \theta_3\).

There are two feasibility conditions; \(\gamma_0, \gamma_1\) are algebraic integers.

For arbitrary \(D\), there are \(D-1\) feasibility conditions;
\(\gamma_0, \gamma_1, \ldots, \gamma_{D-1}\) are algebraic integers.
\end{example}

\begin{lemma}
\protect\hypertarget{lem:aixigi}{}\label{lem:aixigi}With the notation of Theorem \ref{thm:endpoint1}, suppose
\[f_W = \frac{k-\lambda}{k} \quad (\text{so, }\; a_0(W) = -1).\]
Then,
\begin{align}
a_i(W) & = a_i + c_i - c_{i+1} \quad (0\leq i\leq D-1)\\
x_i(W) & = b_ic_i \quad (1\leq i\leq D-1)\\
\gamma_i(W) & = 0.
\end{align}
\end{lemma}

\begin{proof}
Since \(\gamma_0 = a_0(W) = 1\), \(\gamma_i = 0\).
\end{proof}

\hypertarget{lec17}{%
\chapter{Association Schemes}\label{lec17}}

\textbf{Monday, March 1, 1993}

\textbf{Review}

Let \(\Gamma = (X, E)\) be a distance-regular graph of diameter \(D\geq 2\). Pick a vertex \(x\in X\).

Let \(W\) be a thin irreducible \(T(x)\)-module with endpoint \(r = 1\), diameter \(d = D-1\) or \(D-2\), and \(r_0 = a_(W) + 1\).

Show
\[\gamma_i = \frac{c_2c_2\cdots c_{i+1}b_2b_3\cdots b_{i+1}\gamma_0}{x_1(W)\cdots x_i(W)},\]
\(a_i(W)\) and \(x_i(W)\) are all algebraic integers in \(\mathbb{Q}[\gamma_0]\), where
\begin{align}
x_i(W) & = c_ib_i + \gamma_{i-1}(a_i + c_i - c_{i+1}-a_{i-1}(W)) && (1\leq i\leq d)\\
a_i(W) & = \gamma_i - \gamma_{i-1} + a_i + c_i - c_{i+1} && (1\leq i\leq d)
\end{align}

Certainly, \(x_i(W)\), \(\gamma_i\), and \(a_i(W)\) are in \(\mathbb{Q}[\gamma_0]\) by the above lines and so on.
\[\gamma_0 \to a_0(W) \to x_1(W) \to \gamma_1 \to a_1(W)\to x_1(W) \to \cdots .\]
Recall some \(B\in \mathrm{Mat}_n(\mathbb{C})\) is integral whenever
\[B\in \mathrm{Mat}_n(\mathbb{Z}).\]
In this case, the characteristic polynomial
\[\det(\lambda I - B) = \lambda^n + \alpha_{n-1}\lambda^{n-1} + \cdots + \alpha_0, \quad \text{some }\; \alpha_0, \ldots, \alpha_{n-1}\in \mathbb{Z}.\]
Hence, eigenvalues of \(B\) are algebraic integers.
But \(a_i(W)\) is an eigenvalue of an integral matrices,
\[B = E^*_{i+1}(x)AE^*_{i+1}(x).\]
Hence, \(a_i(W)\) is an algebraic integer.

Also, \(x_i(W)\) is an eigenvalue of an integral matrix
\[B = E^*_i(x)AE^*_{i+1}(x)AE^*_i(x).\]
So \(x_i(W)\) is an algebraic integer.
\[\gamma_i - \gamma_{i-1} = a_i(W) - a_i - c_i + c_{i+1}\]
is an algebraic integer.

Since \(\gamma_0 = a_0(W) + 1\) is an algebraic integer, we find \(\gamma\) is an algebraic integer for all \(i\).

\begin{definition}
\protect\hypertarget{def:association-scheme}{}\label{def:association-scheme}A (commutative) association scheme\index{association scheme} is a configuration \(Y = (X, \{R_i\}_{0\leq i\leq D})\), where \(X\) is a finite nonempty set (of vertices),
\(R_0, R_1, \ldots, R_D\) are nonempty subsets of \(X\times X\) such that

\((i)\) \(R_0 = \{(x,x)\mid x\in X\}\),

\((ii)\) \(R_0 \cup \cdots \cup R_D = X\times X \quad (\text{disjoint union})\),

\((iii)\) for every \(i\), \(R_i^\top = \{(y,x)\mid xy\in R\} = R_{i'}\) some \(i'\in \{0,1,\ldots, D\}\),

\((iv)\) for every \(h, i, j\) (\(0\leq h, i, j\leq D\)), and every \(x,y\in X\) such that \((x,y)\in R_h\),

\[p^h_{ij} = |\{z\in X\mid (x,z)\in R_i, \; (z,y)\in R_j\}|\]
depends only on \(h, i, j\) and not on \(x,y\); and

\((v)\) \(p^h_{ij} = p^h_{ji}\) for all \(h,i, j\).

If \(i' = i\) for all \(i\), we say \(Y\) is symmetric\index{symmetrix}.
We call \(D\) the class of scheme and \(R_i\), the \(i\)th relation of \(Y\). We say vertices \(x,y\in X\) are \(i\)-related, or `at distance \(i\)', whenever \((x,y)\in R_i\).

We always assume that a `scheme' is a commutative association scheme.
\end{definition}

Let \(Y = (X, \{R_i\}_{0\leq i\leq D})\) be an association scheme.

\begin{definition}
\protect\hypertarget{def:incidencemat-of-as}{}\label{def:incidencemat-of-as}

The \(i\)-the association matrix\index{association matrix} \(A_i\in \mathbb{Mat}_X(\mathbb{C})\)
\begin{align}
(A_i)_{xy} & = \begin{cases} 1 & \text{if}\; (x,y)\in R_i\\
0 & \text{if}\; (x,y)\not\in R_i,\end{cases} && (x,y\in X, 0\leq i\leq D)
\end{align}
Then,

\((i')\) \(A_0 = I\).

\((ii')\) \(A_0 + A_1 + \cdots + A_D = J\) (= all 1's matrix).

\((iii')\) \(A_i^\top = A_{i'}\) (\(0\leq i\leq D\)).

\((iv')\) \({\displaystyle A_iA_j = \sum_{h=0}^D p^h_{ij}A_h\quad }\) \((0\leq i,j\leq D)\).

\((v')\) \(A_iA_j = A_jA_i\).

\(M := \mathrm{Span}_{\mathbb{C}}(A_0, \ldots, A_D)\) (Bose-Mesner algebra of \(Y\)) is a commutative \(\mathbb{C}\)-algebra of dimension \(D+1\).

\end{definition}

Observe:
\[Y \text{ is symmetric} \leftrightarrow A_i^\top = A_i \text{ for all } i \leftrightarrow M \text{ is symmetric}.\]

\begin{example}
\protect\hypertarget{exm:dr}{}\label{exm:dr}Let \(\Gamma = (X, E)\) be distance-regular of diameter \(D\). Set
\begin{align}
R_i & = \{(x,y)\mid \partial(x,y) = i\} && (0\leq i\leq D).
\end{align}
Then,
\[Y = (X, \{R_i\}_{0\leq i\leq D})\]
is a symmetric scheme.
\[i\text{-th association matrix} = i\text{-th distance matrix} \quad \text{for all $i$.}\]
\end{example}

\begin{example}
\protect\hypertarget{exm:gen-tr}{}\label{exm:gen-tr}Suppose a group \(G\) acts transitively on a seet \(X\). Assume \(G\) is generously transitive, i.e.,
\[\text{for all }x,y\in X, \text{ there exists }g\in G \text{ such that }gx = y, gy = x.\]
Then \(G\) acts on \(X\times X\) by rule;
\[g(x,y) = (gx, gy), \quad \text{for all }\; g\in G, \text{ and for all }x,y\in X.\]
Let \(R_0, \ldots, R_D\) denote orbits of \(G\) on \(X\times X\).

Observe that \(R_i^\top = R_i\) for all \(i\) by generously transitivity, and
\[Y = (X, \{R_i\}_{0\leq i\leq D})\]
is a symmetric scheme.
\end{example}

\begin{exercise}
\protect\hypertarget{exr:gen-tr-case}{}\label{exr:gen-tr-case}In Example Example \ref{exm:gen-tr}, Bose-Mesner algebra
\begin{align}
M & = \{B\in \mathrm{Mat}_X(\mathbb{C}) \mid Bg = gB, \text{ for all }g\in G\}\\
& = \text{the commuting algebra of $G$ on $X$.}
\end{align}
Here, we view each \(g\in G\) as a permutation matrix in \(\mathrm{Mat}_X(\mathbb{C})\) satisfying
\[g\hat{x} = \widehat{gx}, \quad \text{for all }x\in G.\]
\end{exercise}

\begin{example}
\protect\hypertarget{exm:centralizer-alg}{}\label{exm:centralizer-alg}Let \(G\) be any finite group. \(G\) acts on \(X = G\) by conjugation.
\[G\times X \to X, \quad (g,x)\mapsto gxg^{-1}.\]
Let \(C_0, C_1, \ldots, C_D\) denote orbits (i.e., conjugacy classes), and let \(C_0 = \{1_G\}\).
Claim that \(Y = (X, \{R_i\}_{0\leq i\leq D})\) is a commutative scheme (not symmetric in general).

\((i)\) \(R_0 = \{xx\mid x\in X\}\) as \(C_0 = \{1_G\}\).

\((ii)\) \(R_0, \ldots, R_D\) is a partition of \(X\times X\) since \(C_0, \ldots, C_D\) is a partition of \(X = G\).

\((iii)\) \(R_i^\top = R_{i'}\), where \(C_{i'} = \{g^{-1}\mid g\in C_i\}\).

\((iv)\) Set \(H = G\oplus G\), the direct sum. Then \(H\) acts on \(X = G\):

\[\text{for all }h = (g,gz), \text{for all }x\in X, \quad h(x) = gx(gx)^{-1} = gxz^{-1}g^{-1}.\]
\[R_i = \{(x,y)\mid x^{-1}y\in C_i\}, \; h_i\in C_i, \; x^{-1}y = gh_ig^{-1}.\]
\begin{align}
(x,y) & = (x, xgh_ig^{-1})\\
& = (xgg^{-1}, xgh_ig^{-1})\\
& = (xg, g)(1,h_i).
\end{align}
So, \(R_0, \ldots, R_D\) are the orbits of \(H\) on \(X\times X\).

\((v)\) \(p^h_{ij} = p^h_{ji}\)?

Fix \(i,j, h\) and \(x, y\in X\) with \((x,y)\in R_h\). Set
\begin{align}
S & = \{z\in X\mid (x,z)\in R_i, \; (z,y)\in R_j\}\\
T & = \{z\in X\mid (x,z)\in R_j, \; (z,y)\in R_i\}.
\end{align}
Show \(|S| = |T|\).
\[\text{For all }z\in S, \text{ set } \hat{z} = xz^{-1}y.\]
Observe, \(\hat{z}\in T\).
\begin{align}
x^{-1}z\in C_i & x^{-1}\hat{z} = x^{-1}xz^{-1}y\in C_j\\
z^{-1}y\in C_j & \hat{z}^{-1}y = y^{-1}zx^{-1}x^{-1}y = y^{-1}x(x^{-1}z)x^{-1}y \in C_i.
\end{align}
Observe
\[S\to T \quad (z\mapsto z^{-1}) \quad \text{is one-to-one and onto.}\]
\end{example}

\hypertarget{lec18}{%
\chapter{Polynomial Schemes}\label{lec18}}

\textbf{Wednesday, March 3, 1993}

\begin{lemma}
\protect\hypertarget{lem:dr-scheme}{}\label{lem:dr-scheme}Let \(Y = (X, \{R_i\}_{0\leq i\leq D})\) denote the symmetric scheme with associated matrices \(A_0, A_1, \ldots, A_D\). Then the following are equivalent.

\((i)\) The graph \(\Gamma = (X, R_1)\) is distance-regular, and \(R_0, \ldots, R_D\) are labelled so that

\[R_i = \{xy\mid \partial(x,y) = i\}.\]

\((ii)\) There exists \(f_i\in \mathbb{C}[\lambda]\), \(\deg f_i = i\) such that \(f_i(A_1) = A_i\) for all \(i\) with \(0\leq i\leq D\).

\((iii)\) The parameter \(p^h_{ij}\)

\[\begin{cases} = 0 & \text{if one of $h, i, j$ is larger than the sum of the other two}\\
\neq 0 & \text{if one of $h,i,j$ is equal to the sum of the other two.}\end{cases}\]
\end{lemma}

\begin{proof}
\leavevmode

\((i)\Rightarrow (ii)\): Lemma \ref{lem:incidence-matrices}.

\((ii)\Rightarrow (iii)\): Define

\[k_i \equiv p^0_{ii} = |\{z\mid z]in X, \; \partial(x,z) = i \; ((x,z)\in R_i)\}|\]
for any \(x\in X\).
Then \(k_i \neq 0\) \((0\leq i\leq D)\), \(k_0 = 1\).

(By symmetricity, \((x,y)\in R_i\) if and only if \((y,x)\in R_i\).)

Claim.
\begin{align}
k_hp^h_{ij} & = k_ip^i_{hj} = k_jp^j_{ih}\\
& = |X|^{-1}|\{xyz\in X^3\mid \partial(x,y) = h, \partial(x,z) = i, \partial(y,z) = j\}|.
\end{align}
\emph{Pf.}
The number of \(xyz\in X^3\), \(\partial(x,y) = h, \partial(x,z) = i, \partial(y,z) = j\) is equal to
\[|X|k_hp^h_{ij} = |X|k_ip^i_{hj} = k_jp^j_{ih}.\]

In particular,
\[p^h_{ij} = 0 \leftrightarrow p^i_{hj} =0 \leftrightarrow p^j_{ih} = 0.\]
Hence, it suffices to show
\[\begin{cases}
p^h_{ij} = 0 & \text{if }\; h > i+j\\
p^h_{ij} \neq 0 & \text{if }\; h = i+j.
\end{cases}\]

Fix \(i,j\). Without loss of generality, we may assume that \(i+j\leq D\) as trivial otherwise.
\[f_i(A)f_j(A) = A_iA_j = \sum_{\ell = 0}^Dp^{\ell}_{ij}A_\ell = \sum_{\ell=0}^Dp^\ell_{ij}f_\ell(A).\]
\begin{align}
i + j & = \deg \mathrm{LHS}\\
& = \deg \mathrm{RHS}\\
& = \max\{\ell\mid p^\ell_{ij}\neq 0\}.
\end{align}

\((iii)\Rightarrow (i)\)

Let \(A = A_1\), and consider a graph \(\Gamma\) with adjacency matrix \(A\).
\begin{align}
AA_j & = \sum_{h}p^h_{1j}A_h\\
& = p^{j+1}_{1j} A_{j+1} + p^j_{1j}A_j + p^{j-1}_{1j}A_{j-1}.
\end{align}

Then, \(p^{j+1}_{1j} \neq 0 \neq p^{j-1}_{1j}\).

Fix a vertex \(x\in X\), and set \(R_i(x) = \{y\mid (x,y)\in R_i\}\).

Then each \(y\in R_i(x)\) is adjacent in \(\Gamma\) to exactly
\begin{align}
p^i_{1,i+1} & \neq 0  \quad \text{vertices in }\; R_{i}(x),\\
p^i_{1i} & \qquad \text{vertices in }\; R_{i+1}(x),\\
p^i_{1,i-1} & \neq 0 \quad \text{vertices in }\; R_{i-1}(x).
\end{align}
Hence, by induction,
\begin{align}
R_i(x) & = \{y\mid \partial(x,y) = i \text{ in }\Gamma\} && (0\leq i\leq D),
\end{align}
and \(\Gamma\) is distance regular.

\end{proof}

\hypertarget{lec19}{%
\chapter{Commutative Association Schemes}\label{lec19}}

\textbf{Friday, March 5, 1993}

\begin{lemma}
\protect\hypertarget{lem:ei}{}\label{lem:ei}

Let \(Y = (X, \{R_i\}_{0\leq i\leq D})\) be a commutative scheme with Bose-Mesner algebra \(M\).

Then there exists a basis \(E_0, E_1, \ldots, E_D\) for \(M\) such that

\((i)\) \(E_0 = |X|^{-1}J\).

\((ii)\) \(E_iE_j = E_jE_i = \delta_{ij}E_i\) \(\quad (0\leq i,j\leq D)\).

\((iii)\) \(E_0 + E_1 + \cdots + E_D = I\).

\((iv)\) \(E_i^\top = \overline{E_i} = E_{\hat{i}}\) for some \(\hat{i}\in \{0, 1, \ldots, D\}\).

\end{lemma}

\begin{proof}
\(M\) acts on Hermitean space \(V = \mathbb{C}^n\) \((n = |X|)\).

If \(W\) is an \(M\)-module, so is \(W^\bot\).

Each irreducible \(M\)-module is \(1\) dimensional by commutativity of \(M\). So \(V\) is orthognal direct sum of \(1\)-dimensional \(M\)-modules.

Let \(v_1, \ldots, v_n\) be an orthonormal basis for \(V\) consisiting of eigenvectors for all \(m\in M\).

Set \(P\in \mathrm{Mat}_X(\mathbb{C})\) so that the \(i\)-th column of \(P\) is equal to \(v_i\). So,
\[\bar{P}^\top P = I = P\bar{P}^\top = \bar{P}P^\top,\]
and \(P\) is unitary.

Also, for all \(m\in M\),
\begin{align}
P^{-1}mP & = \text{diagonal}\\
& = \mathrm{diag}(\theta_1(m), \ldots, \theta_n(m)).
\end{align}
for some functions
\[\theta_i: M \longrightarrow \mathbb{C}.\]
Observe: each \(\theta = \theta_i\) is a character of \(M\), i.e.,
\[\theta: M\longrightarrow \mathbb{C}\]
is a \(\mathbb{C}\)-algebra homomorphism.

Observe: the \(\theta_1, \ldots, \theta_n\) are not all distinct.

Let \(\sigma_0, \ldots, \sigma_r\) denote distinct elements of
\[\theta_1, \ldots, \theta_n.\]
Say \(\sigma_i\) appears \(m_i\) times.
Without loss of generality, we may assume that
\[P^{-1}mP = \begin{pmatrix}\sigma_0(m)I_{m_0} & O & O &  O \\
O & \sigma_1(m)I_{m_1} & O &  O\\
O & O & \ddots & O \\
O & O & O & \sigma_r(m)I_{m_r}
\end{pmatrix}.\]
Set
\[E_i = P\begin{pmatrix} O & O & O\\
O & I_{m_i} & O\\
O & O & O \end{pmatrix}P^{-1},\]
where \(I_{m_i}\) is in the \(i\)-th block.

Then,
\[E_iE_j = \delta_{ij}E_i \quad (0\leq i,j\leq r),\]
\[E_0 + E_1 + \cdots + E_r = I.\]
Hence for all \(m\in M\),
\[m = \sum_{i=0}^r \sigma_i(m)E_i \in \mathrm{Span}(E_0, \ldots, E_r).\]
So,
\[M \subseteq \mathrm{Span}(E_0, \ldots, E_r).\]
Since \(E_0,\ldots, E_r\) are linearly independent, \(r\geq D\).

Show \(E_i\in M\).

Claim 1. For all distinct \(i, j\) \(\quad (0\leq i, j\leq D)\), there exists \(m\in M\) such that \(\sigma_i(m)\neq 0\), \(\sigma_j(m)=0\).

\emph{Pf of Claim 1.}
\(\sigma_i\neq \sigma_j\) implies that there exists \(m'\in M\) such that \(\sigma_i(m')\neq \sigma_j(m')\).

Set \(m = m'-\sigma_j(m')I\). Then,
\begin{align}
\sigma_j(m) & \sigma_j(m') - \sigma_j(m') & = 0,\\
\sigma_i(m) & \sigma_i(m') - \sigma_j(m') & \neq 0.
\end{align}

Claim 2. \(E_i\in M\) \(\quad (0\leq i \leq D)\).

\emph{Pf of Claim 2.}
Fix a vertex \(x\in X\). For all \(j\neq i\), there exists \(m_j\in M\) such that
\(\sigma_i(m_j)\neq 0, \quad \sigma_j(m_j) = 0, \quad i\neq j.\)\$
Observe
\[s = \sigma_i\left(\prod_{\ell\neq i}m_\ell\right) \neq 0.\]
Set
\[m^* = \sigma_i\left(\prod_{\ell\neq i}m_\ell\right) s^{-1}.\]
Observe
\[\sigma_i(m^*) =1, \quad \sigma_j(m^*) = 0, \quad \text{for all }j\neq i \quad (0\leq j\leq D).\]
So
\[P^{-1}m^*P = \begin{pmatrix} O & O & O\\
O & I_{m_i} & O\\
O & O & O \end{pmatrix}.\]
We have
\[E_i = m^*\in M.\]
Now \(r = D\), \(M = \mathrm{Span}(E_0, \ldots, E_D)\) and \(E_0, \ldots, E_D\) is a basis for \(M\).

Observe
\[P^{-1}E_iP = \begin{pmatrix} O & O & O\\
O & I_{m_i} & O\\
O & O & O \end{pmatrix}\]
implies
\[P^{-1}\overline{E_i}^\top P = \bar{P}^\top \overline{E_i}^\top \overline{P^{-1}}^\top = \begin{pmatrix} O & O & O\\
O & I_{m_i} & O\\
O & O & O \end{pmatrix}^\top = P^{-1}E_i P.\]
Hence,
\[\overline{E_i}^\top = E_i.\]
\(E_0^\top, \ldots, E_D^\top\) are nonzero matrices satisfying
\[E_i^\top E_j^\top = \delta_{ij}E_i^\top,\]
\[E_0^\top + E_1^\top + \cdots + E_D^\top = I.\]
Each \(E_i^\top\) is a linear combination of \(E_0, \ldots, E_D\) with coefficientss that are \(0\) or \(1\), and for no two \(E_i\)'s are coefficients of any \(E_j\) both \(1\)'s.

So, \(E_0^\top, \ldots, E_D^\top\) is a permutation of \(E_0, \ldots, E_D\).

Observe \(J = A_0 + \cdots + A_D\in M\).

The matrix \(|X|^{-1}J\) is an idempotent of rank \(1\).

So, without loss of generality we may assume that
\[E_0 = \frac{1}{|X|}J.\]
We have the assertions.
\end{proof}

Define entry-wise product \(\circ\) on \(\mathrm{Mat}_X(\mathbb{C})\).
\[A_i \circ A_j = \delta_{ij}A_i.\]
So, \(M\) is closed under \(\circ\).
\[E_i \circ E_j = \frac{1}{|X|}\sum_{h=0}^D q^h_{ij}E_h.\]
The numbers \(q^h_{ij}\) is called Krein parameters of \(Y\).

Claim. \(q^h_{ij}\in \mathbb{R}\).

\emph{Pf.}
\begin{align}
\frac{1}{|X|}\sum_{h=0}^D \overline{q^h_{ij}}E_h & = \frac{1}{|X|}\sum_{h=0}^D \overline{q^h_{ij}}\overline{E_h}^\top \\
& = (\overline{E_i\circ E_j})^\top \\
& = E_i\circ E_j \\
& = \frac{1}{|X|}\sum_{h=0}^D q^h_{ij}E_h.
\end{align}
Hence, \(q^h_{ij} = \overline{q^h_{ij}}\).

Observe
\(A_0, \ldots, A_D\), \(E_0, \ldots, E_D\) are bases for \(M\). Hence, there exist \(p_i(j)\), \(q_i(j)\in \mathbb{C}\) such that
\begin{align}
A_i & = \sum_{j = 0}^D p_i(j)E_j\\
E_i & = \frac{1}{|X|}\sum_{j=0}^D q_i(j)A_j.
\end{align}
Taking transpose and conjugate we find,
\begin{align}
\overline{p_i(j)} & =  p_i(j)  =  p_{i'}(\hat{j}) && (0\leq i,j\leq D)\\
\overline{q_i(j)} & =  q_i(j)  =  q_{\hat{i}}({j}') && (0\leq i,j\leq D).
\end{align}

Fix a vertex \(x\in X\). Define
\[E^*_i \equiv E^*_i(x) \in \mathrm{Mat}_X(\mathbb{C})\]
to be a diagonal matrix such that
\[(E^*_i)_{xy} = \begin{cases} 1 & \text{if } (x,y)\in R_i\\ 0 & \text{if } (x,y)\not\in R_i
\end{cases} \quad (0\leq i\leq D, y\in X.)\]
Then,
\[E^*_iE^*_j = \delta_{ij}E^*_i,\]
\[E^*_0 + \cdots + E^*_D = I,\]
\[(E^*_i)^\top = \overline{E^*_i} = E^*_i.\]

\begin{definition}
\protect\hypertarget{def:dual-bose-mesner}{}\label{def:dual-bose-mesner}Dual Bose-Mesner algebra \index{dual Bose-Mesner algebra} \(M^* \equiv M^*(x)\) with respect to \(x\) is
\[\mathrm{Span}(E^*_0, \ldots, E^*_D).\]
\end{definition}

Define dual associate matrices\index{dual associate matrix} \(A_0^*, \ldots, A^*_D\).
Indeed \(A^*_i \equiv A^*_i(x)\in \mathrm{Mat}_X(\mathbb{C})\) is a diagonal matrix with
\[(A_i^*)_{yy} = |X|(E_i)_{xy}\quad (y\in X).\]
\(A^*_i\) is a diagonal matrix having the row \(x\) of \(E_i^*\) on the diagonal.

Observe
\begin{align}
A^*_i & = \sum_{j=0}^Dq_i(j)E^*_j \quad \left(E_i = \frac{1}{|X|}\sum_{j=0}^D q_i(j)A_j\right)\\
E^*_i & = \frac{1}{|X|}\sum_{j=0}^D p_i(j)A^*_j \quad \left(A_i = \sum_{j=0}^D p_i(j)E_j\right).
\end{align}
So, \(A^*_0, \ldots, A^*_D\) form a basis for \(M^*\).

Also,
\[A^*_iE^*_j = q_i(j)E^*_j.\]
\[\left(A^*_iE^*_j = \sum_{h=0}^D q_i(h)E^*_hE^*_j = q_i(j)E^*_j.\right)\]
So, \(q_i(j)\) are dual eigenvalues of \(A^*_i\).

Observe,
\[A^*_0 = I, \quad A^*_0 + \cdots + A^*_D = |X|E^*_0, \quad \overline{A^*_i} = A^*_{\hat{i}},\]
\[A^*_iA^*_j = \sum_{h=0}^D q^h_{ij}A^*_h \quad (0\leq i,j\leq D).\]

\begin{remark}
\emph{Proof.}
\[(A^*_0)_{yy} = |X|(E_0)_{xy} = (J)_{xy} = 1.\]
\[A^*_0 + \cdots + A^*_D = \sum_{i=0}^D\sum_{j=0}^D q_i(j)E^*_j = |X|E^*_0.\]
Note that
\[I = E_0 + \cdots + E_D = \frac{1}{|X|}\sum_{i=0}^D\sum_{j=0}^D q_i(j)A_j.\]
\[\sum_{i=0}^D q_i(j) = \delta_{j0}|X|.\]
\[\overline{A^*_i} = \sum_{j=0}^D\overline{q_i(j)}E^*_j = \sum_{j=0}^D q_{\hat{i}}(j)E^*_j = A^*_{\hat{i}}.\]
\begin{align}
(A^*_iA^*_j)_{yy} & = |X|^2 (E_i)_{xy}(E_j)_{xy}\\
& = |X|^2(E_i\circ E_j)_{xy}\\
& = |X|\sum_{h=0}^D q^h_{ij}(E_h)_{xy}\\
& = \sum_{h=0}^D q^h_{ij}(A^*_h)_{yy}.
\end{align}
\end{remark}

The following statements will be proved after a couple of lemmas in the next lecture.

\textbf{Lemma.}
Let \(Y = (X, \{R_i\}_{0\leq i\leq D})\) be a commutative scheme. Fix a vertex \(x\in X\), and set \(E^*\equiv E^*_i(x)\) and \(A^*_i \equiv A^*(x)\).
Then the following hold.

\((i)\) \(E^*_iA_jE^*_k = O\) if and only if \(p^k_{ij} = 0\) for \(0\leq i,j,k\leq D\).

\((ii)\) \(E_iA^*_jE_k = O\) if and only if \(q^k_{ij} = 0\) for \(0\leq i,j,k\leq D\).

\hypertarget{lec20}{%
\chapter{Vanishing Conditions}\label{lec20}}

\textbf{Monday, March 15, 1993} (Monday after Spring break)

\begin{lemma}
\protect\hypertarget{lem:pij-qij}{}\label{lem:pij-qij}Let \(Y = (X, \{R_i\}_{0\leq i\leq D})\) be a commutative scheme.

\((i)\) \(p_0(i) = 1\).

\((ii)\) \(p_i(0) = k_i\), where

\[k_i = p^0_{ii'} = |\{y\in X\mid (x,y)\in R_i\}|.\]

\((iii)\) \(q_0(i) = 1\).

\((iv)\) \(q_i(0) = m_i\), where

\[m_i = \mathrm{rank} E_i.\]
\end{lemma}

\begin{proof}
\leavevmode

\((i)\) Since \(A_0 = I\) and

\begin{align}
A_0 & = p_0(0)E_0 + p_0(1)E_1 + \cdots + p_0(D)E_D\\
I & = E_0 + E_1 + \cdots + E_D,
\end{align}
\(p_0(i) = 1\) for all \(i\).

\((ii)\) Since

\[A_i = p_i(0)E_0 + p_i(1)E_1 + \cdots + p_i(D)E_D,\]
\(A_i E_0 = p_i(0)E_0\), and
\[k_i J = A_i J = p_i(0)J\]
as there are \(k_i\) \(1\)'s in each row of \(A_i\), we have \(k_i = p_i(0)\).

\((iii)\) Since \(E_0 = |X|^{-1}J\) and

\begin{align}
E_0 & = |X|^{-1}(q_0(0)A_0 + q_0(1)A_1 + \cdots + q_0(D)A_D)\\
|X|^{-1}J & = |X|^{-1}(A_0 + A_1 + \cdots + A_D),
\end{align}
\(q_0(i) = 1\) for all \(i\).

\((iv)\) \(E_i = |X|^{-1}(q_i(0)A_0 + q_i(1)A_1 + \cdots + q_i(D)A_D)\), \(E_i^2 = E_i\), and \(E_i\) is similar to a matrix

\[\begin{pmatrix} I_{m_i} & O \\ O & O\end{pmatrix}.\]
So,
\[m_i = \mathrm{rank}E_i = \mathrm{trace} E_i = \sum_{x\in X}(E_i)_{xx} = |X||X|^{-1}q_i(0) = q_i(0).\]
Note that as
\[E_i = \frac{1}{|X|}\sum_{j=0}^D q_i(j)A_j \to (E_i)_{xx} = \frac{1}{|X|}q_i(0)(A_0)_{xx}.\]
Hence, we have all formulas.

\end{proof}

\begin{lemma}
\protect\hypertarget{lem:phijqhij}{}\label{lem:phijqhij}

With the above notation

\((i)\) \(p^h_{ij} = p^{h'}_{j'i'}\).

\((ii)\) \(k_hp^h_{ij} = k_jp^j_{i'h} = k^i_{hj'}\).

\((iii)\) \(q^h_{ij} = q^{\hat{h}}_{\hat{j}\hat{i}}\).

\((iv)\) \(m_hq^h_{ij} = m_jq^j_{\hat{i}h} = m_iq^i_{h\hat{j}}.\)

\end{lemma}

\begin{proof}
\leavevmode

\((i)\) We have

\begin{align}
\sum_{h = 0}^D p^h_{ij} A_{h'} & \left(\sum_{h=0}^D p^h_{ij}A_h\right)^\top \\
 & = (A_iA_j)^\top  \\
& = A_j^\top A_i^\top\\
& = A_{j'}A_{i'} \\
& = \sum_{h=0}^D p^{h'}_{j'i'}A_h'.
\end{align}

\((ii)\) Count the following number,

\begin{align} 
& |\{xyz\in X^3 \mid (x,y)\in R_h, (x,z)\in R_i, (z,y)\in R_j\}| \\
& \quad =  |X|k_hp^h_{ij} = |X|k_jp^j_{i'h} = |X|k^i_{hj'}.
\end{align}

\((iii)\)

\begin{align}
\frac{1}{|X|}\sum_{h = 0}^D q^h_{ij} E_{\hat{h}} & = \left(\frac{1}{|X|}\sum_{h=0}^D q^h_{ij}E_h\right)^\top \\
 & = (E_i\circ E_j)^\top  \\
& = E_j^\top \circ E_i^\top\\
& = E_{\hat{j}}E_{\hat{i}} \\
& = \frac{1}{|X|}\sum_{h=0}^D q^{\hat{h}}_{\hat{j}\hat{i}}E_{\hat{h}}.
\end{align}

\((iv)\) Let \(\tau(B)\) denote the sum of the entries in the matrix \(B\).

Observe: \(\tau(B\circ C) = \mathrm{trace}(BC^\top)\).

Observe
\[\tau(E_i\circ E_j \circ E_{\hat{k}}) = \tau((E_i\circ E_j\circ E_{\hat{k}})^\top) = \tau(E_{\hat{i}}\circ E_k \circ E_{\hat{j}}) = \tau(E_k\circ E_{\hat{j}}\circ E_{\hat{i}}).\]
Compute each one.
\begin{align}
\tau(E_i\circ E_j \circ E_{\hat{k}}) & = \mathrm{trace}((E_i\circ E_j)E_k) = \mathrm{trace}\left(\left(\frac{1}{|X|}\sum_{h} q^h_{ij}E_h\right)E_k\right)\\
& = \mathrm{trace}\left(\frac{1}{|X|} q^k_{ij}E_k\right) = \frac{1}{|X|}m_kq^k_{ij},\\
\tau(E_{\hat{i}}\circ E_k \circ E_{\hat{j}}) & = \mathrm{trace}((E_{\hat{i}}\circ E_k)E_{\hat{j}}) = \mathrm{trace}\left(\left(\frac{1}{|X|}\sum_{h} q^h_{\hat{i}k}E_h\right)E_{\hat{j}}\right)\\
& = \mathrm{trace}\left(\frac{1}{|X|} q^j_{\hat{i}k}E_k\right) = \frac{1}{|X|}m_jq^j_{\hat{i}k},\\
\tau(E_k\circ E_{\hat{j}}\circ E_{\hat{i}}) & = \mathrm{trace}((E_k\circ E_{\hat{j}})E_i) = \mathrm{trace}\left(\left(\frac{1}{|X|}\sum_{h} q^h_{k\hat{j}}E_h\right)E_i\right)\\
& = \mathrm{trace}\left(\frac{1}{|X|} q^i_{k\hat{j}}E_i\right) = \frac{1}{|X|}m_iq^i_{k\hat{j}}.
\end{align}
Hence, we have \((iv)\).

\end{proof}

\begin{lemma}
\protect\hypertarget{lem:vanishing-condition}{}\label{lem:vanishing-condition}

Let \(Y = (X, \{R_i\}_{0\leq i\leq D})\) be a commutative scheme. Fix a vertex \(x\in X\), and set \(E^*\equiv E^*_i(x)\) and \(A^*_i \equiv A^*(x)\).
Then the following hold.

\((i)\) \(E^*_iA_jE^*_k = O\) if and only if \(p^k_{ij} = 0\) for \(0\leq i,j,k\leq D\).

\((ii)\) \(E_iA^*_jE_k = O\) if and only if \(q^k_{ij} = 0\) for \(0\leq i,j,k\leq D\).

\end{lemma}

\begin{proof}
\leavevmode

\((i)\) Partition rows and columns by \(R_0(x), R_1(x), \ldots, R_D(x)\). Then,

\[E^*_i(x)A_j E^*_h(x)\]
is the \((i,h)\) block of \(A_j\).

Hence this submatrix is zero if and only if there exists no \(y,z\in X\) such that \((x,y)\in R_i\), \((x,z)\in R_h\) and \((y,z)\in R_j\). This is exactly when \(p^h_{ij} = 0\).

\((ii)\) The sum of the squares of norms of entries in \(E_iA^*_jE_k\)

\begin{align}
& = \tau((E_iA^*_jE_k)\circ (\overline{E_jA^*_jE_k}))\\
& = \mathrm{trace}(E_iA^*_jE_k(\overline{E_jA^*_jE_k})^\top)\\
& = \mathrm{trace}(E_iA^*_jE_kA^*_{\hat{j}}E_i)\\
& = \mathrm{trace}(E_iA^*_jE_kA^*_{\hat{j}}) && \text{as $\mathrm{trace}(XY) = \mathrm{trace}(YX)$}\\
& = \sum_{y\in X}(E_iA^*_jE_kA^*_{\hat{j}})_{yy}\\
& = \sum_{y\in X}\left(\sum_{z\in X} (E_i)_{yz}(A^*_j)_{zz}(E_k)_{zy}(A^*_{\hat{j}})_{yy}\right)\\
& = \sum_{y\in X}\left(\sum_{z\in X} (E_{\hat{i}})_{zy}(|X|(E_j)_{xz})(E_k)_{zy}(|X|(E_j)_{yx})\right)\\
& = |X|^2(E_j(E_{\hat{i}}\circ E_k))E_j)_{xx}\\
& = |X|q^j_{\hat{i}k}(E_j)_{xx}\\
& = q^j_{\hat{i}k}m_j \\
& = m_kq^k_{ij}.
\end{align}
Note that since \(|X|E_j = q_j(0)A_0 + q_j(1)A_1 + \cdots q_j(D)A_D\),
\[(E_j)_{xx} = \frac{1}{|X|}q_j(0) = \frac{m_j}{|X|}.\]
Thus, we have \((ii)\).

\end{proof}

\begin{corollary}[Krein Condition]
\protect\hypertarget{cor:qhij}{}\label{cor:qhij}For any commutative scheme \(Y = (X, \{R_i\}_{0\leq i\leq D})\), \(q^h_{ij}\) is a non-negative real number for \(0\leq h, i, j\leq D\).
\end{corollary}

\begin{proof}
Since \(q^h_{ij}m_h\) is a non-negative real by the proof of Lemma \ref{lem:vanishing-condition} \((ii)\).

Note that \(m_h\) is a positive integer.
\end{proof}

An interpretation of the Krein parameters.

Let \(Y = (X, \{R_i\}_{0\leq i\leq D})\) be a commutative scheme with standard module \(V\).

Pick a vector \(v\in V\) with
\[v = \sum_{x\in X}\alpha_x \hat{x}.\]
View \(v\) as a function
\[v: X\longrightarrow \mathbb{C} \quad (x\mapsto \alpha_x).\]
View \(V\) as the set of all functions \(V \longrightarrow \mathbb{C}\). Then the vector space \(V\) together with product of functions is a \(\mathbb{C}\)-algebra.

For
\[v = \sum_{x\in X}\alpha_x \hat{x}, \quad w = \sum_{x\in X}\beta_x \hat{x} \in V,\]
write
\[v\circ w = \sum_{x\in X}\alpha_x\beta_x \hat{x}\]
to represent the product of \(v\) and \(w\) viewed as functions.

\begin{lemma}
\protect\hypertarget{lem:vector-function-product}{}\label{lem:vector-function-product}

With the above notation,

\((i)\) \(A^*_j(x)v = |X|(E_{\hat{j}}\hat{x}\circ v)\) for all \(v\in V\) and for all \(x\in X\).

\((ii)\) \(E_iV\circ E_jV \subseteq \sum_{h: q^h_{ij}\neq 0} E_hV\) for all \(0\leq i, j\leq D\).

\((iii)\) \(E_h(E_i\circ E_jV) = E_hV\) if \(q^h_{ij}\neq 0\) for all \(0\leq h, i, j\leq D\).

\end{lemma}

\hypertarget{lec21}{%
\chapter{Norton Algebras}\label{lec21}}

\textbf{Wednesday, March 17, 1993}

\begin{proof}[Proof of Lemma \ref{lem:vector-function-product}]
\leavevmode

\((i)\) Suppose

\[v = \sum_{x\in X}\alpha_x \hat{x}.\]
Pick a vertex \(z\in X\) and compare \(z\)-coordinate of each side in \((i)\).
\begin{align}
(A^*_j(x)v)_z & = (A^*_j(x))_{zz}v_z = |X|(E_j)_{xz}\alpha_z.\\
|X|(E_{\hat{j}}\hat{x}\circ v)_z & = |X|(E_{\hat{j}}\hat{x})_z\cdot \alpha_z  = |X|(E_j)_{xz}\alpha_z.
\end{align}
Note that \(E_{\hat{j}}\hat{x}\) is the column \(x\) of \(E_{\hat{j}}\) is the row \(x\) of \(E_j\).

\((ii)\) Fix \(i, j, h\) such that \(q^h_{ij} = 0\).

Claim. \(E_h(E_iV \circ E_jV) = 0\).

\begin{align}
E_h(E_iV \circ E_jV) & = E_h(\mathrm{Span}(v\circ w\mid v\in E_iV, w\in E_jV))\\
& = E_h(\mathrm{Span}(E_i\hat{y}\circ E_j\hat{z}\mid y,z\in X))\\
& = \mathrm{Span}(E_h(E_j\hat{z}\circ E_i\hat{y}\mid y,z\in X)\\
& = \mathrm{Span}((E_hA^*_{\hat{j}}(z)E_i)\hat{y}\mid y,z\in X) && \text{by $(i)$}
\end{align}
But \(q^h_{ij} = 0\) implies \(q^{\hat{h}}_{\hat{j}\hat{i}} = 0\).

So, by Lemma \ref{lem:vanishing-condition} \((ii)\),
\[ 0 = (E_{\hat{i}}A^*_{\hat{j}}E_{\hat{h}})^\top = E_h A^*_{\hat{j}}E_i.\]
Hence, \(E_h(E_iV\circ E_jV) = 0\).

\((iii)\) Fix \(i, j, h\) such that \(q^h_{ij}\neq 0\). Then,

\[E_h(E_iV \circ E_jV)\subseteq E_hV\]
is clear. We show the other inclusion. Since
\begin{align}
E_i\hat{y} \circ E_j\hat{y} &=  (\text{column $y$ of $E_i$}\circ \text{column $y$ of $E_j$}) \\
&  = \text{column $y$ of $E_i\circ E_j$}\\
&  = (E_i\circ E_j)\hat{y}\\
&  = \left(\frac{1}{|X|}\sum_{h=0}^D q^h_{ij}E_h\right)\hat{y},
\end{align}
we have,
\begin{align}
E_h(E_iV\circ E_jV) & = E_h\mathrm{Span}(E_i\hat{y}\circ E_j\hat{z}\mid y,z\in X)\\
& \supseteq E_h \mathrm{Span}(E_i\hat{y}\circ E_j\hat{y}\mid y\in X)\\
& = \mathrm{Span}(q^h_{ij}E_h\hat{y}\mid y\in X)\\
& = \mathrm{Span}(E_h\hat{y}\mid y\in X) && \text{since $q^h_{ij}\neq 0$}\\
& = E_hV.
\end{align}
This proves the assertion.

\end{proof}

\begin{lemma}
\protect\hypertarget{lem:norton-algebra}{}\label{lem:norton-algebra}

Given a commutative scheme \(Y = (X, \{R_i\}_{0\leq i\leq D})\), fix \(j\) \((0\leq j\leq D)\).
Define binary multiplication:
\[E_jV \times E_jV \longrightarrow E_jV \quad ((v,w) \mapsto v\ast w = E_j(v\circ w)).\]
Then,

\((i)\) \(v\ast w = w\ast v\), for all \(v,w\in E_jV\),

\((ii)\) \(v\ast (w + w') = v\ast w + v\ast w'\) for all \(v,w,w'\in E_jV\), and

\((iii)\) \((\alpha v)\ast w = \alpha(v\ast w)\) for all \(\alpha \in \mathbb{C}\).

In particular, the vector space \(E_jV\) together with \(\ast\) is a commutative \(\mathbb{C}\)-algebra, (not associative in general).

(\(N_j: (E_jV, \ast)\) is called the Norton algebra \index{Norton algebra} on \(E_jV\).)

\((iv)\) \(v\ast w = 0\) for all \(v, w\in E_jV\) if and only if \(q^j_{jj} = 0\).

\end{lemma}

\begin{proof}
\leavevmode

\((i)-(iii)\) Immediate.

\((iv)\) Immediate from Lemma \ref{lem:vector-function-product} \((ii)\), \((iii)\).

\end{proof}

Let \(Y\), \(j\), \(N_j\) be as in Lemma \ref{lem:norton-algebra}, and \(M\) Bose-Mesner algebra of \(Y\).
Let
\begin{align}
\mathrm{Aut}Y & = \{\sigma\in \mathrm{Mat}_X(\mathbb{C}) \mid \sigma: \text{ permutation matrix }, \sigma \cdot m = m\cdot \sigma \;\text{ for all }m\in M\}\\
& = \{\sigma\in \mathrm{Mat}_X(\mathbb{C}) \mid \sigma: \text{ permutation matrix },\\
& \qquad (x,y)\in R_i \to (\sigma x, \sigma y)\in R_i, \text{ for all } i, \text{ and for all } x,y\in X\}\\
\mathrm{Aut}(N_j) & = \{\sigma: E_jV \to E_jV \mid \sigma \text{ is $\mathbb{C}$-algebra isomorphims},i.e.,\\
& \qquad \sigma(v\ast w) = \sigma(v)\ast\sigma(w) \text{ for all }v, w\in E_jV\}.
\end{align}

\begin{lemma}
\protect\hypertarget{lem:autom-of-norton-algebra}{}\label{lem:autom-of-norton-algebra}

Let \(Y, j, \ast\) be as in Lemma \ref{lem:norton-algebra}.

\((i)\) \(E_jV\) is a module for \(\mathrm{Aut}(Y)\).

\((ii)\) \(\sigma|_{E_jV}\in \mathrm{Aut}(N_j)\) for all \(\sigma \in \mathrm{Aut}(Y)\).

\((iii)\) \(\mathrm{Aut}Y \longrightarrow \mathrm{Aut}(N_j), \; (\sigma \mapsto \sigma|_{E_j})\) is a homomorphism of groups,

(i.e., a representation of \(\mathrm{Aut}(Y)\)).

\((iv)\) Suppose \(R_0, \ldots, R_D\) are orbits of \(\mathrm{Aut}(Y)\) acting on \(X\times X\), (so, we are in Example \ref{exm:gen-tr}) then above representation is irreducible.

\end{lemma}

\begin{proof}
\leavevmode

\((i)\) Pick \(\sigma\in \mathrm{Aut}Y\) and \(v\in V\). Then,

\[\sigma E_j v = E_j\sigma v,\]
since \(\sigma\) commutes with each element of \(M\).

\((ii)\) \(\sigma|_{E_jV}: E_jV \to E_jV\) is an isomorphism of a vector space. Since \(\sigma\) is invertible,for all \(v,w\in E_jV\),

\[\sigma(v\ast w) = \sigma(E_j(E_jv\circ E_jw)) = E_j\sigma(E_jv\circ E_jw) = E_j(E_j\sigma v\circ E_j\sigma w) = \sigma(v)\ast \sigma(w).\]

\((iii)\) Immediate from \((i)\) and \((ii)\).

\((iv)\) Here Bose-Mesner algebra \(M\) is the full commuting algebra, i.e.,

\[M = \{m\in \mathrm{Mat}_X(\mathbb{C})\mid \sigma\cdot m = m\cdot \sigma, \text{ for all }\sigma\in \mathrm{Aut}(Y)\}.\]
Suppose there sia a nonzero proper subspace \(0\neq W\subsetneq E_jV\) that is \(\mathrm{Aut}(Y)\)-invariant.

Set
\[W^\bot = \{v\in E_jV\mid \langle w, v\rangle = 0, \text{ for all }w\in W\}.\]
Then, \(W^\bot\) is a module for \(\mathrm{Aut}(Y)\), since \(\mathrm{Aut}(Y)\) is closed under transpose conjugate.

Let \(e: V\to W\) and \(f: V\to W^\bot\) be orthogonal projection such that \(e + f = E_j\),
\[e^e = e, f^e = f, ef = fe = 0, eE_h = 0, \text{ if } h\neq j.\]

Since \(e\) commutes with all \(\sigma\in \mathrm{Aut}(Y)\), \(e\in M\) and
\[e = \sum_{i=0}^D \alpha_i E_i.\]
If \(h\neq j\), then \(0 = eE_h\) and \(\alpha_h = 0\). Thus, \(e = \alpha_jE_j\), i.e.,
\(e=0\) or \(f=0\).

A contradiction.

\end{proof}

Norton algebras were used in original construction of Monster, a finite simple group \(G\).

Compute character table of \(G\),

\(\quad\to\) \(p^h_{ij}\), \(q^h_{ij}\) of group scheme on \(G\),

\(\quad\to\) find \(j\) where \(m_j = \dim E_jV\) is small and \(q^j_{jj}\neq 0\),

\(\quad\to\) guess abstract structure of \(N_j\) using the knowlege of \(p^h_{ij}\)'s and \(q^h_{ij}\)'s,

\(\quad\to\) compute \(\mathrm{Aut}(N_j)\),

\(\quad\to\) \(G\).

\hypertarget{lec22}{%
\chapter{\texorpdfstring{\(Q\)-Polynomial Schemes}{Q-Polynomial Schemes}}\label{lec22}}

\textbf{Friday, March 19, 1993}

\begin{lemma}
\protect\hypertarget{lem:phijqhij2}{}\label{lem:phijqhij2}

Let \(Y = (X, \{R_i\}_{0\leq i\leq D})\) be a commutative scheme.

\((i)\) \(p^h_{0j} = p^h_{j0} = \delta_{jh}\)..

\((ii)\) \(p^0_{ij} = \delta_{ij'}k_i\).

\((iii)\) \(q^h_{0j} = q^h_{j0} = \delta_{jh}\).

\((iv)\) \(q^0_{ij} = \delta_{i\hat{j}}m_i\).

\((v)\) \({\displaystyle \sum_{j=0}^D p^h_{ij} = k_i}.\)

\((vi)\) \({\displaystyle \sum_{j=0}^D q^h_{ij} = m_i}.\)

\end{lemma}

\begin{proof}
\leavevmode

\((i)\), \((ii)\) These are trivial.

\((iii)\) We have

\[|X|^{-1}\sum_{\ell = 0}^D q^\ell_{0j} E_\ell  = E_0 \circ E_j = |X|^{-1}J\circ E_j = |X|^{-1}E_j.\]

\((iv)\) Recall from Lemma \ref{lem:phijqhij}

\[|X|^{-1}m_h q^h_{ij} = \tau(E_i\circ E_j \circ E_{\hat{h}}),\]
(where \(\tau(B)\) is the sum of entries in matrix \(B\).)

\begin{align}
|X|^{-1}m_0q^0_{ij} & = \tau(E_i\circ E_j\circ E_0) \\
& = |X|^{-1}\tau(E_i\circ E_j) && (E_0 = |X|^{-1}J)\\
& = |X|^{-1}\mathrm{trace}(E_iE_{\hat{j}})\\
& = |X|^{-1}\delta_{i\hat{j}}\mathrm{trace}E_i\\
& = |X|^{-1}\delta_{i\hat{j}}m_i.
\end{align}

\((v)\) Pick \(x,y\in X\) with \((x,y)\in R_h\). Then,

\sum\emph{\{j=0\}\^{}D p\^{}h}\{ij\} \& = \textbar\{z\in X\mid (x,z)\in R\_i, ; (z,y)\in R\_j ; \text{for some $j$}\}\textbackslash{}
\& = \textbar\{z\in X\mid (x,z)\in R\_i\}\textbar\textbackslash{}
\& k\_i.
\textbackslash end\{align\}

\((vi)\)

\[E_i \circ E_j = |X|^{-1}\sum_{h=0}^D q^h_{ij}E_h.\]
So,
\begin{align}
\sum_{j=0}^D E_i\circ E_j & = |X|^{-1}\sum_{h=0}^D \left(\sum_{j=0}^D q^h_{ij}\right) E_h\\
& = E_i \circ \sum_{j=0}^D E_j\\
& = E_i\circ I\\
& = |X|^{-1}(q_i(0)A_0 + q_i(1)A_1 + \cdots + q_i(0)A_D)\circ I\\
& = |X|^{-1}q_i(0)I\\
& = |X|^{-1}m_i(E_0 + E_1 + \cdots + E_D).
\end{align}
This proves the assertions.

\end{proof}

\begin{definition}
\protect\hypertarget{def:q-polynomial}{}\label{def:q-polynomial}Let \(Y = (X, \{R_i\}_{0\leq i\leq D})\) be a commutative scheme.

\(Y\) is \(Q\)-polynomial \index{$Q$-polynomial} with respect to ordering \(E_0, E_1, \ldots, E_D\) of primitive idempotents, if
\[q^h_{ij} \begin{cases} = 0 & \text{if one of $h, i, j$ is greater than the sum of the other two}\\
\neq 0 & \text{if one of $h,i,j$ is equal to the sum of the other two.}\end{cases}\]
In this case, set
\[c^*_i = q^i_{1,i-1}, \; a^*_i = q^i_{1,i}, \; b^*_i = q^i_{1,i+1} \quad (0\leq i\leq D), \;(c^*_0= b^*_D = 0).\]
\end{definition}

Observe: \(Q\)-polynomial \(\to\) \(Y\) is symmetric.

Suppose \(i\neq \hat{i}\) for some \(i\). Then, by the condition in Definition \ref{def:q-polynomial},
\[0 = q^0_{i\hat{i}} = m_i \; (\neq 0)\]
by Lemma \ref{lem:phijqhij2} \((iv)\).
This is a contradiction.

Hence, \({E_i}^\top = E_{\hat{i}} = E_i\) for all \(i\).

Therefore \(M\) is symmetric and \(Y\) is symmetric.

Observe: If \(Y\) is \(Q\)-polynomial,
\[c^*_i + a^*_i + b^*_i = m_1 \quad (0\leq i\leq D)\]
(just as \(c_i + a_i + b_i = k\) for \(P\)-polynomial.)

By Lemma \ref{lem:phijqhij2} \((iv)\),
\[m_1 = q^i_{10} + q^i_{11} + \cdots + q^i_{1,i-1} + q^i_{1i} + q^i_{1,i+1} + \cdots \]
and \(q^i_{10} = q^i_{11} = 0\), \(q^i_{1,i-1} = c^*_i\), \(q^i_{1i} = a^*_i\), and \(q^i_{1,i+1} = b^*_i\).

\begin{lemma}
\protect\hypertarget{lem:q-conditions}{}\label{lem:q-conditions}Assume \(Y = (X, \{R_i\}_{0\leq i\leq D})\) is a symmetric scheme. Pick \(x\in X\), and set \(E^*_i\equiv E^*_i(x)\), \(A^*\equiv A^*(x)\). Then the following are equivalent.

\((i)\) \(\Gamma\) is \(Q\)-polynomial with respect to \(E_0, \ldots, E_D\).

\((ii)\) \$\$q\^{}h\_\{1j\} \textbackslash begin\{cases\} = 0 \& \text{if $\; |h-j| > 0$} \textbackslash{}

\neq 0 \& \text{if $\; |h-j| = 1$}. \textbackslash end\{cases\} \quad (0\leq h,j\leq D).\$\$

\((iii)\) There exists \(f_i^*\in \mathbb{C}[\lambda]\), \(\deg f^*_i = i\), and

\[A^*_i = f^*_i(A^*_1) \quad (0\leq i\leq D).\]

\((iv)\) \(E^*_0V, \ldots, E^*_DV\) are maximal eigenspaces of \(A^*_1\), and

\[E_iA^*_1E_j = O \quad \text{if }\; |i-j|>0, \quad (0\leq i,j\leq D).\]
(Compare \((iv)\) with the definition of \(Q\)-polynomial in Definition \ref{def:q-polynomial-graph}.)
\end{lemma}

\begin{proof}
\leavevmode

\((i)\to (ii)\) Clear.

\((ii)\to(iii)\) \(A^*_0 = I\),

\begin{align}
A^*_iA^*_j & = \sum_{h=0}^D q^h_{ij} A^*_h\\
A^*_1A^*_j & = q^{j-1}_{1j}A^*_{j-1} + q^j_{1j}A^*_j + q^{j+1}_{1j}A^*_{j+1} && (q^{j+1}_{1j}\neq 0, 1\leq j\leq D-1).
\end{align}
Hence \(A^*_j\) is a polynomial of degree exactly \(j\) in \(A^*_1\) by induction on \(j\).
\[\lambda f^*_j(\lambda) = b^*_{j-1}f^*_{j-1}(\lambda) + a^*_jf^*_j(\lambda) + c^*_{j+1}f^*_{j+1}(\lambda) \quad \text{with $c^*_{j+1}\neq 0$,}\]
and \(f^*_{-1} = 0\), \(f^*_0(\lambda) = 1\).

\((iii)\to(i)\) Pick \(i, j, h\) with \(0\leq i,j,h\leq D\) and \(h\geq i+j\). Since

\[m_hq^h_{ij} = m_jq^j_{ih} = m_iq^i_{hj}\]
by Lemma \ref{lem:phijqhij}, it suffices to show that
\[q^h_{ij} \; \begin{cases} = 0 & \text{if }\; h> i+j\\
\neq 0 & \text{if }\; h = i+j.
\end{cases}\]
\begin{align}
A^*_iA^*_j & = \sum_{h=0}^D q^h_{ij}A^*_h\\
f^*_i(A_1)f^*_j(A_1) & = \sum_{h=0}^D q^h_{ij}f^*_h(A_1^*).
\end{align}
Hence,
\[f^*_i(\lambda)f^*_j(\lambda) = \sum_{h=0}^Dq^h_{ij}f^*_h(\lambda).\]
Note that since \(A^*_0, A^*_1, \ldots, A^*_D\) are linearly independent, \(f(A^*_1) = 0\) implies \(\deg f > D\).
\[\deg \mathrm{LHS} = i+j \to q^{i+j}_{ij}\neq 0, \; q^h_{ij} = 0, \text{ if } \; h> i+j.\]

\((iii)\to (iv)\) Recall

\[A^*_1 = q_1(0)E^*_0 + q_1(1)E_1^* + \cdots .\]
Each \(A^*_i\) is a polynomial in \(A^*_1\). Then \(A^*_1\) generates the dual Bose-Mesner algebra. So,
\(q_1(0), q_1(1), \ldots, q_1(D)\) are distinct.

So, \(E^*_0V, \ldots, E^*_DV\) are maximal eigenspaces.

Also, \(|i-j|>1\) implies \(q^j_{11} = 0\).

Thus, \(E_iA^*_1E_j = 0\) by Lemma \ref{lem:vanishing-condition} \((ii)\).

\((iv)\to (ii)\) \(q^i_{1j} = 0\) if \(|i-j| > 1\). since in this case,

\(E_iA^*_1E_j = O\) implies \(q^i_{1j} = 0\) by Lemma \ref{lem:vanishing-condition} \((ii)\).

Suppose \(q^{j+1}_{1j} = 0\) for some \(j\) \((0\leq j\leq D-1)\).

Without loss of generalith, choose \(j\) minimum. Then
\(A^*_h\) is a polynomial of degree \(h\) in \(A^*_1\) \((0\leq h\leq j)\), and
\[A^*_1A^*_j - q^{j-1}_{1j}A^*_{j-1} - q^j_{1j}A^*_j = O.\]
the left hand side is a polynomial in \(A^*_1\) of degree \(j+1\).

Hence, the minimal polynomial of \(A^*_1\) has degree less than or equal to \(j+1 \leq D\). But \%A\^{}*\_1\$ has \(D+1\) distince eigenvalues.

This is a contradiction.

\end{proof}

\hypertarget{lec23}{%
\chapter{Representation of a Scheme}\label{lec23}}

\textbf{Monday, March 22, 1993}

\begin{theorem}
\protect\hypertarget{thm:q-polynomial-space}{}\label{thm:q-polynomial-space}Let \(Y = (X, \{R_i\}_{0\leq i\leq D})\) be a symmetric scheme. (View the standard module \(V\) as an algebra of functions from \(X\) to \(\mathbb{C}\).) Then the following are equivalent.

\((i)\) \(Y\) is \(Q\)-polynomial with respect to ordering \(E_0, E_1, \ldots, E_D\) of primitive idempotents.

\((ii)\) For all \(i\) \(\:(0\leq i\leq D)\),

\[E_0V + E_1V + (E_1V)^2 + \cdots + (E_1V)^i = E_0V + E_1V + \cdots + E_iV.\]
\end{theorem}

\begin{proof}
\leavevmode

By Lemma \ref{lem:vector-function-product} \((ii)\), \((iii)\).

\[E_h(E_iV\circ E_jV) = 0 \text{ if and only if } q^h_{ij} = 0 \quad (0\leq i,j,h\leq D).\]

\((i)\to(ii)\) By our assumption,

\[q^h_{1j} = 0 \text{ if } |h-j|>1, \text{ and } q^{j+1}_{1j}\neq 0.\]
So,
\begin{equation}
E_1V\circ E_jV \subseteq E_{j-1}V + E_jV + E_{j+1}V \quad (0\leq j\leq D), \label{eq:e1vejv}
\end{equation}
\begin{equation}
E_{j+1}(E_1V\circ E_jV) = E_{j+1}V \quad (0\leq j\leq D-1), \label{eq:ej1e1vejv}
\end{equation}
by Lemma \ref{lem:vector-function-product}.

Also \(E_0V \subseteq \mathrm{Span}(\delta)\), where \(\delta\) is all 1's vector, i.e., \(1\) as a function \(X\to \mathbb{C}\).
So,
\begin{equation}
E_0\circ E_jV = E_jV \quad (0\leq j\leq D). \label{eq:e0vejv}
\end{equation}
Show \((ii)\) by induction on \(i\).

The cases \(i=0, 1\) are trivial.

\(i>1\): \(\subseteq\).
\begin{align}
& E_0V + E_1V + (E_1V)^2 + \cdots + (E_1V)^i\\
& \quad = E_0V + E_1V\circ (E_0V + E_1V + \cdots + (E_1V)^{i-1})\\
& \quad = E_0V + E_1V\circ (E_0V + E_1V + \cdots + E_{i-1}V)\\
& \quad \subseteq E_0V  + E_1V + \cdots + E_{i}V
\end{align}
by \eqref{eq:e1vejv}.

\(\supseteq\).

Claim. \(E_i\subseteq E_1V\circ E_{i-1}V + E_{i-1}V + E_{i-2}V \quad (2\leq i\leq D)\).

\emph{Proof of Claim.} By \eqref{eq:ej1e1vejv},
\[E_i(E_1V \circ E_{i-1}V) = E_iV.\]
For all \(v\in E_i V\), there exists \(u\in E_1V\circ E_{i-1}V\) such that \(E_iu = v\).

On the other hand, by \eqref{eq:e1vejv},
\[E_1V\circ E_{i-1}V \subseteq E_{i-2}V + E_{i-1}V + E_{i-2}V.\]
So, \(u = w+v\), where \(w\in E_{i-2}V + E_{i-1}V\). We have,
\[w = u-v \in E_1V \circ E_{i-1}V + E_{i-1}V + E_{i-2}V\]
as desired.

\begin{remark}
\[E_iV \circ E_jV = \mathrm{Span}(u\circ v \mid u\in E_iV, v\in E_jV).\]
\end{remark}

By claim,
\begin{align}
& E_0V + E_1V + \cdots + E_iV\\
& \quad \subseteq E_0V + E_1V + \cdots + E_iV + E_1V\circ E_{i-1}V\\
& \quad \subseteq E_0V + E_1V + \cdots + (E_{1}V)^{i-1} + E_1V(E_0V + E_1V + \cdots + (E_{1}V)^{i-1})\\
& \quad \subseteq E_0V  + E_1V + \cdots + (E_{1}V)^{i-1} + (E_1V)^{i}.
\end{align}

\((ii)\to(i)\)

Claim 1. Pick \(i, j\) \((0\leq i,j\leq D)\) with \(j>i+1\). Then \(q^j_{1i} = 0\).

\emph{Proof of Claim 1.}
\begin{align}
E_j(E_1\circ E_jV) & \subseteq E_j(E_1V\circ(E_0V + E_1V + (E_1V)^2 + \cdots + (E_1V)^i))\\
& \subseteq E_j(E_0V + E_1V + (E_1V)^2 + \cdots + (E_1V)^{i+1})\\
& = E_j(E_0V + E_1V + \cdots + E_{i+1}V)\\
& = 0.
\end{align}
So \(q^j_{1i}=0\) by Lemma \ref{lem:vector-function-product}.

Claim 2. \(q^{i+1}_{1i} \neq 0\) \((0\leq i < D)\).

\emph{Proof of Claim 2.}
\begin{align}
& E_0V + E_1V + \cdots + E_{i+1}V\\
& \quad = E_0V + E_1V + \cdots + (E_1V)^{i+1}\\
& \quad = E_0V + E_1V\circ(E_0V + E_1V + \cdots + (E_{1}V)^{i})\\
& \quad = E_0V + E_1V\circ(E_0V + E_1V + \cdots + E_iV)\\
& \quad = E_0V  + E_1V\circ(E_0V + \cdots + E_iV).
\end{align}
So,
\begin{align}
E_{i+1}V & = E_{i+1}(E_1V\circ (E_0V + \cdots + E_iV))\\
& = E_{i+1}(E_1V \circ E_iV)
\end{align}
by Claim 1 and Lemma \ref{lem:vector-function-product}.

Hence, \(q^{i+1}_{1i}\neq 0\) by Lemma \ref{lem:vector-function-product}.

\end{proof}

Let \(Y = (X, \{R_i\}_{0\leq i\leq D})\) be a commutative scheme with standard module \(V\).

\begin{definition}
\protect\hypertarget{def:representation-of-y}{}\label{def:representation-of-y}A representation of \(Y\) is a pair \((\rho, H)\), where \(H\) is a non-zero Hermitean space (with inner product \(\langle \;, \;\rangle\)) and
\(\rho: X\to H\) is a map satisfying the following.

\(\mathrm{R1}\). \(H = \mathrm{Span}(\rho(x)\mid x\in X)\).

\(\mathrm{R2}\). \(\langle \rho(x), \rho(y)\rangle\) depends only on \(i\) for which \((x,y)\in R_i\) \((x,y\in X)\).

\(\mathrm{R3}\). For every \(x\in X\) and for all \(i\) \((0\leq i\leq D)\),

\[\sum_{y\in X, (y,x)\in R_i}\rho(y)\in \mathrm{Span}(\rho(x)).\]

Above representation is nondegenerate if \(\{\rho(x)\mid x\in X\}\) are distinct.
\end{definition}

\begin{example}
\protect\hypertarget{exm:representation-of-hd2}{}\label{exm:representation-of-hd2}\(Y = H(D,2)\), \(X = \{a_1\cdots a_D\mid a_i\in \{1,-1\}, 1\leq i\leq D\}\).
Let \(H = \mathbb{C}^D\) and \(\langle \;, \;\rangle\) usual Hermitean dot product.

For a vertex \(x = a_1\cdots a_D\in X\), define
\[\rho(x) = a_1\cdots a_D\in H.\]
Then, \(\mathrm{R1}-\mathrm{R3}\) hold.
\end{example}

\begin{remark}
\(\mathrm{R1}, \mathrm{R2}\) are obvious.
For \(\mathrm{R3}\), we may assume that \(x = 1\cdots 1\).
Restrict
\[\sum_{y\in X, (y,x)\in R_i}\rho(y)\]
on the first coordinate. Then,
\begin{align}
-1 & \quad \text{appers }\; \binom{D-1}{i-1} \;\text{ times}\\
1 & \quad \text{appers } \;\binom{D-1}{i} \;\text{ times}.
\end{align}
Hence,
\[\sum_{y\in X, (y,x)\in R_i}\rho(y) = \left(\binom{D-1}{i} - \binom{D-1}{i-1}\right)\rho(x).\]
\end{remark}

Let \((\rho, H)\) be a representation of arbitrary commutative scheme \(Y\). Set
\[E = (\langle \rho(x),\rho(y)\rangle)_{x,y\in X}\]
Gram matrix of the representation.

\begin{definition}
\protect\hypertarget{def:equivalence-of-representation}{}\label{def:equivalence-of-representation}Representations \((\rho, H)\), \((\rho', H')\) of \(Y\) are equivalent, whenever, Gram matrices are related by
\[E'\in \mathrm{Span} E.\]
We do not distinguish between equivalent representations.
\end{definition}

\textbf{Note.}
Suppose \((\rho, H)\) is a representation of a symmetric scheme \(Y\). Pick \(x,y\in X\) with \((x,y)\in R_j\).

Then \((y,x)\in R_j\). So, by \(\mathrm{R2}\),
\[\langle \rho(x), \rho(y)\rangle = \langle \rho(y),\rho(x)\rangle = \overline{\langle \rho(x), \rho(y)\rangle},\]
since \(\langle \;, \;\rangle\) is Hermitean.

Hence, the Gram matrix \(E\) of \(\rho\) is real symmetirc. Without loss of generality, we can view \(H\) as a real Euclidean space in this case.

\begin{lemma}
\protect\hypertarget{lem:rep-of-scheme}{}\label{lem:rep-of-scheme}

Let \(Y = (X, \{R_i\}_{0\leq i\leq D})\) be a commutative scheme and \(V\) a standard module.

Let \(E_j\) be any primitive idempotent of \(Y\).

\((i)\) \((\rho, H)\) is a representation of \(Y\), where \(H = E_jV\) (with inner product inherited from \(Y\)).

\[\rho: X \to H \quad (x\mapsto E_j\hat{x})\]
(i.e., \(\rho(x)\) is the \(x\)-th column of \(E_j\).)

\((ii)\) \(\langle \rho(x),\rho(y)\rangle = |X|^{-1}q_j(i)\), if \((x,y)\in R_i\), \((x,y\in X)\).

\((iii)\) For \(0\leq i\leq D\) and \(x,y\in X\),

\[\sum_{y\in X, (y,x)\in R_i}\rho(y) = p_i(j)\rho(x).\]

\((iv)\) \((\rho,H)\) is nondegenerate if and only if \(q_j(i) \neq q_j(0)\) for all \(i\), \((0\leq i\leq D)\).

\((v)\) Every representation of \(Y\) is equivalent to a representation of the above type for some \(j\) \((0\leq j\leq D)\), and \(j\) is unique.

\end{lemma}

\begin{proof}
\leavevmode

\((i)-(iii)\).

\(\mathrm{R1}\): \(\mathrm{Span}(\rho X)\) is the column space of \(E_j\) which is equal to \(H\).

\(\mathrm{R2}\):
\begin{align}
\langle \rho(x),\rho(y)\rangle & = \langle E_j\hat{x}, E_j\hat{y}\rangle \\
& = (\overline{E_j\hat{x}})^\top E_j\hat{y}\\
& = \hat{x}^\top \overline{E_j}^\top E_j\hat{y}\\
& = \hat{x}^\top E_j \hat{y}\\
& (E_j)_{xy}.
\end{align}
Note that \(\overline{E_j}^\top = E_j\) by Lemma \ref{lem:ei}.

Recall
\[E_j = |X|^{-1}(q_j(0)A_0 + \cdots + q_j(D)A_D).\]
So,
\[(E_j)_{xy} = |X|^{-1}q_j(i), \quad \text{ where } \; (x,y)\in R_i.\]

\(\mathrm{R2}\): Recall
\[A_i = p_i(0)E_0 + \cdots + p_i(D)E_D.\]
So,
\(E_jA_i = p_i(j)E_j\), and
\[p_i(j)\rho(x) = p_i(j)E_j\hat{x} = E_jA_i\hat{x} = E_j\sum_{y\in X, (y,x)\in R_i}\hat{y} = \sum_{y\in X, (y,x)\in R_i}\rho(y).\]

\textbf{Note.}
\[A_i\hat{x} = \sum_{y\in X, (x,y)\in R_{i'}}\hat{y}.\]

\emph{Pf.}
\begin{align}
\text{$z$ entry of LHS} & = (A_i\hat{x})_z \\
& = \sum_{w\in X}(A_i)_{zw}\hat{x}_w\\
& = (A_i)_{zx}\\
& = \begin{cases}
1 & \text{if $(x,z)\in R_{i'}$}\\
0 & \text{else}.
\end{cases}
\end{align}
\begin{align}
\text{$z$ entry of RHS} & = \sum_{y\in X, (x,y)\in R_{i'}, z = y}1\\
& = \begin{cases}
1 & \text{if $(x,z)\in R_{i'}$}\\
0 & \text{else}.
\end{cases}
\end{align}

\((iv)\) By \((ii)\),

\begin{align}
\|\rho(x)\|^2 & = \langle \rho(x), \rho(y)\rangle\\
& |X|^{-1}q_j(0)\\
& |X|^{-1}m_j,
\end{align}
as \(m_j = \dim E_jV\), and is independent of \(x\in X\).

Pick distinct \(x,y\in X\) such that \((x,y)\in R_i\) with \(i\neq 0\).

Then,
\begin{align}
\rho(x) = \rho(y) & \Leftrightarrow \langle \rho(x),\rho(y)\rangle = \|\rho(x)^2\| = |X|^{-1}q_j(0)\\
& \Leftrightarrow |X|^{-1}q_j(i) = |X|^{-1}q_j(0)\\
& \Leftrightarrow q_j(i) = q_j(0).
\end{align}

Hence, we have \((iv)\). To be continued.

\end{proof}

\hypertarget{lec24}{%
\chapter{Balanced Conditions, I}\label{lec24}}

\textbf{Wednesday, March 23, 1993}

No Class on Friday (another conference).

\begin{proof}[Proof of Lemma \ref{lem:rep-of-scheme} continued]
Let \(E_j\) be a primitive idempotent, \(H = E_jV\) and
\[\rho: X\to H \quad (x\mapsto E_j\hat{x}).\]

\((v)\) Every representation \((\rho, H)\) of \(Y\) is equivalent to a representation of above type, for some \(j\) \((0\leq j\leq D)\) and \(j\) is unique.

Let \(E:=(\langle \rho(x), \rho(y))_{x,y\in X}\).

By \(\mathrm{R2}\),
\[E = \sum_{i = 0}^D \sigma_i A_i, \quad \text{some}\; \sigma_0, \sigma_1, \ldots, \sigma_D\in \mathbb{C}.\]
Hence, \(E\) belongs to the Bose-Mesner algebra \$M of \(Y\).

We want to show that \(E\) is a scalar multiple of a primitive idempotent.

Fix \(x\in X\) and fix \(i\) \((0\leq i\leq D)\).

By \(\mathrm{R3}\),
\begin{equation}
\sum_{y\in X, (y,x)\in R_i}\rho(y) = \alpha \rho(x), \quad \text{some }\; \alpha\in \mathbb{C}. \label{eq:sumrhoy}
\end{equation}
So,
\[k_i\overline{\sigma_i} = \left\langle \sum_{y\in X, (y,x)\in R_i}\rho(y),\rho(x)\right\rangle = \bar{\alpha}\langle \rho(x), \rho(x)\rangle = \bar{\alpha}\sigma_0.\]
Hence, \(\alpha\) is independent of \(x\). In maatrix form \eqref{eq:sumrhoy} becomes
\[EA_i\hat{x} = \alpha E\hat{x}.\]

\begin{remark}
\[Eu = Ev \Leftrightarrow \langle z, Eu\rangle = \langle z, Ev\rangle \text{ for all }z\in X \Leftrightarrow (Eu)_z = (Ev)_z \text{ for all }z\in X.\]
\begin{align}
(EA_i\hat{x})_z & = \left\langle \rho(z), \sum_{y\in X, (y,x)\in R_i}\rho(y)\right\rangle\\
& = \alpha \langle \rho(z), \rho(x)\rangle\\
& = (\alpha E\hat{x})_z.
\end{align}
Hence,
\[EA_i\hat{x} = \alpha E\hat{x}.\]
\end{remark}

Since \(x\) is arbitrary,
\[EA_i = \alpha E.\]
So,
\[EA_i \in \mathrm{Span} E\; \text{ and }\; EM = \mathrm{Span} E.\]
We have \(E\in \mathrm{E_j}\) for unique \(j\) \((0\leq j\leq D)\).
\end{proof}

\begin{remark}
\[E = \tau_0 E_0 + \cdots + \tau)D E_D, \; \tau_j\in \mathbb{C}\quad (0\leq j\leq D).\]
And, at least one of \(\tau_j\) is nonzero, and
\[\tau_jE_j = EE_j \in \mathrm{Span}E.\]
So,
\[\tau_jE_j = E\]
as \(E_0, \ldots, E_D\) are linearly independent.
\end{remark}

Let \(Y = (X, \{R_i\}_{0\leq i\leq D})\) be a symmetric scheme, and let \(E\) be a primitive idempotent.

\begin{definition}
\protect\hypertarget{def:q-poly-representation}{}\label{def:q-poly-representation}\(Y\) is \(Q\)-polynomial\index{$Q$-polynomial} with respect to \(E\), if and only if \(Y\) is \(Q\)-polynomial with respect to some ordering \(E_0, E_1, \ldots, E_D\) of primitive idempotents, where \(E_0 = |X|^{-1}J\), and \(E_1 = E\).
\end{definition}

\begin{theorem}
\protect\hypertarget{thm:balanced}{}\label{thm:balanced}Assume \(Y = (X, \{R_i\}_{0\leq i\leq D})\) is \(P\)-polynomial (i.e., \((X, R_1)\) is distance-regular). Let \(E\) be any primitive idempotent of \(Y\). Let \((\rho, H)\) be the corresponding representation.

\((i)\) The following are equivalent.

\(\quad (ia)\) \(Y\) is \(Q\)-polyonimial with respect to \(E\).

\(\quad (ib)\) \((\rho, H)\) is nondegenerate and for all \(x,y\in X\), and for all \(i,j\) \((0\leq i,j\leq D)\),

\[\sum_{z\in X, (x,z)\in R_i, (y,z)\in R_j}\rho(z) - \sum_{z'\in X, (x,z')\in R_j, (y,z')\in R_i}\rho(z')\in \mathrm{Span}(\rho(x)-\rho(y)).\]

\(\quad (ic)\) \((\rho, H)\) is nondegenerate and for all \(x,y\in X\),

\[\sum_{z\in X, (x,z)\in R_1, (y,z)\in R_2}\rho(z) - \sum_{z'\in X, (x,z')\in R_2, (y,z')\in R_1}\rho(z')\in \mathrm{Span}(\rho(x)-\rho(y)).\]

\((ii)\) Wirte

\[E = |X|^{-1}\sum_{j=0}^D \theta^*_j A_j,\]
and suppose \((ia)-(ic)\) hold. Then the coefficient in \((ib)\) is
\[p^h_{ij}\frac{\theta_i^*-\theta_j^*}{\theta^*_0-\theta^*_h} \quad (1\leq h\leq D, 0\leq i,j\leq D).\]
\end{theorem}

\begin{proof}
\leavevmode

\((ia)\to(ib)\) Without loss of generality, assume \(E \equiv E_1\), and \(Y\) is \(Q\)-polynomial with respect to \(E\).

Then by Lemma \ref{lem:q-conditions},
\(\theta_0^*, \ldots, \theta^*_D\) are distinct. So \(\theta^*_h\neq \theta^*_0\) for all \(h\in \{1,2\ldots, D\}\), and \((\rho, H)\) is nondegenerate.

Fix \(x\in X\), write \(E^*_i\equiv E^*_i(x)\), \(A^*_i \equiv A^*_i(x)\), \(A^* \equiv A_1^*\).

Let \(M\) be the Bose-Mesner algebra. Set
\[L = \{mA^*n - nA^*m\mid m, n\in M\}.\]

Claim 1. \(\dim L \leq D\).

\emph{Proof of Claim 1.}
\begin{align}
L & = \mathrm{Span}(E_iA^*E_j - E_jA^*E_i \mid 0\leq i<j\leq D)\\
& = \mathrm{Span}(E_iA^*E_{i+1} - E_{i+1}A^*E_i \mid 0\leq i\leq D-1).
\end{align}
Since \(E_iA^*E_j = O\) if \(q^1_{ij} = 0\) by Lemma \ref{lem:phijqhij} and Lemma \ref{lem:vanishing-condition},
and this occurs if \(|i-j|>1\) by \(Q\)-polynomial property.

Hence, \(\dim L \leq D\).

Claim 2. \((i)\) \(\{A^*A_h - A_hA^*\mid 1\leq h\leq D\}\) is a basis for \(L\). In particular,

\((ii)\) there exist \(r^h_{ij}\in \mathbb{C}\) \((1\leq h\leq D, 0\leq i,j\leq D)\) such that

\[A_iA^*A_j - A_jA^*A_i = \sum_{h=1}^D r^h_{ij}(A^*A_h - A_hA^*).\]

\emph{Proof of Claim 2.}

\((i)\) The column \(x\) of \(A^*A_h - A_hA^*\) is a nonzero scalar \(\theta^*_h - \theta^*_0\) times the column \(x\) of \(A_h\).

\begin{remark}
\[ ((A^*A_h - A_hA^*)\hat{x})_y = E_{xy}(A_h)_{yx}- (A_h)_{yx}E_{xx} = (\theta^*_h-\theta^*_0)(A_h)_{yz}.\]
\end{remark}

Also the column \(x\) of \(A_0, A_1, \ldots, A_D\) are linearly independent.

Hence, the matrices given are linearly independent.

They are in \(L\) by construction, so they form a basis for \(L\) by Claim 1.

\((ii)\) This is immediate since

\[A_iA^*A_j - A_jA^*A_i\in L, \quad \text{for all $i,j$}.\]

Cloim 3.
\[r^\ell_{ij} = p^\ell_{ij}\left(\frac{\theta^*-\theta^*_j}{\theta^*_0 - \theta^*_\ell}\right)\quad (1\leq \ell\leq D, 0\leq i,j\leq D).\]

\emph{Proof of Claim 3.}
Fix \(i,j\),
\[A_iA^*A_j - A_jA^*A_i - \sum_{h=1}^D r^h_{ij}(A^*A_h - A_hA^*) = 0.\]
Pick \(\ell\) \((1\leq \ell \leq D)\). Pick \(y\in X\) such that \((x,y)\in R_\ell\).
\begin{align}
(A_iA^*A_j)_{xy} & = \sum_{z\in X}(A_i)_{xz}(A^*)_{zz}(A_j)_{zy}\\
& = \sum_{z\in X, (x,z)\in R_i, (y,z)\in R_j}(A^*)_{zz}\\
& = |X|^{-1}p^\ell_{ij}\theta^*_i.
\end{align}
Similarly,
\[(A_jA^*A_i)_{xy} = |X|^{-1}p^\ell_{ij}\theta^*_j.\]
\begin{align}
(A^*A_h-A_hA^*)_{xy} & = (A_0A^*A_h - A_hA^*A_0)_{xy}\\
& = |X|^{-1}p^\ell_{0h}(\theta^*_0 - \theta^*_h)\\
& = \begin{cases}
0 & \text{ if }\; \ell \neq h\\
|X|^{-1}(\theta^*_0-\theta^*_h) & \text{ if } \ell = h.
\end{cases}
\end{align}
Hence,
\[\sum_{h=1}^D r^h_{ij}(A^*A_h - A_hA^*)_{xy} = |X|^{-1}r^\ell_{ij}(\theta^*_0-\theta^*_\ell).\]
Comparing terms, we have
\[p^\ell_{ij}(\theta^*_i-\theta^*_j) - r^\ell_{ij}(\theta^*_0-\theta^*_\ell) = 0.\]

Claim 4. For all \(h\) \((1\leq h\leq D)\), for all \(i,j\) \((0\leq i,j\leq D)\), for all \(w,y\in X\), \((w,y)\in R_h\),
\begin{equation}
\sum_{z\in X,(w,z)\in R_i, (y,z)\in R_j}\rho(z)-\sum_{z'\in X, (w,z')\in R_j, (y,z)\in R_i}\rho(z') - r^h_{ij}(\rho(w)-\rho(y))=0. \label{eq:rhozrhoz}
\end{equation}

\emph{Proof of Claim 4.}
Set \(L = \langle \mathrm{LHS}\) of \eqref{eq:rhozrhoz}, \(\rho(x)\rangle\)
It suffices to show that \(L = 0\).

Note that since \(x\) is arbitrary, if \(\mathrm{LHS}\) of \eqref{eq:rhozrhoz} is zero.
\begin{align}
L & = \sum_{z\in X,(w,z)\in R_i, (y,z)\in R_j}\langle \rho(z), \rho(x)\rangle -\sum_{z'\in X, (w,z')\in R_j, (y,z)\in R_i}\langle\rho(z'),\rho(x)\rangle \\
& \quad - r^h_{ij}\langle \rho(w)-\rho(y), \rho(x)\rangle\\
& = |X|^{-1}(A_iA^*A_j)_{wy} - |X|^{-1}(A_jA^*A_i)_{wy}-|X|^{-1}\sum_{\ell=1}^Dr^\ell_{ij}(A^*A_\ell - A_\ell A^*)_{wy}\\
& = |X|^{-1} \text{times $wy$ entry of a matrix known to be zero by Claim 2}\\
& = 0.
\end{align}

\end{proof}

\begin{remark}
\begin{align}
|X|^{-1}\sum_{\ell=1}^D r^\ell_{ij}(A^*A_\ell - A_\ell A^*)_{wy} & = |X|^{-1}r^h_{ij}(A^*A_h- A_hA^*)_{wy}\\
& = r^h_{ij}(\langle \rho(x),\rho(w)\rangle - \langle \rho(x),\rho(y)\rangle)
\end{align}
\end{remark}

\hypertarget{lec25}{%
\chapter{Balanced Conditions, II}\label{lec25}}

\textbf{Monday, March 29, 1993}

\begin{proof}[Proof of Theorem \ref{thm:balanced} continued]
\leavevmode

\((ib)\to(ic)\) Obvious.

\((ic)\to(ia)\) Without loss of generality, we may assume \(D\geq 3\), else trivial.

\begin{remark}
The case \(D = 2\) should be treated somewhere, but the assumption \(D\geq 3\) is not used.
\end{remark}

Fix \(w\in X\), and write \(E^*_i \equiv E^*_i(w)\), \(A^*_i\equiv A^*_i(w)\), \(A^*\equiv A^*_1\), and \(A_i\), \(i\)-th distance matrix. Set
\[E \equiv E_1 = |X|^{-1}\sum_{i=0}^D \theta^*_i A_i.\]
Since \((\rho, H)\) is nondegenerate,
\[\theta^*_0 \neq \theta^*_h, \; \text{for all }h\in \{1,2,\ldots, D\}\]
See Lemma \ref{lem:rep-of-scheme} \((iv)\).

Claim 1. Pick \(h\) \((1\leq h\leq D)\), and \(x,y\) with \((x,y)\in R_h\). Then
\[\sum_{z\in X, (x,z)\in R_1, (y,z)\in R_2}\rho(z) - \sum_{z'\in X, (x,z')\in R_2, (y,z')\in R_1}\rho(z') = r^h_{12}(\rho(x)-\rho(y)),\]
where
\[r^h_{12} = p^h_{12}\frac{\theta_1^* - \theta^*_2}{\theta^*_0-\theta^*_h}.\]

\emph{Proof of Claim 1.}
By our assumption,
\[\sum_{z\in X, (x,z)\in R_1, (y,z)\in R_2}\rho(z) - \sum_{z'\in X, (x,z')\in R_2, (y,z')\in R_1}\rho(z') = \alpha(\rho(x)-\rho(y)).\]
Hence,
\begin{align}
|X|^{-1}p^h_{12}(\theta^*_1-\theta^*_2) & = \left\langle \sum_{z\in X, (x,z)\in R_1, (y,z)\in R_2}\rho(z) - \sum_{z'\in X, (x,z')\in R_2, (y,z')\in R_1}\rho(z'), \rho(x)\right\rangle \\
& = \alpha\langle \rho(x)-\rho(y), \rho(x)\\
& = \alpha |X|^{-1}(\theta_0^*-\theta^*_h).
\end{align}
We have
\[\alpha = p^h_{12}\frac{\theta_1^* - \theta^*_2}{\theta^*_0-\theta^*_h}.\]

Claim 2.
\({\displaystyle A_1A^*A_2 - A_2A^*A_1 = \sum_{h=1}^D r^h_{12}(A^*A_h - A_hA^*).}\)

\emph{Proof of Claim 2.}
The \(xy\) entry of the \(\mathrm{LHS} - \mathrm{RHS}\) is
\[|X|\left\langle \sum_{z\in X, (x,z)\in R_1, (y,z)\in R_2}\rho(z) - \sum_{z'\in X, (x,z')\in R_2, (y,z')\in R_1}\rho(z') - r^h_{12}(\rho(x)-\rho(y)),\rho(w)\right\rangle,\]
where \((x,y)\in R_h\), \(h = 1, 2, \ldots, D\), and the \(xy\) entry of the \(\mathrm{LHS} - \mathrm{RHS}\) is \(0\) if \(x = y\).

But the vector on the left in the above inner product is \(0\) by Claim 1, so the inner product is \(0\).

Thus, the \(xy\) entry of the \(\mathrm{LHS} - \mathrm{RHS}\) is always \(0\), and we have Claim 2.

Claim 3. \(A^*A_3 - A_3A^* \in \mathrm{Span}(AA^*A_2 - A_2A^*A, A^*A_2 - A_2A^*, A^*A-AA^*).\)

\emph{Proof of Claim 3.}
Since \(p^h_{12} = 0\), if \(h>3\), and \(p^h_{12} \neq 0\), if \(h=3\), we have \(r^h_{12} = 0\) if \(h > 0\), and \(r^h_{12} \neq 0\), if \(h = 3\). Note that \(\theta^*_1\neq \theta^*_2\).

Now we are done by Claim 2.

Claim 4. There exist \(\beta, \gamma, \delta\in \mathbb{R}\) such that
\begin{align}
0 & = [A, A^2A^*-\beta AA^*A + A^*A^2 - \gamma(AA^*+A^*A) - \delta A^*]\\
& = A^3A^* - A^*A^3 - (\beta+1)(A^2A^*A-AA^*A^2)-\gamma(A^2A^*-A^*A^2)-\delta(AA^*-A^*A).
\end{align}

\emph{Proof of Claim 4.}
There exists \(f_i\in \mathbb{R}[\lambda]\), \(\deg f_i = i\) such that \(A_i = f_i(A_1)\).

Writing \(A_2\), \(A_3\) as polynomials in \(A\) in Claim 3 and simplifying, we find
\[A^3A^*-A^*A^3 \in \mathrm{Span}(A^2A^*A-AA^*A^2, A^2A^*-A^*A^2, AA^*-A^*A).\]

\begin{remark}
Let \(A_3 = \beta_3A^3 + \beta_2 A^2 + \beta_1 A + \beta_0 I\) with \(\beta_3\neq 0\), and \(A_2 = \gamma_2 A^2 + \gamma_1 A + \gamma_0 I\), with \(\gamma_2\neq 0\). Then
\begin{align}
A^*A_3-A_3A^* & = A^*(\beta_3 A^3 + \beta_2 A^2 + \beta_1 A + \beta_0 I) - (\beta_3 A^3 + \beta_2 A^2 + \beta_1 A + \beta_0 I)A^*.\\
A^3A^*-A^*A^3 & \in \mathrm{Span}(A^*A_3 - A_3A^*, A^2A^* - A^*A^2, AA^*-A^*A)\\
& \subseteq \mathrm{Span}(AA^*A_2 - A_2A^*A, A^*A_2-A_2A^*, A^2A^*-A^*A^2, AA^*-A^*A)\\
A^*A_2 - A_2A^* & = A^*(\gamma_2 A^2 + \gamma_1 A + \gamma_0 I) - (\gamma_2 A^2 + \gamma_1 A + \gamma_0 I)A^*\\
AA^*A_2 - A_2A^*A & = AA^*(\gamma_2 A^2 + \gamma_1 A + \gamma_0 I) - (\gamma_2 A^2 + \gamma_1 A + \gamma_0 I)A^*A\\
A^*A_2 - A_2A^* & \in \mathrm{Span}(A^2A^*-A^*A^2, AA^*-AA^*)\\
AA^*A_2 - A_2A^*A & \in \mathrm{Span}(A^2A^*A-AA^*A^2, AA^*-AA^*)\\
A^3A^*-A^*A^3 & \in \mathrm{Span}(A^2A^*A-AA^*A^2, A^2A^*-A^*A^2, AA^*-A^*A).
\end{align}
\end{remark}

Hence, we can find \(\delta, \gamma, \delta\) satisfying
\[0 = A^3A^*-A^*A^3 - (\beta+1)(A^2A^*A-AA^*A^2)-\gamma(A^2A^*-A^*A^2)-\delta(AA^*-A^*A).\]
On the other hand,
\begin{align}
& [A, A^2A^*-\beta AA^*A+A^*A^2-\gamma(AA^*+A^*A)-\delta A^*]\\
& \quad = A^3A^*-A^2A^*A-\beta A^2A^*A + \beta AA^*A^2 + AA^*A^2 - A^*A^3 \\
& \quad\quad - \gamma A^2A^* - \gamma AA^*A + \gamma AA^*A + \gamma A^*A^2 - \delta AA^* + \delta A^*A\\
& \quad = A^3A^* - A^*A^3 - (\beta+1)(A^2A^*A-AA^*A^2)-\gamma(A^2A^*-A^*A^2)-\delta (AA^*-A^*A).
\end{align}
Thus we have \((i)\) and \((ii)\).

Define a diagram \(D_E\) on nodes \(0, 1, \ldots, D\).

Connect distinct nodes \(,\) by undirected arc if \(q^1_{ij}\neq 0\). (Note \(q^1_{ij} = q^1_{ji}\)).

Since \(q^1_{0j} = \delta_{1j}\), the \(0\)-node is adjacent to the \(1\)-node and no other node.

\(Y\) is \(Q\)-polynomial with respect to \(E\) if and only if \(E_E\) is a path.

Claim 5. \(D_E\) is connected.

\emph{Proof of Claim 5.}
Suppose there exists \(\Delta \subseteq \{0,1,\ldots, D\}\) such that \(i,j\) not connected for every \(i\in \Delta\) and \(j\in \{0,1,\ldots, D\}\setminus D\).

Set
\[f = \sum_{i\in \Delta}E_i.\]
Observe
\begin{align}
fA^* & = \sum_{i\in \Delta} E_i A^* \left(\sum_{j=0}^D E_j\right)\\
& = \sum_{i\in \Delta, j\in \Delta}E_iA^*E_j \quad \text{(since $E_iA^*E_j=O$ if $q^1_{ij}=0$)}\\
& = fA^*f.
\end{align}
Also, \(A^*f = fA^*f\).

Hence, \(f\) commutes with \(A^*\).

But \(f\) is an element of the Bose-Mesner algebra
\[f = \sum_{i=0}^D \alpha_i A_i \quad \text{for some $\alpha_0, \ldots, \alpha_D\in \mathbb{C}$}.\]
We have
\[0 = fA^*-A^*f = \sum_{i=1}^D \alpha_i(A_iA^*- A^*A_i).\]
But \(\{A_hA^* - A^*A_h \mid 1\leq h\leq D\}\) are linearly independent.
(The column \(w\) of \(A_hA^*-A^*A_h\) is \(\theta^*_h - \theta^*_0\) times the column \(w\) of \(A_h\).)

Hence, \(\alpha_1 = \cdots = \alpha_D = 0\), and \(f = \alpha_0 I\). Since \(f^2 = f\), \(\alpha_0\) or \(1\).

If \(\alpha_0 = 0\), \(f=O\) and \(\Delta = \emptyset\).

If \(\alpha_0 = 1\), \(f=I\) and \(\Delta = \{0, 1, \ldots, D\}\).

This proves Claim 5.

\end{proof}

\begin{remark}
Claim 5 proves the following in general.

Let \(Y = (X, \{R_i\}_{0\leq i\leq D})\) be a symmetric association scheme. Fix a vertex \(x\in X\), and let
\[E = \frac{1}{|X|}\sum_{j=0}^D \theta^*_j A_j \quad (\theta^*_j = q_1(j) \; \text{ if $E = E_1$})\]
be a primitive idempotent and \(E^*_j\equiv E^*_j(x)\).
\[A^* = \sum_{j=0}^D \theta_j^*E^*_j.\]
If \(\theta_0 = \theta^*_h\), \(h=1, \ldots, D\), then the following hold.

\((i)\) \(\{A_hA^* - A^*A_h \mid 1\leq h\leq D\}\) are linearly independent.

\((ii)\) The diagram \(D_E\) on nodes \(0, 1, \ldots, D\) defined by

\[i\sim j \Leftrightarrow E(E_i\circ E_j)\neq O\]
is connected.

\((iii)\) \(C_M(A^*) = \{L\in M\mid LA^* = A^*L\} = \mathrm{Span}(I).\)

\emph{Proof.}
\textbar{} \((i)\) The column \(x\) of \(A_hA^* - A^*(A_h)\) is \(\theta^*_0-\theta^*_h\) times the column \(x\) of \(A_h\).

\((iii)\) \({\displaystyle 0 = [\sum_{h=0}^D\alpha_hA_h, A^*] = \sum_{h=1}^D\alpha_h(A_hA^*-A^*A_h)}\). Hence, \(\alpha_0 = \cdots =\alpha_D = 0\).

\((ii)\) \(\Delta\) is a connected component. Let \(f = \sum_{i\in \Delta}E_i\), then \(f\in C_M(A^*)\).

Let \(Y = (X, \{R_i\}_{0\leq i\leq 2})\) be a symmetric association scheme with \(D = 2\). Let
\[E = \frac{1}{|X|}\sum_{j=0}^2\theta^*_j A_j\]
be a primitive idempotent. If \(\theta^*_0, \theta_1^*, \theta^*_2\).

Then \(Y\) us \(Q\)-polynomial with respect to \(E\).

\emph{Proof.}
By the previous lemma, \(D_E\) is connected.

\textbf{Note.}
It seems \(\theta^*_1 \neq \theta^*_2\) is necessary. Clarify the condition \(\theta^*_1 = \theta^*_2\).

Terwilliger claims that \(\theta^*_1 = \theta^*_2\) does not occur under the assumption \((ic)\). (March 7, 1995)
\end{remark}

\hypertarget{lec26}{%
\chapter{Representation Diagrams}\label{lec26}}

\textbf{Wednesday, March 31, 1993}

\begin{proof}[Proof of Theorem \ref{thm:balanced} continued]
Assume \(Y = (X, \{R_i\}_{0\leq i\leq D})\) is \(P\)-polynomial.
Let \(E\) be a primitive idempotent of \(Y\) such that the corresponding representation \((\rho, H)\) is nondegenerate.

Show for all \(x, y\in X\),
\[\sum_{z\in X, (x,z)\in R_1, (y,z)\in R_2}\rho(z) - \sum_{z'\in X, (x,z')\in R_2, (y,z')\in R_1}\rho(z') \in \mathrm{Span}(\rho(x)-\rho(y))\]
implies that \(Y\) is \(Q\)-polynomial with respect to \(E\).

Define a diagram \(D_E\) on nodes \(0, 1, \ldots, D\), for \(i\neq j\),
\[i \frown j \leftrightarrow q^1_{ij}\neq 0\]
by setting \(E = E_1\).

We showed that \(0 \frown j \leftrightarrow j = 1\) \((1\leq j\leq D)\) and \(D_E\) is connected.

Now it is sufficient to show the following.

Claim 6. Let \(i\) be a node in \(D_E\). Then \(i\) is adjacent to at most \(2\) arcs.

\emph{Proof of Claim 6.}
Suppose the node \(j\) is adjacent to \(i\) in \(D_E\). By claim 4,
\begin{align}
0 & = E_i(A^3A^* - A^*A^3 - (\beta+1)(A^2A^*A-AA^*A^2) - \gamma(A^2A^*-A^*A) - \delta(AA^*-A^*A))E_j\\
& = E_iA^*E_j(\theta^3_i-\theta^3_j-(\beta+1)(\theta^2\theta_j - \theta_i\theta_j^2)-\gamma(\theta_i^2-\theta_j^2)-\delta(\theta_i-\theta_j))\\
& = E_iA^*A_j(\theta_i-\theta_j)p(\theta_i, \theta_j),
\end{align}
where
\[p(s,t) = s^2 - \beta st + t^2 - \gamma(s+t) - \delta.\]

\begin{remark}
\begin{align}
& (\theta_i-\theta_j)(\theta_i^2 - \beta \theta_i\theta_j + \theta_j^2 - \gamma(\theta_i + \theta_j) - \delta)\\
& = \quad \theta^3_i-\theta^3_j-(\beta+1)(\theta^2\theta_j - \theta_i\theta_j^2)-\gamma(\theta_i^2-\theta_j^2)-\delta(\theta_i-\theta_j)
\end{align}
\end{remark}

Since \(i\) is adjacent to \(j\), \(q^1_{ij}\neq 0\) and
\[E_iA^*E_j\neq O\]
by Lemma \ref{lem:vanishing-condition} \((ii)\).
Since \(Y\) is \(P\)-polynomial,
\[\theta_i \neq \theta_j \quad \text{ if }; i\neq j.\]
Hence \(p(\theta_i,\theta_j) = 0\). But \(p\) is quadratic in \(t\). So \(p(\theta_i,t) = 0\) has at most two solutions for \(\theta_j\).

Now \(D_E\) is a pth, and \(\Gamma\) is \(Q\)-polynomial with respect to \(E\).

This proves Theorem \ref{thm:balanced}.
\end{proof}

\hypertarget{lec55}{%
\chapter{Title of the Chapter}\label{lec55}}

\textbf{Wednesday, February 17, 1993} \# Edit Date

  \bibliography{book.bib,packages.bib}

\printindex

\end{document}
